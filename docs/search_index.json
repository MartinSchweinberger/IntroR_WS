[["introduction-to-text-analysis-with-r.html", "Section 3 Introduction to Text Analysis with R 3.1 What is Text Analysis? 3.2 Preparation and session set up", " Section 3 Introduction to Text Analysis with R This part of the workshop introduces basic methods of Text Analysis (see Bernard and Ryan 1998; Kabanoff 1997; Popping 2000), i.e. computer-based analysis of language data or the (semi-)automated extraction of information from text. In the following, we will explore selected methods. The methods we will focus on are: Sentiment Analysis Keyword Detection Topic Modelling Network Analysis 3.1 What is Text Analysis? Text Analysis (TA) refers to the process of examining, processing, and interpreting unstructured data (texts) to uncover actionable knowledge using computational methods. Unstructured data (text) can, for example, include emails, literary texts, letters, articles, advertisements, official documents, social media content, transcripts, and product reviews. Actionable knowledge refers to insights and patterns used to classify, sort, extract information, determine relationships, identify trends, and make informed decisions. Sometimes, Text Analysis is distinguished from Text Analytics. In this context, Text Analysis refers to manual, close-reading, and qualitative interpretative approaches, while Text Analytics refers to quantitative, computational analysis of text. However, in this tutorial, we consider Text Analysis and Text Analytics to be synonymous, encompassing any computer-based qualitative or quantitative method for analyzing text. 3.2 Preparation and session set up To ensure the scripts below run smoothly, we need to install specific R packages from a library. If you’ve already installed these packages, you can skip this section. To install them, run the code below (which may take 1 to 5 minutes). install.packages(&quot;flextable&quot;) install.packages(&quot;GGally&quot;) install.packages(&quot;ggraph&quot;) install.packages(&quot;gutenbergr&quot;) install.packages(&quot;igraph&quot;) install.packages(&quot;lda&quot;) install.packages(&quot;ldatuning&quot;) install.packages(&quot;Matrix&quot;) install.packages(&quot;network&quot;) install.packages(&quot;quanteda&quot;) install.packages(&quot;quanteda.textplots&quot;) install.packages(&quot;quanteda.textstats&quot;) install.packages(&quot;RColorBrewer&quot;) install.packages(&quot;reshape2&quot;) install.packages(&quot;slam&quot;) install.packages(&quot;sna&quot;) install.packages(&quot;textdata&quot;) install.packages(&quot;tidygraph&quot;) install.packages(&quot;tidytext&quot;) install.packages(&quot;tm&quot;) install.packages(&quot;topicmodels&quot;) install.packages(&quot;udpipe&quot;) install.packages(&quot;wordcloud&quot;) install.packages(&quot;wordcloud2&quot;) Once all packages are installed, you can activate them by executing (running) the code chunk below. # load packages library(flextable) library(GGally) library(ggraph) library(gutenbergr) library(igraph) library(lda) library(ldatuning) library(Matrix) library(network) library(quanteda) library(quanteda.textplots) library(quanteda.textstats) library(RColorBrewer) library(reshape2) library(slam) library(sna) library(textdata) library(tidygraph) library(tidytext) library(tidyverse) library(tm) library(topicmodels) library(udpipe) library(wordcloud) library(wordcloud2) # activate klippy for copy-to-clipboard button klippy::klippy() Next, we need to load our data. For the first part which focuses on sentiment analysis, we will use the IMDB data consisting of positive and negative reviews which we load by executing the code chunk below. # load reviews posreviews &lt;- list.files(here::here(&quot;data/reviews_pos&quot;), full.names = T, pattern = &quot;.*txt&quot;) %&gt;% purrr::map_chr(~ readr::read_file(.)) %&gt;% str_c(collapse = &quot; &quot;) %&gt;% str_replace_all(&quot;&lt;.*?&gt;&quot;, &quot; &quot;) negreviews &lt;- list.files(here::here(&quot;data/reviews_neg&quot;), full.names = T, pattern = &quot;.*txt&quot;) %&gt;% purrr::map_chr(~ readr::read_file(.))%&gt;% str_c(collapse = &quot; &quot;) %&gt;% str_replace_all(&quot;&lt;.*?&gt;&quot;, &quot; &quot;) # inspect str(posreviews); str(negreviews) ## chr &quot;One of the other reviewers has mentioned that after watching just 1 Oz episode you&#39;ll be hooked. They are right&quot;| __truncated__ ## chr &quot;Basically there&#39;s a family where a little boy (Jake) thinks there&#39;s a zombie in his closet &amp; his parents are fi&quot;| __truncated__ "],["exercise-2-identifying-keywords-and-sentiments.html", "Section 4 Exercise 2: Identifying Keywords and Sentiments", " Section 4 Exercise 2: Identifying Keywords and Sentiments EXERCISE TIME Have a look at the texts in the folder called ex2_sentiment (you can find it in the exercise folder in the data folder - you may need to unzip them before you can inspect them). The aim is to determine which of the reviews are positive and which are negative and to identify words that indicate what group a review belongs to. "],["sentiment-analysis.html", "Section 5 Sentiment Analysis 5.1 What is Sentiment Analysis? 5.2 Exporting the results 5.3 Summarizing results 5.4 Visualizing results 5.5 Identifying important emotives", " Section 5 Sentiment Analysis This part of the workshop showcases how to perform SA on textual data using R. The analysis shown here is in parts based on the 2nd chapter of Text Mining with R - the e-version of this chapter on sentiment analysis can be found here. 5.1 What is Sentiment Analysis? Sentiment Analysis (SA) extracts information on emotion or opinion from natural language (Silge and Robinson 2017). Most forms of SA provides information about positive or negative polarity, e.g. whether a tweet is positive or negative. As such, SA represents a type of classifier that assigns values to texts. As most SA only provide information about polarity, SA is often regarded as rather coarse-grained and, thus, rather irrelevant for the types of research questions in linguistics. In the language sciences, SA can also be a very helpful tool if the type of SA provides more fine-grained information. In the following, we will perform such a information-rich SA. The SA used here does not only provide information about polarity but it will also provide association values for eight core emotions. The more fine-grained output is made possible by relying on the Word-Emotion Association Lexicon (Mohammad and Turney 2013), which comprises 10,170 terms, and in which lexical elements are assigned scores based on ratings gathered through the crowd-sourced Amazon Mechanical Turk service. For the Word-Emotion Association Lexicon raters were asked whether a given word was associated with one of eight emotions. The resulting associations between terms and emotions are based on 38,726 ratings from 2,216 raters who answered a sequence of questions for each word which were then fed into the emotion association rating (cf. Mohammad and Turney 2013). Each term was rated 5 times. For 85 percent of words, at least 4 raters provided identical ratings. For instance, the word cry or tragedy are more readily associated with SADNESS while words such as happy or beautiful are indicative of JOY and words like fit or burst may indicate ANGER. This means that the SA here allows us to investigate the expression of certain core emotions rather than merely classifying statements along the lines of a crude positive-negative distinction. We start by writing a function that clean the data. This allows us to feed our texts into the function and avoids duplicating code. Also, this showcases how you can write functions. # Define a function &#39;txtclean&#39; that takes an input &#39;x&#39; and a &#39;title&#39; txtclean &lt;- function(x, title) { # Load required libraries require(dplyr) require(stringr) require(tibble) # Convert encoding to UTF-8 x &lt;- x %&gt;% iconv(to = &quot;UTF-8&quot;) %&gt;% # Convert text to lowercase base::tolower() %&gt;% # Collapse text into a single string paste0(collapse = &quot; &quot;) %&gt;% # Remove extra whitespace stringr::str_squish() %&gt;% # Split text into individual words stringr::str_split(&quot; &quot;) %&gt;% # Unlist the words into a vector unlist() %&gt;% # Convert the vector to a tibble tibble::tibble() %&gt;% # Select the first column and name it &#39;word&#39; dplyr::select(word = 1, everything()) %&gt;% # Add a new column &#39;type&#39; with the given title dplyr::mutate(type = title) %&gt;% # Remove stop words dplyr::anti_join(stop_words) %&gt;% # Remove non-word characters from the &#39;word&#39; column dplyr::mutate(word = str_remove_all(word, &quot;\\\\W&quot;)) %&gt;% # Filter out empty words dplyr::filter(word != &quot;&quot;) } Process and clean texts. # process text data posreviews_clean &lt;- txtclean(posreviews, &quot;Positive Review&quot;) negreviews_clean &lt;- txtclean(negreviews, &quot;Negative Review&quot;) # inspect str(posreviews_clean); str(negreviews_clean) ## tibble [95,527 × 2] (S3: tbl_df/tbl/data.frame) ## $ word: chr [1:95527] &quot;reviewers&quot; &quot;mentioned&quot; &quot;watching&quot; &quot;1&quot; ... ## $ type: chr [1:95527] &quot;Positive Review&quot; &quot;Positive Review&quot; &quot;Positive Review&quot; &quot;Positive Review&quot; ... ## tibble [90,656 × 2] (S3: tbl_df/tbl/data.frame) ## $ word: chr [1:90656] &quot;basically&quot; &quot;family&quot; &quot;boy&quot; &quot;jake&quot; ... ## $ type: chr [1:90656] &quot;Negative Review&quot; &quot;Negative Review&quot; &quot;Negative Review&quot; &quot;Negative Review&quot; ... Now, we combine the data with the Word-Emotion Association Lexicon (Mohammad and Turney 2013). # Combine positive and negative reviews into one dataframe reviews_annotated &lt;- rbind(posreviews_clean, negreviews_clean) %&gt;% # Group the combined dataframe by the &#39;type&#39; column dplyr::group_by(type) %&gt;% # Add a new column &#39;words&#39; with the count of words for each group dplyr::mutate(words = n()) %&gt;% # Join the sentiment data from the &#39;nrc&#39; sentiment lexicon dplyr::left_join(tidytext::get_sentiments(&quot;nrc&quot;)) %&gt;% # Convert &#39;type&#39; and &#39;sentiment&#39; columns to factors dplyr::mutate(type = factor(type), sentiment = factor(sentiment)) # inspect data reviews_annotated %&gt;% as.data.frame() %&gt;% head(10) ## word type words sentiment ## 1 reviewers Positive Review 95527 &lt;NA&gt; ## 2 mentioned Positive Review 95527 &lt;NA&gt; ## 3 watching Positive Review 95527 &lt;NA&gt; ## 4 1 Positive Review 95527 &lt;NA&gt; ## 5 oz Positive Review 95527 &lt;NA&gt; ## 6 episode Positive Review 95527 &lt;NA&gt; ## 7 hooked Positive Review 95527 negative ## 8 right Positive Review 95527 &lt;NA&gt; ## 9 happened Positive Review 95527 &lt;NA&gt; ## 10 me Positive Review 95527 &lt;NA&gt; The resulting table shows each word token by the type of review in which it occurred, the overall number of tokens in the type of review, and the sentiment with which a token is associated. 5.2 Exporting the results To export the table with the results as an MS Excel spreadsheet, we use write_xlsx. Be aware that we use the here function to save the file in the current working directory. # save data write_xlsx(reviews_annotated, here::here(&quot;data/reviews_annotated.xlsx&quot;)) 5.3 Summarizing results After preforming the sentiment analysis, we can now display and summarize the results of the SA visually and add information to the table produced by the sentiment analysis (such as calculating the percentages of the prevalence of emotions across the review type and the rate of emotions across review types). # Group the annotated reviews by type and sentiment reviews_summarised &lt;- reviews_annotated %&gt;% dplyr::group_by(type, sentiment) %&gt;% # Summarise the data: get unique sentiments, count frequency, and unique word counts dplyr::summarise(sentiment = unique(sentiment), sentiment_freq = n(), words = unique(words)) %&gt;% # Filter out any rows where sentiment is NA dplyr::filter(is.na(sentiment) == F) %&gt;% # Add a percentage column and set the order of sentiment factor levels dplyr::mutate(percentage = round(sentiment_freq/words*100, 1), sentiment = factor(sentiment, levels = c( #negative &quot;anger&quot;, &quot;fear&quot;, &quot;disgust&quot;, &quot;sadness&quot;, #positive &quot;anticipation&quot;, &quot;surprise&quot;, &quot;trust&quot;, &quot;joy&quot;, # stance &quot;negative&quot;, &quot;positive&quot;))) %&gt;% # Group by sentiment to prepare for total percentage calculation dplyr::group_by(sentiment) %&gt;% # Calculate the total percentage for each sentiment dplyr::mutate(total = sum(percentage)) %&gt;% # Group by both sentiment and type for the ratio calculation dplyr::group_by(sentiment, type) %&gt;% # Calculate the ratio of percentage to total percentage for each sentiment-type pair dplyr::mutate(ratio = round(percentage/total*100, 1)) # inspect data reviews_summarised %&gt;% as.data.frame() %&gt;% head(10) ## type sentiment sentiment_freq words percentage total ratio ## 1 Negative Review anger 4628 90656 5.1 8.5 60.0 ## 2 Negative Review anticipation 4617 90656 5.1 10.7 47.7 ## 3 Negative Review disgust 4093 90656 4.5 6.9 65.2 ## 4 Negative Review fear 5624 90656 6.2 10.9 56.9 ## 5 Negative Review joy 3743 90656 4.1 9.9 41.4 ## 6 Negative Review negative 9460 90656 10.4 17.5 59.4 ## 7 Negative Review positive 9335 90656 10.3 22.8 45.2 ## 8 Negative Review sadness 4914 90656 5.4 9.2 58.7 ## 9 Negative Review surprise 2605 90656 2.9 5.9 49.2 ## 10 Negative Review trust 5361 90656 5.9 12.9 45.7 To export the table with the results as an MS Excel spreadsheet, we use write_xlsx. Be aware that we use the here function to save the file in the current working directory. # save data write_xlsx(reviews_summarised, here::here(&quot;data/reviews_summarised.xlsx&quot;)) 5.4 Visualizing results After performing the SA, we can display the emotions by review type ordered from more negative (red) to more positive (blue). reviews_summarised %&gt;% dplyr::filter(sentiment != &quot;positive&quot;, sentiment != &quot;negative&quot;) %&gt;% # plot ggplot(aes(type, percentage, fill = sentiment, label = percentage)) + geom_bar(stat=&quot;identity&quot;, position=position_dodge()) + geom_text(hjust=1.5, position = position_dodge(0.9)) + scale_fill_brewer(palette = &quot;RdBu&quot;) + theme_bw() + theme(legend.position = &quot;right&quot;) + coord_flip() + labs(x = &quot;&quot;, y = &quot;Percent (%)&quot;) We can also visualize the results and show the rate to identify what type is more “positive” and what type is more “negative”. reviews_summarised %&gt;% dplyr::filter(sentiment != &quot;positive&quot;, sentiment != &quot;negative&quot;) %&gt;% # plot ggplot(aes(sentiment, ratio, fill = type, label = ratio)) + geom_bar(stat=&quot;identity&quot;, position=position_fill()) + geom_text(position = position_fill(vjust = 0.5)) + scale_fill_manual(name = &quot;&quot;, values=c(&quot;orange&quot;, &quot;gray70&quot;)) + scale_y_continuous(name =&quot;Percent&quot;, breaks = seq(0, 1, .2), labels = seq(0, 100, 20)) + theme_bw() + theme(legend.position = &quot;top&quot;) 5.5 Identifying important emotives We now check, which words have contributed to the emotionality scores. In other words, we investigate, which words are most important for the emotion scores within each review type. For the sake of interpretability, we will remove several core emotion categories and also the polarity. # Filter the annotated reviews to exclude certain sentiments and remove NA sentiments reviews_importance &lt;- reviews_annotated %&gt;% dplyr::filter(!is.na(sentiment), # Keep only rows with non-NA sentiments sentiment != &quot;anticipation&quot;, # Exclude &#39;anticipation&#39; sentiment sentiment != &quot;surprise&quot;, # Exclude &#39;surprise&#39; sentiment sentiment != &quot;disgust&quot;, # Exclude &#39;disgust&#39; sentiment sentiment != &quot;negative&quot;, # Exclude &#39;negative&#39; sentiment sentiment != &quot;sadness&quot;, # Exclude &#39;sadness&#39; sentiment sentiment != &quot;positive&quot;) %&gt;% # Convert &#39;sentiment&#39; to a factor with specified levels dplyr::mutate(sentiment = factor(sentiment, levels = c(&quot;anger&quot;, &quot;fear&quot;, &quot;trust&quot;, &quot;joy&quot;))) %&gt;% # Group the data by &#39;type&#39; dplyr::group_by(type) %&gt;% # Count the occurrences of each word within each sentiment and type, sorting the counts in descending order dplyr::count(word, sentiment, sort = TRUE) %&gt;% # Further group the data by &#39;type&#39; and &#39;sentiment&#39; dplyr::group_by(type, sentiment) %&gt;% # Select the top 5 words within each &#39;type&#39; and &#39;sentiment&#39; dplyr::top_n(5) %&gt;% # Calculate the score for each word as the proportion of the total count within each group dplyr::mutate(score = n/sum(n)) # inspect data reviews_importance %&gt;% as.data.frame() %&gt;% head(10) ## type word sentiment n score ## 1 Negative Review bad anger 575 0.5203620 ## 2 Negative Review bad fear 575 0.4622186 ## 3 Positive Review love joy 329 0.3884298 ## 4 Positive Review watch fear 296 0.4111111 ## 5 Negative Review watch fear 284 0.2282958 ## 6 Positive Review real trust 225 0.3040541 ## 7 Negative Review pretty trust 177 0.2338177 ## 8 Negative Review pretty joy 177 0.2637854 ## 9 Negative Review director trust 160 0.2113606 ## 10 Negative Review real trust 149 0.1968296 We can now visualize the top three words for the remaining core emotion categories. # Group the reviews importance data by &#39;type&#39; reviews_importance %&gt;% dplyr::group_by(type) %&gt;% # Select the top 20 rows within each group based on the &#39;score&#39; column dplyr::slice_max(score, n = 20) %&gt;% # Arrange the rows in descending order of &#39;score&#39; dplyr::arrange(desc(score)) %&gt;% # Ungroup the data to remove the grouping structure dplyr::ungroup() %&gt;% # plot ggplot(aes(x = reorder(word, score), y = score, fill = word)) + facet_wrap(type~sentiment, ncol = 4, scales = &quot;free_y&quot;) + geom_col(show.legend = FALSE) + coord_flip() + labs(x = &quot;Words&quot;) If you are interested in learning more about SA in R, Silge and Robinson (2017) is highly recommended as it goes more into detail and offers additional information. "],["keyword-detection.html", "Section 6 Keyword Detection 6.1 Dimensions of keyness 6.2 Identifying keywords 6.3 Visualising keywords", " Section 6 Keyword Detection Keywords play a crucial role in text analysis, serving as distinctive terms that hold particular significance within a given text, context, or collection. These words stand out due to their heightened frequency in a specific text or context, setting them apart from their occurrence in another. In essence, keywords are linguistic markers that encapsulate the essence or topical focus of a document or data set. The process of identifying keywords involves a methodology akin to the one employed for detecting collocations using kwics. This entails comparing the use of a particular word in corpus A, against its use in corpus B. By discerning the frequency disparities, we gain valuable insights into the salient terms that contribute significantly to the unique character and thematic emphasis of a given text or context. 6.1 Dimensions of keyness Before we start with the practical part of this tutorial, it is important to talk about the different dimensions of keyness (see Sönning 2023). Keyness analysis identifies typical items in a discourse domain, where typicalness traditionally relates to frequency of occurrence. The emphasis is on items used more frequently in the target corpus compared to a reference corpus. Egbert and Biber (2019) expanded this notion, highlighting two criteria for typicalness: content-distinctiveness and content-generalizability. Content-distinctiveness refers to an item’s association with the domain and its topical relevance. Content-generalizability pertains to an item’s widespread usage across various texts within the domain. These criteria bridge traditional keyness approaches with broader linguistic perspectives, emphasizing both the distinctiveness and generalizability of key items within a corpus. Following Sönning (2023), we adopt Egbert and Biber (2019) keyness criteria, distinguishing between frequency-oriented and dispersion-oriented approaches to assess keyness. These perspectives capture distinct, linguistically meaningful attributes of typicalness. We also differentiate between keyness features inherent to the target variety and those that emerge from comparing it to a reference variety. This four-way classification, detailed in the table below, links methodological choices to the linguistic meaning conveyed by quantitative measures. Typical items exhibit a sufficiently high occurrence rate to be discernible in the target variety, with discernibility measured solely within the target corpus. Key items are also distinct, being used more frequently than in reference domains of language use. While discernibility and distinctiveness both rely on frequency, they measure different aspects of typicalness. .cl-038737b4{table-layout:auto;width:75%;}.cl-037e93e8{font-family:'Arial';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-037e93f2{font-family:'Arial';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-0381e2f0{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-0381fd1c{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0381fd30{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0381fd31{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0381fd32{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0381fd3a{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0381fd3b{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0381fd44{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0381fd4e{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0381fd4f{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.1: Dimensions of keyness (see Soenning, 2023: 3) AnalysisFrequency.orientedDispersion.orientedTarget variety in isolationDiscernibility of item in the target varietyGenerality across texts in the target varietyComparison to reference varietyDistinctiveness relative to the reference varietyComparative generality relative to the reference variety The second aspect of keyness involves an item’s dispersion across texts in the target domain, indicating its widespread use. A typical item should appear evenly across various texts within the target domain, reflecting its generality. This breadth of usage can be compared to its occurrence in the reference domain, termed as comparative generality. Therefore, a key item should exhibit greater prevalence across target texts compared to those in the reference domain. 6.2 Identifying keywords Here, we focus on a frequency-based approach that assesses distinctiveness relative to the reference variety. To identify these keywords, we can follow the procedure we have used to identify collocations using kwics - the idea is essentially identical: we compare the use of a word in a target corpus A to its use in a reference corpus. To determine if a token is a keyword and if it occurs significantly more frequently in a target corpus compared to a reference corpus, we use the following information (that is provided by the table above): O11 = Number of times wordx occurs in target corpus O12 = Number of times wordx occurs in reference corpus (without target corpus) O21 = Number of times other words occur in target corpus O22 = Number of times other words occur in reference corpus Example: target corpus reference corpus token O11 O12 = R1 other tokens O21 O22 = R2 = C1 = C2 = N We begin with loading two texts (posreviews is our target and negreviews is our reference). As a first step, we create a frequency table of first text. positive_words &lt;- posreviews %&gt;% # remove non-word characters stringr::str_remove_all(&quot;[^[:alpha:] ]&quot;) %&gt;% # convert to lower tolower() %&gt;% # tokenize the corpus files quanteda::tokens(remove_punct = T, remove_symbols = T, remove_numbers = T) %&gt;% # unlist the tokens to create a data frame unlist() %&gt;% as.data.frame() %&gt;% # rename the column to &#39;token&#39; dplyr::rename(token = 1) %&gt;% # group by &#39;token&#39; and count the occurrences dplyr::group_by(token) %&gt;% dplyr::summarise(n = n()) %&gt;% # add column stating where the frequency list is &#39;from&#39; dplyr::mutate(type = &quot;positive&quot;) # inspect head(positive_words) ## # A tibble: 6 × 3 ## token n type ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 a 6462 positive ## 2 aaliyah 1 positive ## 3 aames 1 positive ## 4 aamir 1 positive ## 5 aardman 2 positive ## 6 aaron 2 positive Now, we create a frequency table of second text. negative_words &lt;- negreviews %&gt;% # remove non-word characters stringr::str_remove_all(&quot;[^[:alpha:] ]&quot;) %&gt;% # convert to lower tolower() %&gt;% # tokenize the corpus files quanteda::tokens(remove_punct = T, remove_symbols = T, remove_numbers = T) %&gt;% # unlist the tokens to create a data frame unlist() %&gt;% as.data.frame() %&gt;% # rename the column to &#39;token&#39; dplyr::rename(token = 1) %&gt;% # group by &#39;token&#39; and count the occurrences dplyr::group_by(token) %&gt;% dplyr::summarise(n = n()) %&gt;% # add column stating where the frequency list is &#39;from&#39; dplyr::mutate(type = &quot;negative&quot;) # inspect head(negative_words) ## # A tibble: 6 × 3 ## token n type ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 a 6134 negative ## 2 aaargh 1 negative ## 3 aap 1 negative ## 4 aaron 2 negative ## 5 abandoned 8 negative ## 6 abandoning 1 negative In a next step, we combine the tables. texts_df &lt;- dplyr::left_join(positive_words, negative_words, by = c(&quot;token&quot;)) %&gt;% # rename columns and select relevant columns dplyr::rename(positive = n.x, negative = n.y) %&gt;% dplyr::select(-type.x, -type.y) %&gt;% # replace NA values with 0 in &#39;corpus&#39; and &#39;kwic&#39; columns tidyr::replace_na(list(positive = 0, negative = 0)) # inspect texts_df %&gt;% as.data.frame() %&gt;% head(10) ## token positive negative ## 1 a 6462 6134 ## 2 aaliyah 1 0 ## 3 aames 1 0 ## 4 aamir 1 0 ## 5 aardman 2 0 ## 6 aaron 2 2 ## 7 aawip 1 0 ## 8 ab 2 0 ## 9 abandon 2 0 ## 10 abandoned 4 8 We now calculate the frequencies of the observed and expected frequencies as well as the row and column totals. # Convert &#39;positive&#39; and &#39;negative&#39; columns to numeric texts_df %&gt;% dplyr::mutate(positive = as.numeric(positive), negative = as.numeric(negative)) %&gt;% # Calculate column sums for &#39;positive&#39;, &#39;negative&#39;, and their total dplyr::mutate(C1 = sum(positive), # Total count of positive values C2 = sum(negative), # Total count of negative values N = C1 + C2) %&gt;% # Total count of all values # Process each row individually dplyr::rowwise() %&gt;% # Calculate row-wise totals and other derived metrics dplyr::mutate(R1 = positive + negative, # Row total for positive and negative R2 = N - R1, # Total remaining count O11 = positive, # Observed positive count O12 = R1 - O11, # Observed negative count O21 = C1 - O11, # Total positive minus observed positive O22 = C2 - O12) %&gt;% # Total negative minus observed negative # Calculate expected counts for each cell in a contingency table dplyr::mutate(E11 = (R1 * C1) / N, # Expected count for O11 E12 = (R1 * C2) / N, # Expected count for O12 E21 = (R2 * C1) / N, # Expected count for O21 E22 = (R2 * C2) / N) %&gt;% # Expected count for O22 # Select all columns except for &#39;positive&#39; and &#39;negative&#39; dplyr::select(-positive, -negative) -&gt; stats_raw # inspect stats_raw %&gt;% as.data.frame() %&gt;% head(10) ## token C1 C2 N R1 R2 O11 O12 O21 O22 ## 1 a 225776 206392 432168 12596 419572 6462 6134 219314 200258 ## 2 aaliyah 225776 206392 432168 1 432167 1 0 225775 206392 ## 3 aames 225776 206392 432168 1 432167 1 0 225775 206392 ## 4 aamir 225776 206392 432168 1 432167 1 0 225775 206392 ## 5 aardman 225776 206392 432168 2 432166 2 0 225774 206392 ## 6 aaron 225776 206392 432168 4 432164 2 2 225774 206390 ## 7 aawip 225776 206392 432168 1 432167 1 0 225775 206392 ## 8 ab 225776 206392 432168 2 432166 2 0 225774 206392 ## 9 abandon 225776 206392 432168 2 432166 2 0 225774 206392 ## 10 abandoned 225776 206392 432168 12 432156 4 8 225772 206384 ## E11 E12 E21 E22 ## 1 6580.4837378 6015.5162622 219195.5 200376.5 ## 2 0.5224265 0.4775735 225775.5 206391.5 ## 3 0.5224265 0.4775735 225775.5 206391.5 ## 4 0.5224265 0.4775735 225775.5 206391.5 ## 5 1.0448529 0.9551471 225775.0 206391.0 ## 6 2.0897059 1.9102941 225773.9 206390.1 ## 7 0.5224265 0.4775735 225775.5 206391.5 ## 8 1.0448529 0.9551471 225775.0 206391.0 ## 9 1.0448529 0.9551471 225775.0 206391.0 ## 10 6.2691176 5.7308824 225769.7 206386.3 We could now calculate the keyness statistics for all words in the reviews. However, this takes a few minutes an do we will exclude tokens that occur less than 10 times. stats_redux &lt;- stats_raw %&gt;% dplyr::filter(R1 &gt; 10) We can now calculate the keyness measures. stats_redux %&gt;% # determine number of rows dplyr::mutate(Rws = nrow(.)) %&gt;% # work row-wise dplyr::rowwise() %&gt;% # calculate fishers&#39; exact test dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(O11, O12, O21, O22), ncol = 2, byrow = T))[1]))) %&gt;% # extract descriptives dplyr::mutate(ptw_target = O11/C1*1000, ptw_ref = O12/C2*1000) %&gt;% # extract x2 statistics dplyr::mutate(X2 = (O11-E11)^2/E11 + (O12-E12)^2/E12 + (O21-E21)^2/E21 + (O22-E22)^2/E22) %&gt;% # extract keyness measures dplyr::mutate(phi = sqrt((X2 / N)), MI = log2(O11 / E11), t.score = (O11 - E11) / sqrt(O11), PMI = log2( (O11 / N) / ((O11+O12) / N) * ((O11+O21) / N) ), DeltaP = (O11 / R1) - (O21 / R2), LogOddsRatio = log(((O11 + 0.5) * (O22 + 0.5)) / ( (O12 + 0.5) * (O21 + 0.5) )), G2 = 2 * ((O11+ 0.001) * log((O11+ 0.001) / E11) + (O12+ 0.001) * log((O12+ 0.001) / E12) + O21 * log(O21 / E21) + O22 * log(O22 / E22)), # traditional keyness measures RateRatio = ((O11+ 0.001)/(C1*1000)) / ((O12+ 0.001)/(C2*1000)), RateDifference = (O11/(C1*1000)) - (O12/(C2*1000)), DifferenceCoefficient = RateDifference / sum((O11/(C1*1000)), (O12/(C2*1000))), OddsRatio = ((O11 + 0.5) * (O22 + 0.5)) / ( (O12 + 0.5) * (O21 + 0.5) ), LLR = 2 * (O11 * (log((O11 / E11)))), RDF = abs((O11 / C1) - (O12 / C2)), PDiff = abs(ptw_target - ptw_ref) / ((ptw_target + ptw_ref) / 2) * 100, SignedDKL = sum(ifelse(O11 &gt; 0, O11 * log(O11 / ((O11 + O12) / 2)), 0) - ifelse(O12 &gt; 0, O12 * log(O12 / ((O11 + O12) / 2)), 0))) %&gt;% # determine Bonferroni corrected significance dplyr::mutate(Sig_corrected = dplyr::case_when(p / Rws &gt; .05 ~ &quot;n.s.&quot;, p / Rws &gt; .01 ~ &quot;p &lt; .05*&quot;, p / Rws &gt; .001 ~ &quot;p &lt; .01**&quot;, p / Rws &lt;= .001 ~ &quot;p &lt; .001***&quot;, T ~ &quot;N.A.&quot;)) %&gt;% # round p-value dplyr::mutate(p = round(p, 5), type = ifelse(E11 &gt; O11, &quot;antitype&quot;, &quot;type&quot;), phi = ifelse(E11 &gt; O11, -phi, phi), G2 = ifelse(E11 &gt; O11, -G2, G2)) %&gt;% # filter out non significant results dplyr::filter(Sig_corrected != &quot;n.s.&quot;) %&gt;% # arrange by G2 dplyr::arrange(-G2) %&gt;% # remove superfluous columns dplyr::select(-any_of(c(&quot;TermCoocFreq&quot;, &quot;AllFreq&quot;, &quot;NRows&quot;, &quot;R1&quot;, &quot;R2&quot;, &quot;C1&quot;, &quot;C2&quot;, &quot;E12&quot;, &quot;E21&quot;, &quot;E22&quot;, &quot;upp&quot;, &quot;low&quot;, &quot;op&quot;, &quot;t.score&quot;, &quot;z.score&quot;, &quot;Rws&quot;))) %&gt;% dplyr::relocate(any_of(c(&quot;token&quot;, &quot;type&quot;, &quot;Sig_corrected&quot;, &quot;O11&quot;, &quot;O12&quot;, &quot;ptw_target&quot;, &quot;ptw_ref&quot;, &quot;G2&quot;, &quot;RDF&quot;, &quot;RateRatio&quot;, &quot;RateDifference&quot;, &quot;DifferenceCoefficient&quot;, &quot;LLR&quot;, &quot;SignedDKL&quot;, &quot;PDiff&quot;, &quot;LogOddsRatio&quot;, &quot;MI&quot;, &quot;PMI&quot;, &quot;phi&quot;, &quot;X2&quot;, &quot;OddsRatio&quot;, &quot;DeltaP&quot;, &quot;p&quot;, &quot;E11&quot;, &quot;O21&quot;, &quot;O22&quot;))) -&gt; keys keys %&gt;% as.data.frame() %&gt;% head(10) ## token type Sig_corrected O11 O12 ptw_target ptw_ref G2 ## 1 great type p &lt; .001*** 480 188 2.1260010 0.9108880 107.34903 ## 2 excellent type p &lt; .001*** 149 21 0.6599461 0.1017481 97.43274 ## 3 love type p &lt; .001*** 329 133 1.4571965 0.6444048 69.25605 ## 4 wonderful type p &lt; .001*** 122 27 0.5403586 0.1308190 57.33152 ## 5 loved type p &lt; .001*** 103 21 0.4562044 0.1017481 52.00009 ## 6 best type p &lt; .001*** 333 157 1.4749132 0.7606884 49.89776 ## 7 world type p &lt; .001*** 212 91 0.9389838 0.4409086 39.47303 ## 8 his type p &lt; .001*** 1264 884 5.5984693 4.2831117 37.98642 ## 9 still type p &lt; .001*** 275 134 1.2180214 0.6492500 37.82586 ## 10 perfect type p &lt; .001*** 95 25 0.4207710 0.1211287 37.50859 ## RDF RateRatio RateDifference DifferenceCoefficient LLR ## 1 0.0012151130 2.333980 1.215113e-06 0.4001177 306.01822 ## 2 0.0005581980 6.485811 5.581980e-07 0.7328374 154.19084 ## 3 0.0008127917 2.261296 8.127917e-07 0.3867488 203.82465 ## 4 0.0004095396 4.130462 4.095396e-07 0.6101806 109.64037 ## 5 0.0003544562 4.483494 3.544562e-07 0.6352803 95.52600 ## 6 0.0007142248 1.938912 7.142248e-07 0.3194777 175.16342 ## 7 0.0004980752 2.129643 4.980752e-07 0.3609522 123.86079 ## 8 0.0013153575 1.307103 1.315358e-06 0.1331121 300.87033 ## 9 0.0005687714 1.876037 5.687714e-07 0.3046003 138.77984 ## 10 0.0002996423 3.473649 2.996423e-07 0.5529478 78.97468 ## SignedDKL PDiff LogOddsRatio MI PMI phi X2 ## 1 282.11307 80.02354 0.8471803 0.4598864 -1.413514 0.015449817 103.15714 ## 2 112.99367 146.56747 1.8500360 0.7464777 -1.126923 0.014060743 85.44156 ## 3 189.77269 77.34975 0.8145226 0.4468948 -1.426505 0.012423660 66.70397 ## 4 87.57706 122.03612 1.4045688 0.6482689 -1.225131 0.011018859 52.47179 ## 5 75.01710 127.05606 1.4821074 0.6690043 -1.204396 0.010453774 47.22792 ## 6 172.05941 63.89553 0.6611666 0.3794405 -1.493960 0.010600888 48.56653 ## 7 117.61721 72.19045 0.7533356 0.4214466 -1.451954 0.009399071 38.17882 ## 8 377.99898 26.62241 0.2689656 0.1717026 -1.701698 0.009342598 37.72141 ## 9 138.10143 60.92006 0.6278269 0.3640309 -1.509369 0.009239335 36.89216 ## 10 65.54229 110.58957 1.2309816 0.5996651 -1.273735 0.008983216 34.87516 ## OddsRatio DeltaP p E11 O21 O22 N ## 1 2.333059 0.19644005 0 348.98088 225296 206204 432168 ## 2 6.360048 0.35418345 0 88.81250 225627 206371 432168 ## 3 2.258098 0.18989775 0 241.36103 225447 206259 432168 ## 4 4.073770 0.29646770 0 77.84154 225654 206365 432168 ## 5 4.402213 0.30830716 0 64.78088 225673 206371 432168 ## 6 1.937051 0.15734377 0 255.98897 225443 206235 432168 ## 7 2.124073 0.17736786 0 158.29522 225564 206301 432168 ## 8 1.308610 0.06635773 0 1122.17204 224512 205508 432168 ## 9 1.873535 0.15008722 0 213.67242 225501 206258 432168 ## 10 3.424590 0.26931498 0 62.69118 225681 206367 432168 The above table shows the keywords for positive IMDB reviews. The table starts with token (word type), followed by type, which indicates whether the token is a keyword in the target data (type) or a keyword in the reference data (antitype). Next is the Bonferroni corrected significance (Sig_corrected), which accounts for repeated testing. This is followed by O11, representing the observed frequency of the token, and Exp which represents the expected frequency of the token if it were distributed evenly across the target and reference data. After this, the table provides different keyness statistics (for information about these different keyness statistics see here). 6.3 Visualising keywords We can now visualize the keyness strengths in a dot plot as shown in the code chunk below. # sort the keys data frame in descending order based on the &#39;G2&#39; column keys %&gt;% dplyr::arrange(-G2) %&gt;% # select the top 20 rows after sorting head(20) %&gt;% # create a ggplot with &#39;token&#39; on the x-axis (reordered by &#39;G2&#39;) and &#39;G2&#39; on the y-axis ggplot(aes(x = reorder(token, G2, mean), y = G2)) + # add a scatter plot with points representing the &#39;G2&#39; values geom_point() + # flip the coordinates to have horizontal points coord_flip() + # set the theme to a basic white and black theme theme_bw() + # set the x-axis label to &quot;Token&quot; and y-axis label to &quot;Keyness (G2)&quot; labs(x = &quot;Token&quot;, y = &quot;Keyness (G2)&quot;) Another option to visualize keyness is a bar plot as shown below. # get top 10 keywords for text 1 top &lt;- keys %&gt;% dplyr::ungroup() %&gt;% dplyr::slice_head(n = 12) # get top 10 keywords for text 2 bot &lt;- keys %&gt;% dplyr::ungroup() %&gt;% dplyr::slice_tail(n = 12) # combine into table rbind(top, bot) %&gt;% # create a ggplot ggplot(aes(x = reorder(token, G2, mean), y = G2, label = G2, fill = type)) + # add a bar plot using the &#39;phi&#39; values geom_bar(stat = &quot;identity&quot;) + # add text labels above the bars with rounded &#39;phi&#39; values geom_text(aes(y = ifelse(G2&gt; 0, G2 - 20, G2 + 20), label = round(G2, 1)), color = &quot;white&quot;, size = 3) + # flip the coordinates to have horizontal bars coord_flip() + # set the theme to a basic white and black theme theme_bw() + # remove legend theme(legend.position = &quot;none&quot;) + # define colors scale_fill_manual(values = c(&quot;orange&quot;,&quot;darkgray&quot;)) + # set the x-axis label to &quot;Token&quot; and y-axis label to &quot;Keyness (G2)&quot; labs(title = &quot;Top 10 keywords for positive and negative IMDB reviews&quot;, x = &quot;Keyword&quot;, y = &quot;Keyness (G2)&quot;) "],["networks-and-collocation-analysis.html", "Section 7 Networks and Collocation Analysis 7.1 Networks 7.2 Collocations", " Section 7 Networks and Collocation Analysis In this part of the workshop, we will extract collocatiosn and display tem in a network. 7.1 Networks Networks are a powerful method for visualizing relationships among various elements, such as authors, characters, or words (Silge and Robinson 2017, 131–37). Network analysis goes beyond mere visualization; it’s a technique for uncovering patterns and structures within complex systems. In essence, network analysis represents relationships as nodes (elements) connected by edges (relationships) which provides a unique perspective for understanding the connections and interactions within your data. Networks, also known as graphs, are powerful tools that represent relationships among entities. They consist of nodes (often depicted as dots) and edges (typically represented as lines) and can be categorized as directed or undirected networks. In directed networks, the direction of edges is captured, signifying the flow or relationship from one node to another. An example of a directed network is the trade relationships between countries, where arrows on the edges indicate the direction of exports. The thickness of these arrows can also encode additional information, such as the frequency or strength of the relationship. Undirected networks, on the other hand, represent symmetric relationships where the connection between two nodes is mutual. For example, in a social network, the connections between individuals are often undirected, as the relationship between friends is reciprocal. Network analysis involves exploring the structure and properties of these networks. One key concept is centrality, which identifies the most important nodes in a network. Centrality metrics, such as degree centrality (number of connections) and betweenness centrality (importance in connecting other nodes), help unveil the significance of specific nodes. 7.2 Collocations Collocations are like linguistic buddies. They’re those word pairs that just seem to go hand in hand, like Merry Christmas. You see, these words have a special relationship – they occur together way more often than if words were just randomly strung together in a sentence. Before we start tough, it is important to understand that identifying words pairs (w1 and w2) that collocate (i.e. collocations) and determining their association strength (a measure of how strongly attracted words are to each other) is based on the co-occurrence frequencies of word pairs in a contingency table (see below, O is short for observed frequency). w2 present w2 absent w1 present O11 O12 = R1 w1 absent O21 O22 = R2 = C1 = C2 = N In the following, we will extract collocations from the sentences in our example text. In a first step, we split our example text into sentences and clean the data (removing punctuation, converting to lower case, etc.). # load reviews list.files(here::here(&quot;data/reviews_pos&quot;), full.names = T, pattern = &quot;.*txt&quot;) %&gt;% purrr::map_chr(~ readr::read_file(.)) %&gt;% str_c(collapse = &quot; &quot;) %&gt;% str_replace_all(&quot;&lt;.*?&gt;&quot;, &quot; &quot;) %&gt;% # split text into sentences tokenizers::tokenize_sentences() %&gt;% # unlist sentences unlist() %&gt;% # remove non-word characters stringr::str_replace_all(&quot;\\\\W&quot;, &quot; &quot;) %&gt;% stringr::str_replace_all(&quot;[^[:alnum:] ]&quot;, &quot; &quot;) %&gt;% # remove superfluous white spaces stringr::str_squish() %&gt;% # convert to lower case and save in &#39;sentences&#39; object tolower() -&gt; sentences # inspect sentences %&gt;% as.data.frame() %&gt;% head(10) ## . ## 1 one of the other reviewers has mentioned that after watching just 1 oz episode you ll be hooked ## 2 they are right as this is exactly what happened with me ## 3 the first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go ## 4 trust me this is not a show for the faint hearted or timid ## 5 this show pulls no punches with regards to drugs sex or violence ## 6 its is hardcore in the classic use of the word ## 7 it is called oz as that is the nickname given to the oswald maximum security state penitentary ## 8 it focuses mainly on emerald city an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda ## 9 em city is home to many ## 10 aryans muslims gangstas latinos christians italians irish and more so scuffles death stares dodgy dealings and shady agreements are never far away Next, we tabulate the data and reformat it so that we have the relevant information to calculate the association statistics (word 1 and word 2 as well as O11, O12, O21, and O22). # tokenize the &#39;sentences&#39; data using quanteda package sentences %&gt;% quanteda::tokens(remove_punct = T, remove_symbols = T, remove_numbers = T) %&gt;% # remove stopwords tokens_remove(stopwords(&quot;en&quot;)) %&gt;% # create a document-feature matrix (dfm) using quanteda quanteda::dfm() %&gt;% # create a feature co-occurrence matrix (fcm) without considering trigrams quanteda::fcm(tri = FALSE) %&gt;% # tidy the data using tidytext package tidytext::tidy() %&gt;% # rearrange columns for better readability dplyr::relocate(term, document, count) %&gt;% # rename columns for better interpretation dplyr::rename(w1 = 1, w2 = 2, O11 = 3) -&gt; coll_basic # inspect coll_basic %&gt;% as.data.frame() %&gt;% head(10) ## w1 w2 O11 ## 1 one one 92 ## 2 one reviewers 4 ## 3 one mentioned 6 ## 4 one watching 25 ## 5 one just 67 ## 6 one oz 2 ## 7 one episode 14 ## 8 one ll 7 ## 9 one hooked 3 ## 10 one right 12 We now enhance our table by calculating all observed frequencies (O11, O12, O21, O22) as well as row totals (R1, R2), column totals (C1, C2), and the overall total (N). # calculate the total number of observations (N) coll_basic %&gt;% dplyr::mutate(N = sum(O11)) %&gt;% # calculate R1, O12, and R2 dplyr::group_by(w1) %&gt;% dplyr::mutate(R1 = sum(O11), O12 = R1 - O11, R2 = N - R1) %&gt;% dplyr::ungroup(w1) %&gt;% # calculate C1, O21, C2, and O22 dplyr::group_by(w2) %&gt;% dplyr::mutate(C1 = sum(O11), O21 = C1 - O11, C2 = N - C1, O22 = R2 - O21) -&gt; colldf # inspect colldf %&gt;% as.data.frame() %&gt;% head(10) ## w1 w2 O11 N R1 O12 R2 C1 O21 C2 O22 ## 1 one one 92 1899996 15293 15201 1884703 15293 15201 1884703 1869502 ## 2 one reviewers 4 1899996 15293 15289 1884703 194 190 1899802 1884513 ## 3 one mentioned 6 1899996 15293 15287 1884703 418 412 1899578 1884291 ## 4 one watching 25 1899996 15293 15268 1884703 2299 2274 1897697 1882429 ## 5 one just 67 1899996 15293 15226 1884703 7671 7604 1892325 1877099 ## 6 one oz 2 1899996 15293 15291 1884703 145 143 1899851 1884560 ## 7 one episode 14 1899996 15293 15279 1884703 859 845 1899137 1883858 ## 8 one ll 7 1899996 15293 15286 1884703 1518 1511 1898478 1883192 ## 9 one hooked 3 1899996 15293 15290 1884703 101 98 1899895 1884605 ## 10 one right 12 1899996 15293 15281 1884703 1591 1579 1898405 1883124 We could calculate all collocations in the corpus (based on co-occurrence within the same sentence) or we can find collocations of a specific term - here, we will find collocations fo the term movie. Now that we have all the relevant information, we will reduce the data and add additional information to the data so that the computing of the association measures runs smoothly. # reduce and complement data colldf %&gt;% # determine Term dplyr::filter(w1 == &quot;movie&quot;, # set minimum number of occurrences of w2 (O11+O21) &gt; 10, # set minimum number of co-occurrences of w1 and w2 O11 &gt; 5) %&gt;% dplyr::rowwise() %&gt;% dplyr::mutate(E11 = R1 * C1 / N, E12 = R1 * C2 / N, E21 = R2 * C1 / N, E22 = R2 * C2 / N) -&gt; colldf_redux # inspect colldf_redux %&gt;% as.data.frame() %&gt;% head(10) ## w1 w2 O11 N R1 O12 R2 C1 O21 C2 O22 ## 1 movie one 191 1899996 19401 19210 1880595 15293 15102 1884703 1865493 ## 2 movie reviewers 6 1899996 19401 19395 1880595 194 188 1899802 1880407 ## 3 movie mentioned 7 1899996 19401 19394 1880595 418 411 1899578 1880184 ## 4 movie watching 49 1899996 19401 19352 1880595 2299 2250 1897697 1878345 ## 5 movie just 103 1899996 19401 19298 1880595 7671 7568 1892325 1873027 ## 6 movie ll 21 1899996 19401 19380 1880595 1518 1497 1898478 1879098 ## 7 movie right 12 1899996 19401 19389 1880595 1591 1579 1898405 1879016 ## 8 movie exactly 9 1899996 19401 19392 1880595 723 714 1899273 1879881 ## 9 movie first 60 1899996 19401 19341 1880595 5206 5146 1894790 1875449 ## 10 movie thing 28 1899996 19401 19373 1880595 2156 2128 1897840 1878467 ## E11 E12 E21 E22 ## 1 156.157957 19244.84 15136.8420 1865458 ## 2 1.980948 19399.02 192.0191 1880403 ## 3 4.268229 19396.73 413.7318 1880181 ## 4 23.475259 19377.52 2275.5247 1878319 ## 5 78.329150 19322.67 7592.6709 1873002 ## 6 15.500411 19385.50 1502.4996 1879093 ## 7 16.245819 19384.75 1574.7542 1879020 ## 8 7.382607 19393.62 715.6174 1879879 ## 9 53.158852 19347.84 5152.8411 1875442 ## 10 22.015076 19378.98 2133.9849 1878461 Now we can calculate the collocation statistics (the association strength). colldf_redux %&gt;% # determine number of rows dplyr::mutate(Rws = nrow(.)) %&gt;% # work row-wise dplyr::rowwise() %&gt;% # calculate fishers&#39; exact test dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(O11, O12, O21, O22), ncol = 2, byrow = T))[1]))) %&gt;% # extract AM # 1. bias towards top left dplyr::mutate(btl_O12 = ifelse(C1 &gt; R1, 0, R1-C1), btl_O11 = ifelse(C1 &gt; R1, R1, R1-btl_O12), btl_O21 = ifelse(C1 &gt; R1, C1-R1, C1-btl_O11), btl_O22 = ifelse(C1 &gt; R1, C2, C2-btl_O12), # 2. bias towards top right btr_O11 = 0, btr_O21 = R1, btr_O12 = C1, btr_O22 = C2-R1) %&gt;% # 3. calculate AM dplyr::mutate(upp = btl_O11/R1, low = btr_O11/R1, op = O11/R1) %&gt;% dplyr::mutate(AM = op / upp) %&gt;% # remove superfluous columns dplyr::select(-btr_O21, -btr_O12, -btr_O22, -btl_O12, -btl_O11, -btl_O21, -btl_O22, -btr_O11) %&gt;% # extract x2 statistics dplyr::mutate(X2 = (O11-E11)^2/E11 + (O12-E12)^2/E12 + (O21-E21)^2/E21 + (O22-E22)^2/E22) %&gt;% # extract association measures dplyr::mutate(phi = sqrt((X2 / N)), MI = log2(O11 / E11), DeltaP12 = (O11 / (O11 + O12)) - (O21 / (O21 + O22)), DeltaP21 = (O11 / (O11 + O21)) - (O21 / (O12 + O22)), LogOddsRatio = log(((O11 + 0.5) * (O22 + 0.5)) / ( (O12 + 0.5) * (O21 + 0.5) ))) %&gt;% # determine Bonferroni corrected significance dplyr::mutate(Sig_corrected = dplyr::case_when(p / Rws &gt; .05 ~ &quot;n.s.&quot;, p / Rws &gt; .01 ~ &quot;p &lt; .05*&quot;, p / Rws &gt; .001 ~ &quot;p &lt; .01**&quot;, p / Rws &lt;= .001 ~ &quot;p &lt; .001***&quot;, T ~ &quot;N.A.&quot;)) %&gt;% # round p-value dplyr::mutate(p = round(p, 5)) %&gt;% # filter out non significant results dplyr::filter(Sig_corrected != &quot;n.s.&quot;, # filter out instances where the w1 and w2 repel each other E11 &lt; O11) %&gt;% # arrange by phi (association measure) dplyr::arrange(-AM) %&gt;% # remove superfluous columns dplyr::select(-any_of(c(&quot;TermCoocFreq&quot;, &quot;AllFreq&quot;, &quot;NRows&quot;, &quot;E12&quot;, &quot;E21&quot;, &quot;E22&quot;, &quot;O12&quot;, &quot;O21&quot;, &quot;O22&quot;, &quot;R1&quot;, &quot;R2&quot;, &quot;C1&quot;, &quot;C2&quot;))) -&gt; assoc_tb # inspect assoc_tb %&gt;% as.data.frame() %&gt;% head(10) ## w1 w2 O11 N E11 Rws p upp low ## 1 movie dogma 6 1899996 0.4594983 683 0.00001 0.002319468 0 ## 2 movie critic 6 1899996 0.4799205 683 0.00001 0.002422556 0 ## 3 movie horehounds 6 1899996 0.6739309 683 0.00006 0.003401887 0 ## 4 movie depressing 6 1899996 0.6739309 683 0.00006 0.003401887 0 ## 5 movie goriest 7 1899996 0.7964638 683 0.00002 0.004020411 0 ## 6 movie pegg 8 1899996 1.2865953 683 0.00005 0.006494511 0 ## 7 movie porn 16 1899996 2.6446682 683 0.00000 0.013349827 0 ## 8 movie ledger 6 1899996 1.0211074 683 0.00060 0.005154373 0 ## 9 movie laughter 8 1899996 1.3682839 683 0.00008 0.006906860 0 ## 10 movie makers 6 1899996 1.0823739 683 0.00081 0.005463636 0 ## op AM X2 phi MI DeltaP12 ## 1 0.0003092624 0.13333333 67.49661 0.005960252 3.706831 0.0002885243 ## 2 0.0003092624 0.12765957 64.14895 0.005810566 3.644095 0.0002874608 ## 3 0.0003092624 0.09090909 42.52759 0.004731066 3.154290 0.0002773576 ## 4 0.0003092624 0.09090909 42.52759 0.004731066 3.154290 0.0002773576 ## 5 0.0003608061 0.08974359 48.81888 0.005068945 3.135674 0.0003230521 ## 6 0.0004123499 0.06349206 35.39402 0.004316071 2.636442 0.0003496038 ## 7 0.0008246998 0.06177606 68.14826 0.005988955 2.596913 0.0006954853 ## 8 0.0003092624 0.06000000 24.52869 0.003593030 2.554828 0.0002592782 ## 9 0.0004123499 0.05970149 32.47608 0.004134333 2.547632 0.0003453498 ## 10 0.0003092624 0.05660377 22.57436 0.003446921 2.470764 0.0002560877 ## DeltaP21 LogOddsRatio Sig_corrected ## 1 0.13331281 2.769783 p &lt; .001*** ## 2 0.12763799 2.720390 p &lt; .001*** ## 3 0.09087751 2.343430 p &lt; .001*** ## 4 0.09087751 2.343430 p &lt; .001*** ## 5 0.08970622 2.319522 p &lt; .001*** ## 6 0.06342995 1.939496 p &lt; .001*** ## 7 0.06164815 1.882933 p &lt; .001*** ## 8 0.05995052 1.897455 p &lt; .001*** ## 9 0.05963517 1.874163 p &lt; .001*** ## 10 0.05655114 1.835894 p &lt; .001*** The resulting table shows collocations in the example text descending by collocation strength. We now use a network graph, or network for short, to visualise the collocations of our keyword (alice). Networks are a powerful and versatile visual representation used to depict relationships or connections among various elements. Network graphs typically consist of nodes, representing individual entities, and edges, indicating the connections or interactions between these entities. We start by extracting the tokens that we want to show (the top 20 collocates of movie). # sort the coocStatz data frame in descending order based on the &#39;phi&#39; column top20colls &lt;- assoc_tb %&gt;% dplyr::arrange(-phi) %&gt;% # select the top 20 rows after sorting head(20) %&gt;% # extract the &#39;token&#39; column dplyr::pull(w2) %&gt;% # add keyword c(&quot;movie&quot;) # inspect the top 20 tokens with the highest &#39;phi&#39; values top20colls ## [1] &quot;watch&quot; &quot;saw&quot; &quot;see&quot; &quot;good&quot; &quot;watched&quot; ## [6] &quot;seen&quot; &quot;think&quot; &quot;porn&quot; &quot;dogma&quot; &quot;critic&quot; ## [11] &quot;lot&quot; &quot;movies&quot; &quot;goriest&quot; &quot;relations&quot; &quot;really&quot; ## [16] &quot;enjoy&quot; &quot;will&quot; &quot;gore&quot; &quot;horehounds&quot; &quot;depressing&quot; ## [21] &quot;movie&quot; We then need to generate a feature co-occurrence matrix from a document-feature matrix based on the cleaned, lower case sentences of our text. # tokenize the &#39;sentences&#39; data using quanteda package keyword_fcm &lt;- sentences %&gt;% quanteda::tokens(remove_punct = T, remove_symbols = T, remove_numbers = T) %&gt;% # remove stopwords tokens_remove(stopwords(&quot;en&quot;)) %&gt;% # create a document-feature matrix (dfm) using quanteda quanteda::dfm() %&gt;% # select features based on &#39;top20colls&#39; and the term &quot;movie&quot; pattern quanteda::dfm_select(pattern = c(top20colls, &quot;movie&quot;)) %&gt;% # create a feature co-occurrence matrix (fcm) without considering trigrams quanteda::fcm(tri = FALSE) # inspect the first 6 rows and 6 columns of the resulting fcm keyword_fcm[1:6, 1:6] ## Feature co-occurrence matrix of: 6 by 6 features. ## features ## features saw watched think seen really movie ## saw 4 2 4 7 8 57 ## watched 2 2 9 3 6 42 ## think 4 9 18 15 39 106 ## seen 7 3 15 8 14 88 ## really 8 6 39 14 13 114 ## movie 57 42 106 88 114 191 # create a network plot using the fcm quanteda.textplots::textplot_network(keyword_fcm, # set the transparency of edges to 0.8 for visibility edge_alpha = 0.8, # set the color of edges to gray edge_color = &quot;gray&quot;, # set the size of edges to 2 for better visibility edge_size = 2, # adjust the size of vertex labels # based on the logarithm of row sums of the fcm vertex_labelsize = log(rowSums(keyword_fcm))) "],["exercise-3-manual-topic-modelling.html", "Section 8 Exercise 3: Manual Topic Modelling", " Section 8 Exercise 3: Manual Topic Modelling EXERCISE TIME Have a look at the words in the document called ex3_topic.pdf (you can find it in the exercise folder in the data folder). The aim is to determine which of the words belong together and to come up with thematic groups in reviews are positive and which are negative and to identify words that indicate what group a review belongs to. "],["topic-modelling.html", "Section 9 Topic Modelling 9.1 Getting started with Topic Modelling 9.2 Human-in-the-loop Topic Modelling 9.3 Loading and preparing data 9.4 Initial unsupervised topic model 9.5 Supervised, seeded topic model", " Section 9 Topic Modelling Topic models refers to a suit of methods employed to uncover latent structures within a corpus of text. These models operate on the premise of identifying abstract topics that recur across documents. In essence, topic models sift through the textual data to discern recurring patterns of word co-occurrence, revealing underlying semantic themes (Busso et al. 2022; Blei, Ng, and Jordan 2003). This technique is particularly prevalent in text mining, where it serves to unveil hidden semantic structures in large volumes of textual data. Conceptually, topics can be understood as clusters of co-occurring terms, indicative of shared semantic domains within the text. The underlying assumption is that if a document pertains to a specific topic, words related to that topic will exhibit higher frequency compared to documents addressing other subjects. For example, in documents discussing dogs, terms like dog and bone are likely to feature prominently, while in documents focusing on cats, cat and meow would be more prevalent. Meanwhile, ubiquitous terms such as the and is are expected to occur with similar frequency across diverse topics, serving as noise rather than indicative signals of topic specificity. Various methods exist for determining topics within topic models. For instance, Gerlach, Peixoto, and Altmann (2018) and Hyland et al. (2021) advocate for an approach grounded in stochastic block models. However, most applications of topic models use Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) or Structural Topic Modeling (Roberts, Stewart, and Tingley 2016). LDA, in particular, emerges as a widely embraced technique for fitting topic models. It operates by treating each document as a blend of topics and each topic as a blend of words. Consequently, documents can exhibit content overlaps, akin to the fluidity observed in natural language usage, rather than being strictly segregated into distinct groups. Gillings and Hardie (2022) state that topic modelling is based on the following key assumptions: The corpus comprises a substantial number of documents. A topic is delineated as a set of words with varying probabilities of occurrence across the documents. Each document exhibits diverse degrees of association with multiple topics. The collection is structured by underlying topics, which are finite in number, organizing the corpus. Given the availability of vast amounts of textual data, topic models can help to organize and offer insights and assist in understanding large collections of unstructured text and they are widely used in natural language processing and computational text analytics. However, the use of topic modelling in discourse studies has received criticism (Brookes and McEnery 2019) due to the following issues: Thematic Coherence: While topic modeling can group texts into topics, the degree of thematic coherence varies. Some topics may be thematically coherent, but others may lack cohesion or accuracy in capturing the underlying themes present in the texts. Nuanced Perspective: Compared to more traditional approaches to discourse analysis, topic modeling often provides a less nuanced perspective on the data. The automatically generated topics may overlook subtle nuances and intricacies present in the texts, leading to a less accurate representation of the discourse. Distance from Reality: Brookes and McEnery (2019) suggest that the insights derived from topic modeling may not fully capture the “reality” of the texts. The topics generated by the model may not accurately reflect the complex nature of the discourse, leading to potential misinterpretations or oversimplifications of the data. Utility for Discourse Analysts: While topic modeling may offer a method for organizing and studying sizable data sets, Brookes and McEnery (2019) questions the utility for discourse analysts and suggests that traditional discourse analysis methods consistently provide a more nuanced and accurate perspective on the data compared to topic modeling approaches. This criticism is certainly valid if topic modeling is solely reliant on a purely data-driven approach without human intervention. In this tutorial, we will demonstrate how to combine data-driven topic modeling with human-supervised seeded methods to arrive at more reliable and accurate topics. 9.1 Getting started with Topic Modelling In this tutorial, we’ll explore a two-step approach to topic modeling. Initially, we’ll employ an unsupervised method to generate a preliminary topic model, uncovering inherent topics within the data. Subsequently, we’ll introduce a human-supervised, seeded model, informed by the outcomes of the initial data-driven approach. Following this (recommended) procedure, we’ll then delve into an alternative purely data-driven approach. Our tutorial begins by gathering the necessary corpus data. We’ll be focusing on analyzing the State of the Union Addresses (SOTU) delivered by US presidents, with the aim of understanding how the addressed topics have evolved over time. Given the length of these addresses (amounting to 231 in total), it’s important to acknowledge that document length can influence topic modeling outcomes. In cases where texts are exceptionally short (like Twitter posts) or long (such as books), adjusting the document units for modeling purposes can be beneficial—either by combining or splitting them accordingly. To tailor our approach to the SOTU speeches, we’ve chosen to model at the paragraph level instead of analyzing entire speeches at once. This allows for a more detailed analysis, potentially leading to clearer and more interpretable topics. We’ve provided a data set named sotu_paragraphs.rda, which contains the speeches segmented into paragraphs for easier analysis. 9.2 Human-in-the-loop Topic Modelling In this human-in-the-loop approach to topic modelling which mainly uses and combines the quanteda package (Benoit et al. 2018), the topicmodels package (Grün and Hornik 2024, 2011), and the seededlda package (Watanabe and Xuan-Hieu 2024). Now that we have cleaned the data, we can perform the topic modelling. This consists of two steps: First, we perform an unsupervised LDA. We do this to check what topics are in our corpus. Then, we perform a supervised LDA (based on the results of the unsupervised LDA) to identify meaningful topics in our data. For the supervised LDA, we define so-called seed terms that help in generating coherent topics. 9.3 Loading and preparing data When preparing the data for analysis, we employ several preprocessing steps to ensure its cleanliness and readiness for analysis. Initially, we load the data and then remove punctuation, symbols, and numerical characters. Additionally, we eliminate common stop words, such as the and and, which can introduce noise and hinder the topic modeling process. To standardize the text, we convert it to lowercase and, lastly, we apply stemming to reduce words to their base form. # load data txts &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/sotu_paragraphs.rda&quot;, &quot;rb&quot;)) txts$text %&gt;% # tokenize quanteda::tokens(remove_punct = TRUE, # remove punctuation remove_symbols = TRUE, # remove symbols remove_number = TRUE) %&gt;% # remove numbers # remove stop words quanteda::tokens_select(pattern = stopwords(&quot;en&quot;), selection = &quot;remove&quot;) %&gt;% # stemming quanteda::tokens_wordstem() %&gt;% # convert to document-frequency matrix quanteda::dfm(tolower = T) -&gt; ctxts # add docvars docvars(ctxts, &quot;president&quot;) &lt;- txts$president docvars(ctxts, &quot;date&quot;) &lt;- txts$date docvars(ctxts, &quot;speechid&quot;) &lt;- txts$speech_doc_id docvars(ctxts, &quot;docid&quot;) &lt;- txts$doc_id # clean data ctxts &lt;- dfm_subset(ctxts, ntoken(ctxts) &gt; 0) # inspect data ctxts[1:5, 1:5] ## Document-feature matrix of: 5 documents, 5 features (80.00% sparse) and 4 docvars. ## features ## docs fellow-citizen senat hous repres embrac ## text1 1 1 1 1 0 ## text2 0 0 0 0 1 ## text3 0 0 0 0 0 ## text4 0 0 0 0 0 ## text5 0 0 0 0 0 9.4 Initial unsupervised topic model Now that we have loaded and prepared the data for analysis, we will follow a two-step approach. First, we perform an unsupervised topic model using Latent Dirichlet Allocation (LDA) to identify the topics present in our data. This initial step helps us understand the broad themes and structure within the data set. Then, based on the results of the unsupervised topic model, we conduct a supervised topic model using LDA to refine and identify more meaningful topics in our data. This combined approach allows us to leverage both data-driven insights and expert supervision to enhance the accuracy and interpretability of the topics. In the initial step that implements a unsupervised, data-driven topic model, we vary the number of topics the LDA algorithm looks for until we identify coherent topics in the data. We use the LDA function from the topicmodels package instead of the textmodel_lda function from the seededlda package because the former allows us to include a seed. Including a seed ensures that the results of this unsupervised topic model are reproducible, which is not the case if we do not seed the model, as each model will produce different results without setting a seed. # generate model: change k to different numbers, e.g. 10 or 20 and look for consistencies in the keywords for the topics below. topicmodels::LDA(ctxts, k = 15, control = list(seed = 1234)) -&gt; ddlda Now that we have generated an initial data-driven model, the next step is to inspect it to evaluate its performance and understand the topics it has identified. To do this, we need to examine the terms associated with each detected topic. By analyzing these terms, we can gain insights into the themes represented by each topic and assess the coherence and relevance of the model’s output. # define number of topics ntopics = 15 # define number of terms nterms = 10 # generate table tidytext::tidy(ddlda, matrix = &quot;beta&quot;) %&gt;% dplyr::group_by(topic) %&gt;% dplyr::slice_max(beta, n = nterms) %&gt;% dplyr::ungroup() %&gt;% dplyr::arrange(topic, -beta) %&gt;% dplyr::mutate(term = paste(term, &quot; (&quot;, round(beta, 3), &quot;)&quot;, sep = &quot;&quot;), topic = paste(&quot;topic&quot;, topic), topic = factor(topic, levels = c(paste(&quot;topic&quot;, 1:ntopics))), top = rep(paste(&quot;top&quot;, 1:nterms), nrow(.)/nterms), top = factor(top, levels = c(paste(&quot;top&quot;, 1:nterms)))) %&gt;% dplyr::select(-beta) %&gt;% tidyr::spread(topic, term) -&gt; ddlda_top_terms ddlda_top_terms ## # A tibble: 10 × 16 ## top `topic 1` `topic 2` `topic 3` `topic 4` `topic 5` `topic 6` `topic 7` ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 top 1 state (0.… countri … state (0… govern (… state (0… state (0… state (0… ## 2 top 2 countri (… upon (0.… unite (0… will (0.… countri … will (0.… upon (0.… ## 3 top 3 will (0.0… present … congress… year (0.… will (0.… unite (0… will (0.… ## 4 top 4 congress … war (0.0… may (0.0… unite (0… congress… govern (… congress… ## 5 top 5 nation (0… can (0.0… treati (… law (0.0… public (… power (0… may (0.0… ## 6 top 6 can (0.00… unite (0… citizen … may (0.0… year (0.… law (0.0… govern (… ## 7 top 7 subject (… nation (… nation (… upon (0.… nation (… peopl (0… citizen … ## 8 top 8 govern (0… author (… great (0… act (0.0… can (0.0… last (0.… nation (… ## 9 top 9 land (0.0… may (0.0… territor… public (… law (0.0… duti (0.… import (… ## 10 top 10 made (0.0… subject … made (0.… last (0.… import (… part (0.… great (0… ## # ℹ 8 more variables: `topic 8` &lt;chr&gt;, `topic 9` &lt;chr&gt;, `topic 10` &lt;chr&gt;, ## # `topic 11` &lt;chr&gt;, `topic 12` &lt;chr&gt;, `topic 13` &lt;chr&gt;, `topic 14` &lt;chr&gt;, ## # `topic 15` &lt;chr&gt; In a real analysis, we would re-run the unsupervised model multiple times, adjusting the number of topics that the Latent Dirichlet Allocation (LDA) algorithm “looks for.” For each iteration, we would inspect the key terms associated with the identified topics to check their thematic consistency. This evaluation helps us determine whether the results of the topic model make sense and accurately reflect the themes present in the data. By varying the number of topics and examining the corresponding key terms, we can identify the optimal number of topics that best represent the underlying themes in our data set. However, we will skip re-running the model here, as this is just a tutorial intended to showcase the process rather than a comprehensive analysis. To obtain a comprehensive table of terms and their association strengths with topics (the beta values), follow the steps outlined below. This table can help verify if the data contains thematically distinct topics. Additionally, visualizations and statistical modeling can be employed to compare the distinctness of topics and determine the ideal number of topics. However, I strongly recommend not solely relying on statistical measures when identifying the optimal number of topics. In my experience, human intuition is still essential for evaluating topic coherence and consistency. # extract topics ddlda_topics &lt;- tidy(ddlda, matrix = &quot;beta&quot;) # inspect head(ddlda_topics, 20) ## # A tibble: 20 × 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 fellow-citizen 0.000249 ## 2 2 fellow-citizen 0.000351 ## 3 3 fellow-citizen 0.000416 ## 4 4 fellow-citizen 0.0000333 ## 5 5 fellow-citizen 0.0000797 ## 6 6 fellow-citizen 0.000183 ## 7 7 fellow-citizen 0.000445 ## 8 8 fellow-citizen 0.000306 ## 9 9 fellow-citizen 0.000381 ## 10 10 fellow-citizen 0.000332 ## 11 11 fellow-citizen 0.000187 ## 12 12 fellow-citizen 0.000147 ## 13 13 fellow-citizen 0.000278 ## 14 14 fellow-citizen 0.000336 ## 15 15 fellow-citizen 0.000205 ## 16 1 senat 0.000708 ## 17 2 senat 0.000477 ## 18 3 senat 0.00263 ## 19 4 senat 0.00118 ## 20 5 senat 0.000436 The purpose of this initial step, in which we generate data-driven unsupervised topic models, is to identify the number of coherent topics present in the data and to determine the key terms associated with these topics. These key terms will then be used as seed terms in the next step: the supervised, seeded topic model. This approach ensures that the supervised model is grounded in the actual thematic structure of the data set, enhancing the accuracy and relevance of the identified topics. 9.5 Supervised, seeded topic model To implement the supervised, seeded topic model, we start by creating a dictionary containing the seed terms we have identified in the first step. To check terms (to see if ), you can use the following code chunk: ddlda_topics %&gt;% select(term) %&gt;% unique() %&gt;% filter(str_detect(term, &quot;agri&quot;)) ## # A tibble: 3 × 1 ## term ## &lt;chr&gt; ## 1 agricultur ## 2 agriculturist ## 3 agricultural-colleg # semisupervised LDA dict &lt;- dictionary(list(military = c(&quot;armi&quot;, &quot;war&quot;, &quot;militari&quot;, &quot;conflict&quot;), liberty = c(&quot;freedom&quot;, &quot;liberti&quot;, &quot;free&quot;), nation = c(&quot;nation&quot;, &quot;countri&quot;, &quot;citizen&quot;), law = c(&quot;law&quot;, &quot;court&quot;, &quot;prison&quot;), treaty = c(&quot;claim&quot;, &quot;treati&quot;, &quot;negoti&quot;), indian = c(&quot;indian&quot;, &quot;tribe&quot;, &quot;territori&quot;), labor = c(&quot;labor&quot;, &quot;work&quot;, &quot;condit&quot;), money = c(&quot;bank&quot;, &quot;silver&quot;, &quot;gold&quot;, &quot;currenc&quot;, &quot;money&quot;), finance = c(&quot;debt&quot;, &quot;invest&quot;, &quot;financ&quot;), wealth = c(&quot;prosper&quot;, &quot;peac&quot;, &quot;wealth&quot;), industry = c(&quot;produc&quot;, &quot;industri&quot;, &quot;manufactur&quot;), navy = c(&quot;navi&quot;, &quot;ship&quot;, &quot;vessel&quot;, &quot;naval&quot;), consitution = c(&quot;constitut&quot;, &quot;power&quot;, &quot;state&quot;), agriculture = c(&quot;agricultur&quot;, &quot;grow&quot;, &quot;land&quot;), office = c(&quot;office&quot;, &quot;serv&quot;, &quot;duti&quot;))) tmod_slda &lt;- seededlda::textmodel_seededlda(ctxts, dict, residual = TRUE, min_termfreq = 2) # inspect seededlda::terms(tmod_slda) ## military liberty nation law treaty indian ## [1,] &quot;war&quot; &quot;free&quot; &quot;countri&quot; &quot;law&quot; &quot;treati&quot; &quot;territori&quot; ## [2,] &quot;militari&quot; &quot;peopl&quot; &quot;nation&quot; &quot;court&quot; &quot;claim&quot; &quot;indian&quot; ## [3,] &quot;armi&quot; &quot;can&quot; &quot;citizen&quot; &quot;case&quot; &quot;govern&quot; &quot;tribe&quot; ## [4,] &quot;forc&quot; &quot;govern&quot; &quot;govern&quot; &quot;upon&quot; &quot;negoti&quot; &quot;mexico&quot; ## [5,] &quot;offic&quot; &quot;must&quot; &quot;foreign&quot; &quot;person&quot; &quot;unite&quot; &quot;part&quot; ## [6,] &quot;servic&quot; &quot;upon&quot; &quot;american&quot; &quot;offic&quot; &quot;minist&quot; &quot;new&quot; ## [7,] &quot;men&quot; &quot;liberti&quot; &quot;right&quot; &quot;provis&quot; &quot;two&quot; &quot;texa&quot; ## [8,] &quot;command&quot; &quot;public&quot; &quot;time&quot; &quot;may&quot; &quot;relat&quot; &quot;line&quot; ## [9,] &quot;conflict&quot; &quot;everi&quot; &quot;just&quot; &quot;execut&quot; &quot;convent&quot; &quot;boundari&quot; ## [10,] &quot;order&quot; &quot;great&quot; &quot;protect&quot; &quot;made&quot; &quot;britain&quot; &quot;now&quot; ## labor money finance wealth industry navy ## [1,] &quot;condit&quot; &quot;bank&quot; &quot;year&quot; &quot;peac&quot; &quot;produc&quot; &quot;vessel&quot; ## [2,] &quot;work&quot; &quot;money&quot; &quot;debt&quot; &quot;prosper&quot; &quot;industri&quot; &quot;navi&quot; ## [3,] &quot;labor&quot; &quot;gold&quot; &quot;amount&quot; &quot;will&quot; &quot;manufactur&quot; &quot;ship&quot; ## [4,] &quot;report&quot; &quot;currenc&quot; &quot;expenditur&quot; &quot;us&quot; &quot;import&quot; &quot;naval&quot; ## [5,] &quot;depart&quot; &quot;silver&quot; &quot;treasuri&quot; &quot;peopl&quot; &quot;product&quot; &quot;coast&quot; ## [6,] &quot;congress&quot; &quot;govern&quot; &quot;increas&quot; &quot;great&quot; &quot;foreign&quot; &quot;port&quot; ## [7,] &quot;secretari&quot; &quot;treasuri&quot; &quot;fiscal&quot; &quot;interest&quot; &quot;trade&quot; &quot;construct&quot; ## [8,] &quot;servic&quot; &quot;public&quot; &quot;last&quot; &quot;everi&quot; &quot;increas&quot; &quot;sea&quot; ## [9,] &quot;attent&quot; &quot;issu&quot; &quot;estim&quot; &quot;may&quot; &quot;upon&quot; &quot;great&quot; ## [10,] &quot;recommend&quot; &quot;note&quot; &quot;revenu&quot; &quot;happi&quot; &quot;articl&quot; &quot;commerc&quot; ## consitution agriculture office other ## [1,] &quot;state&quot; &quot;land&quot; &quot;duti&quot; &quot;congress&quot; ## [2,] &quot;power&quot; &quot;agricultur&quot; &quot;will&quot; &quot;act&quot; ## [3,] &quot;constitut&quot; &quot;public&quot; &quot;may&quot; &quot;last&quot; ## [4,] &quot;unite&quot; &quot;grow&quot; &quot;subject&quot; &quot;repres&quot; ## [5,] &quot;govern&quot; &quot;larg&quot; &quot;can&quot; &quot;session&quot; ## [6,] &quot;right&quot; &quot;improv&quot; &quot;congress&quot; &quot;presid&quot; ## [7,] &quot;union&quot; &quot;will&quot; &quot;consider&quot; &quot;senat&quot; ## [8,] &quot;shall&quot; &quot;made&quot; &quot;measur&quot; &quot;hous&quot; ## [9,] &quot;one&quot; &quot;acr&quot; &quot;object&quot; &quot;upon&quot; ## [10,] &quot;act&quot; &quot;reserv&quot; &quot;shall&quot; &quot;day&quot; Now, we extract files and create a data frame of topics and documents. This shows what topic is dominant in which file in tabular form. # generate data frame data.frame(tmod_slda$data$date, tmod_slda$data$president, seededlda::topics(tmod_slda)) %&gt;% dplyr::rename(Date = 1, President = 2, Topic = 3) %&gt;% dplyr::mutate(Date = stringr::str_remove_all(Date, &quot;-.*&quot;), Date = stringr::str_replace_all(Date, &quot;.$&quot;, &quot;0&quot;)) %&gt;% dplyr::mutate_if(is.character, factor) -&gt; topic_df # inspect head(topic_df) ## Date President Topic ## text1 1790 George Washington other ## text2 1790 George Washington wealth ## text3 1790 George Washington wealth ## text4 1790 George Washington wealth ## text5 1790 George Washington office ## text6 1790 George Washington office Using the table (or data frame) we have just created, we can visualize the use of topics over time. topic_df %&gt;% dplyr::group_by(Date, Topic) %&gt;% dplyr::summarise(freq = n()) %&gt;% ggplot(aes(x = Date, y = freq, fill = Topic)) + geom_bar(stat=&quot;identity&quot;, position=&quot;fill&quot;, color = &quot;black&quot;) + theme_bw() + labs(x = &quot;Decade&quot;) + scale_fill_manual(values = rev(colorRampPalette(brewer.pal(8, &quot;RdBu&quot;))(ntopics+1))) + scale_y_continuous(name =&quot;Percent of paragraphs&quot;, labels = seq(0, 100, 25)) ## `summarise()` has grouped output by &#39;Date&#39;. You can override using the ## `.groups` argument. The figure illustrates the relative frequency of topics over time in the State of the Union (SOTU) texts. Notably, paragraphs discussing the topic of “office,” characterized by key terms such as office, serv, and duti, have become less prominent over time. This trend suggests a decreasing emphasis on this particular theme, as evidenced by the diminishing number of paragraphs dedicated to it. "],["network-analysis.html", "Section 10 Network Analysis 10.1 Data preparation 10.2 Creating a matrix 10.3 Visualizing Networks 10.4 Network Statistics", " Section 10 Network Analysis In R, there are several packages that provide essential tools for constructing, analyzing, and visualizing networks. In this section we will focus on the igraph, tidygraph, and ggraph packages. We will use these packages to generate a tidy network with is rather complex but also offers great flexibility and options for customization. While the quantada.textplot package is much easier to use, it does not allow to extract network statistics easily. To showcase how to prepare and generate network graphs, we will visualize the network of collocations and the characters in William Shakespeare’s Romeo and Juliet. 10.1 Data preparation If one does not generate a network based on text data, it’s recommendable to have at least one table indicating the start and end points of edges (lines connecting nodes). Additionally, two additional tables providing information on node size/type and edge size/type are valuable. In the upcoming sections, we’ll create these tables from raw data. Alternatively, you can generate network graphs by uploading tables containing the necessary information. We’ll generate a network showing the frequency of characters in William Shakespeare’s Romeo and Juliet appearing in the same scene. Our focus is on investigating the networks of personas in Shakespeare’s Romeo and Juliet, and thus, we’ll load this renowned work of fiction. 10.2 Creating a matrix We start by loading the data which represents a table that contains the personas that are present during a sub-scene as well as how many contributions they make and how often they occur. # load data net_dat &lt;- read.delim(&quot;https://slcladal.github.io/data/romeo_tidy.txt&quot;, sep = &quot;\\t&quot;) # inspect data net_dat %&gt;% as.data.frame() %&gt;% head(15) ## actscene person contrib occurrences ## 1 ACT I_SCENE I BENVOLIO 24 7 ## 2 ACT I_SCENE I CAPULET 2 9 ## 3 ACT I_SCENE I FIRST CITIZEN 1 2 ## 4 ACT I_SCENE I LADY CAPULET 1 10 ## 5 ACT I_SCENE I MONTAGUE 6 3 ## 6 ACT I_SCENE I PRINCE 1 3 ## 7 ACT I_SCENE I ROMEO 16 14 ## 8 ACT I_SCENE I TYBALT 2 3 ## 9 ACT I_SCENE II BENVOLIO 5 7 ## 10 ACT I_SCENE II CAPULET 3 9 ## 11 ACT I_SCENE II PARIS 2 5 ## 12 ACT I_SCENE II ROMEO 11 14 ## 13 ACT I_SCENE II SERVANT 8 3 ## 14 ACT I_SCENE III JULIET 5 11 ## 15 ACT I_SCENE III LADY CAPULET 11 10 We now transform that table into a co-occurrence matrix. net_cmx &lt;- crossprod(table(net_dat[1:2])) diag(net_cmx) &lt;- 0 net_df &lt;- as.data.frame(net_cmx) # inspect data net_df[1:5, 1:5]%&gt;% as.data.frame() %&gt;% tibble::rownames_to_column(&quot;Persona&quot;) ## Persona BALTHASAR BENVOLIO CAPULET FIRST CITIZEN FIRST SERVANT ## 1 BALTHASAR 0 0 1 0 0 ## 2 BENVOLIO 0 0 3 2 1 ## 3 CAPULET 1 3 0 1 2 ## 4 FIRST CITIZEN 0 2 1 0 0 ## 5 FIRST SERVANT 0 1 2 0 0 The data shows how often a character has appeared with each other character in the play - only Friar Lawrence and Friar John were excluded because they only appear in one scene where they talk to each other. 10.3 Visualizing Networks There are various different ways to visualize a network structure. We will focus on a method for generating networks that is extremely flexible. First, we define the nodes and we can also add information about the nodes that we can use later on (such as frequency information). # create a new data frame &#39;va&#39; using the &#39;net_dat&#39; data net_dat %&gt;% # rename the &#39;person&#39; column to &#39;node&#39; and &#39;occurrences&#39; column to &#39;n&#39; dplyr::rename(node = person, n = occurrences) %&gt;% # group the data by the &#39;node&#39; column dplyr::group_by(node) %&gt;% # summarize the data, calculating the total occurrences (&#39;n&#39;) for each &#39;node&#39; dplyr::summarise(n = sum(n)) -&gt; va # inspect va %&gt;% as.data.frame() %&gt;% head(10) ## node n ## 1 BALTHASAR 4 ## 2 BENVOLIO 49 ## 3 CAPULET 81 ## 4 FIRST CITIZEN 4 ## 5 FIRST SERVANT 4 ## 6 FRIAR LAWRENCE 49 ## 7 JULIET 121 ## 8 LADY CAPULET 100 ## 9 MERCUTIO 16 ## 10 MONTAGUE 9 The next part is optional but it can help highlight important information. We add a column with additional information to our nodes table. # define family mon &lt;- c(&quot;ABRAM&quot;, &quot;BALTHASAR&quot;, &quot;BENVOLIO&quot;, &quot;LADY MONTAGUE&quot;, &quot;MONTAGUE&quot;, &quot;ROMEO&quot;) cap &lt;- c(&quot;CAPULET&quot;, &quot;CAPULET’S COUSIN&quot;, &quot;FIRST SERVANT&quot;, &quot;GREGORY&quot;, &quot;JULIET&quot;, &quot;LADY CAPULET&quot;, &quot;NURSE&quot;, &quot;PETER&quot;, &quot;SAMPSON&quot;, &quot;TYBALT&quot;) oth &lt;- c(&quot;APOTHECARY&quot;, &quot;CHORUS&quot;, &quot;FIRST CITIZEN&quot;, &quot;FIRST MUSICIAN&quot;, &quot;FIRST WATCH&quot;, &quot;FRIAR JOHN&quot; , &quot;FRIAR LAWRENCE&quot;, &quot;MERCUTIO&quot;, &quot;PAGE&quot;, &quot;PARIS&quot;, &quot;PRINCE&quot;, &quot;SECOND MUSICIAN&quot;, &quot;SECOND SERVANT&quot;, &quot;SECOND WATCH&quot;, &quot;SERVANT&quot;, &quot;THIRD MUSICIAN&quot;) # create color vectors va &lt;- va %&gt;% dplyr::mutate(type = dplyr::case_when(node %in% mon ~ &quot;MONTAGUE&quot;, node %in% cap ~ &quot;CAPULET&quot;, TRUE ~ &quot;Other&quot;)) # inspect updates nodes table head(va, 10) ## # A tibble: 10 × 3 ## node n type ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 BALTHASAR 4 MONTAGUE ## 2 BENVOLIO 49 MONTAGUE ## 3 CAPULET 81 CAPULET ## 4 FIRST CITIZEN 4 Other ## 5 FIRST SERVANT 4 CAPULET ## 6 FRIAR LAWRENCE 49 Other ## 7 JULIET 121 CAPULET ## 8 LADY CAPULET 100 CAPULET ## 9 MERCUTIO 16 Other ## 10 MONTAGUE 9 MONTAGUE Now, we define the edges, i.e., the connections between nodes and, again, we can add information in separate variables that we can use later on. # create a new data frame &#39;ed&#39; using the &#39;dat&#39; data ed &lt;- net_df %&gt;% # add a new column &#39;from&#39; with row names dplyr::mutate(from = rownames(.)) %&gt;% # reshape the data from wide to long format using &#39;gather&#39; tidyr::gather(to, n, BALTHASAR:TYBALT) %&gt;% # remove zero frequencies dplyr::filter(n != 0) # inspect ed %&gt;% as.data.frame() %&gt;% head(10) ## from to n ## 1 CAPULET BALTHASAR 1 ## 2 FRIAR LAWRENCE BALTHASAR 1 ## 3 JULIET BALTHASAR 1 ## 4 LADY CAPULET BALTHASAR 1 ## 5 MONTAGUE BALTHASAR 1 ## 6 PARIS BALTHASAR 1 ## 7 PRINCE BALTHASAR 1 ## 8 ROMEO BALTHASAR 2 ## 9 CAPULET BENVOLIO 3 ## 10 FIRST CITIZEN BENVOLIO 2 Now that we have generated tables for the edges and the nodes, we can generate a graph object. ig &lt;- igraph::graph_from_data_frame(d=ed, vertices=va, directed = FALSE) We will also add labels to the nodes as follows: tg &lt;- tidygraph::as_tbl_graph(ig) %&gt;% tidygraph::activate(nodes) %&gt;% dplyr::mutate(label=name) When we now plot our network, it looks as shown below. # set seed (so that the exact same network graph is created every time) set.seed(12345) # create a graph using the &#39;tg&#39; data frame with the Fruchterman-Reingold layout tg %&gt;% ggraph::ggraph(layout = &quot;fr&quot;) + # add arcs for edges with various aesthetics geom_edge_arc(colour = &quot;gray50&quot;, lineend = &quot;round&quot;, strength = .1, aes(edge_width = ed$n, alpha = ed$n)) + # add points for nodes with size based on log-transformed &#39;v.size&#39; and color based on &#39;va$Family&#39; geom_node_point(size = log(va$n) * 2, aes(color = va$type)) + # add text labels for nodes with various aesthetics geom_node_text(aes(label = name), repel = TRUE, point.padding = unit(0.2, &quot;lines&quot;), size = sqrt(va$n), colour = &quot;gray10&quot;) + # adjust edge width and alpha scales scale_edge_width(range = c(0, 2.5)) + scale_edge_alpha(range = c(0, .3)) + # set graph background color to white theme_graph(background = &quot;white&quot;) + # adjust legend position to the top theme(legend.position = &quot;top&quot;, # suppress legend title legend.title = element_blank()) + # remove edge width and alpha guides from the legend guides(edge_width = FALSE, edge_alpha = FALSE) 10.4 Network Statistics In addition to visualizing networks, we will analyze the network and extract certain statistics about the network that tell us about structural properties of networks. To extract the statistics, we use the edge object generated above (called ed) and then repeat each combination as often as it occurred based on the value in the Frequency column. dg &lt;- ed[rep(seq_along(ed$n), ed$n), 1:2] rownames(dg) &lt;- NULL # inspect data dg %&gt;% as.data.frame() %&gt;% head(10) ## from to ## 1 CAPULET BALTHASAR ## 2 FRIAR LAWRENCE BALTHASAR ## 3 JULIET BALTHASAR ## 4 LADY CAPULET BALTHASAR ## 5 MONTAGUE BALTHASAR ## 6 PARIS BALTHASAR ## 7 PRINCE BALTHASAR ## 8 ROMEO BALTHASAR ## 9 ROMEO BALTHASAR ## 10 CAPULET BENVOLIO 10.4.1 Degree centrality We now generate an edge list from the dg object and then extract the degree centrality. The degree centrality reflects how many edges each node has with the most central node having the highest value. dgg &lt;- graph.edgelist(as.matrix(dg), directed = T) # extract degree centrality igraph::degree(dgg) %&gt;% as.data.frame() %&gt;% tibble::rownames_to_column(&quot;node&quot;) %&gt;% dplyr::rename(`degree centrality` = 2) %&gt;% dplyr::arrange(-`degree centrality`) -&gt; dc_tbl # inspect data dc_tbl %&gt;% as.data.frame() %&gt;% head(10) ## node degree centrality ## 1 ROMEO 108 ## 2 CAPULET 92 ## 3 LADY CAPULET 90 ## 4 NURSE 76 ## 5 JULIET 72 ## 6 BENVOLIO 68 ## 7 MONTAGUE 44 ## 8 PRINCE 44 ## 9 TYBALT 44 ## 10 PARIS 42 10.4.2 Central node Next, we extract the most central node. names(igraph::degree(dgg))[which(igraph::degree(dgg) == max(igraph::degree(dgg)))] ## [1] &quot;ROMEO&quot; 10.4.3 Betweenness centrality We now extract the betweenness centrality. Betweenness centrality provides a measure of how important nodes are for information flow between nodes in a network. The node with the highest betweenness centrality creates the shortest paths in the network. The higher a node’s betweenness centrality, the more important it is for the efficient flow of goods in a network. igraph::betweenness(dgg) %&gt;% as.data.frame() %&gt;% tibble::rownames_to_column(&quot;node&quot;) %&gt;% dplyr::rename(`betweenness centrality` = 2) %&gt;% dplyr::arrange(-`betweenness centrality`) -&gt; bc_tbl # inspect data bc_tbl %&gt;% as.data.frame() %&gt;% head(10) ## node betweenness centrality ## 1 ROMEO 27.624370 ## 2 LADY CAPULET 16.276864 ## 3 CAPULET 15.623219 ## 4 BENVOLIO 9.615121 ## 5 NURSE 7.401454 ## 6 JULIET 5.554710 ## 7 TYBALT 3.199408 ## 8 MONTAGUE 2.182203 ## 9 PRINCE 2.182203 ## 10 PARIS 1.859429 10.4.4 Closeness In addition, we extract the closeness statistic of all edges in the dg object by using the closeness function from the igraph package. Closeness centrality refers to the shortest paths between nodes. The distance between two nodes represents the length of the shortest path between them. The closeness of a node is the average distance from that node to all other nodes. igraph::closeness(dgg) %&gt;% as.data.frame() %&gt;% tibble::rownames_to_column(&quot;node&quot;) %&gt;% dplyr::rename(closeness = 2) %&gt;% dplyr::arrange(-closeness) -&gt; c_tbl # inspect data c_tbl %&gt;% as.data.frame() %&gt;% head(10) ## node closeness ## 1 LADY CAPULET 0.05882353 ## 2 ROMEO 0.05882353 ## 3 CAPULET 0.05555556 ## 4 BENVOLIO 0.05263158 ## 5 JULIET 0.05000000 ## 6 NURSE 0.04761905 ## 7 TYBALT 0.04761905 ## 8 MONTAGUE 0.04545455 ## 9 PARIS 0.04545455 ## 10 PRINCE 0.04545455 "],["citation-session-info.html", "Section 11 Citation &amp; Session Info", " Section 11 Citation &amp; Session Info Schweinberger, Martin. 2024. Introduction to R for Social Science. University of Eastern Finland, Joensuu. url: https://martinschweinberger.github.io/IntroR_WS (Version 2024.06.11). @manual{schweinberger2024introrss, author = {Schweinberger, Martin}, title = {Introduction to R for Social Science}, note = {https://martinschweinberger.github.io/IntroR_WS}, year = {2024}, organization = {University of Eastern Finland}, address = {Joensuu}, edition = {2024.06.11} } sessionInfo() ## R version 4.3.2 (2023-10-31 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 11 x64 (build 22621) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=English_Australia.utf8 LC_CTYPE=English_Australia.utf8 ## [3] LC_MONETARY=English_Australia.utf8 LC_NUMERIC=C ## [5] LC_TIME=English_Australia.utf8 ## ## time zone: Australia/Brisbane ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices datasets utils methods base ## ## other attached packages: ## [1] wordcloud2_0.2.1 wordcloud_2.6 ## [3] udpipe_0.8.11 topicmodels_0.2-16 ## [5] tm_0.7-13 NLP_0.2-1 ## [7] lubridate_1.9.3 forcats_1.0.0 ## [9] stringr_1.5.1 dplyr_1.1.4 ## [11] purrr_1.0.2 readr_2.1.5 ## [13] tidyr_1.3.1 tibble_3.2.1 ## [15] tidyverse_2.0.0 tidytext_0.4.2 ## [17] tidygraph_1.3.1 textdata_0.4.5 ## [19] sna_2.7-2 statnet.common_4.9.0 ## [21] slam_0.1-50 reshape2_1.4.4 ## [23] RColorBrewer_1.1-3 quanteda.textstats_0.97 ## [25] quanteda.textplots_0.94.4 quanteda_4.0.2 ## [27] network_1.18.2 Matrix_1.6-5 ## [29] ldatuning_1.0.2 lda_1.5.2 ## [31] igraph_2.0.3 gutenbergr_0.2.4 ## [33] ggraph_2.2.1 GGally_2.2.1 ## [35] ggplot2_3.5.0 flextable_0.9.6 ## ## loaded via a namespace (and not attached): ## [1] rstudioapi_0.16.0 jsonlite_1.8.8 magrittr_2.0.3 ## [4] modeltools_0.2-23 farver_2.1.1 rmarkdown_2.26 ## [7] fs_1.6.3 ragg_1.3.0 vctrs_0.6.5 ## [10] memoise_2.0.1 askpass_1.2.0 htmltools_0.5.8 ## [13] curl_5.2.1 janeaustenr_1.0.0 sass_0.4.9 ## [16] bslib_0.7.0 htmlwidgets_1.6.4 tokenizers_0.3.0 ## [19] plyr_1.8.9 cachem_1.0.8 uuid_1.2-0 ## [22] mime_0.12 lifecycle_1.0.4 pkgconfig_2.0.3 ## [25] R6_2.5.1 fastmap_1.1.1 shiny_1.8.1 ## [28] digest_0.6.35 colorspace_2.1-0 rprojroot_2.0.4 ## [31] textshaping_0.3.7 SnowballC_0.7.1 labeling_0.4.3 ## [34] fansi_1.0.6 timechange_0.3.0 polyclip_1.10-6 ## [37] compiler_4.3.2 here_1.0.1 fontquiver_0.2.1 ## [40] withr_3.0.0 viridis_0.6.5 ggstats_0.6.0 ## [43] highr_0.10 ggforce_0.4.2 MASS_7.3-60 ## [46] openssl_2.1.1 proxyC_0.3.4 rappdirs_0.3.3 ## [49] gfonts_0.2.0 tools_4.3.2 stopwords_2.3 ## [52] zip_2.3.1 httpuv_1.6.15 glue_1.7.0 ## [55] promises_1.2.1 grid_4.3.2 generics_0.1.3 ## [58] gtable_0.3.4 tzdb_0.4.0 data.table_1.15.2 ## [61] hms_1.1.3 xml2_1.3.6 utf8_1.2.4 ## [64] ggrepel_0.9.5 pillar_1.9.0 nsyllable_1.0.1 ## [67] later_1.3.2 tweenr_2.0.3 lattice_0.21-9 ## [70] klippy_0.0.0.9500 renv_1.0.5 tidyselect_1.2.1 ## [73] fontLiberation_0.1.0 knitr_1.45 fontBitstreamVera_0.1.1 ## [76] gridExtra_2.3 bookdown_0.38 crul_1.4.0 ## [79] stats4_4.3.2 xfun_0.43 graphlayouts_1.1.1 ## [82] stringi_1.8.3 yaml_2.3.8 evaluate_0.23 ## [85] httpcode_0.3.0 officer_0.6.5 gdtools_0.3.7 ## [88] cli_3.6.2 RcppParallel_5.1.7 xtable_1.8-4 ## [91] systemfonts_1.0.6 munsell_0.5.0 jquerylib_0.1.4 ## [94] Rcpp_1.0.12 coda_0.19-4.1 parallel_4.3.2 ## [97] assertthat_0.2.1 viridisLite_0.4.2 scales_1.3.0 ## [100] crayon_1.5.2 rlang_1.1.3 fastmatch_1.1-4 ## [103] seededlda_1.2.1 "],["references.html", "Section 12 References", " Section 12 References Benoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, and Akitaka Matsuo. 2018. “Quanteda: An r Package for the Quantitative Analysis of Textual Data.” Journal of Open Source Software 3 (30): 774. https://doi.org/https://doi.org/10.21105/joss.00774. Bernard, H. Russell, and Gery Ryan. 1998. “Text Analysis.” Handbook of Methods in Cultural Anthropology 613. Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. “Latent Dirichlet Allocation.” Journal of Machine Learning Research 3: 993–1022. https://doi.org/https://doi.org/10.5555/944919.944937. Brookes, Gavin, and Tony McEnery. 2019. “The Utility of Topic Modelling for Discourse Studies.” Discourse Studies 21 (1): 3–21. https://doi.org/10.1177/14614456188140. Busso, Luciana, Monika Petyko, Steven Atkins, and Tim Grant. 2022. “Operation Heron: Latent Topic Changes in an Abusive Letter Series.” Corpora 17 (2): 225–58. https://doi.org/https://doi.org/10.3366/cor.2022.0255. Egbert, Jesse, and Douglas Biber. 2019. “Incorporating Text Dispersion into Keyword Analysis.” Corpora 14 (1): 77–104. https://doi.org/10.3366/cor.2019.0162. Gerlach, Martin, Tiago P. Peixoto, and Eduardo G. Altmann. 2018. “A Network Approach to Topic Models.” Science Advances 4: eaar1360. https://doi.org/https://doi.org/10.1126/sciadv.aaq1360. Gillings, Mark, and Andrew Hardie. 2022. “The Interpretation of Topic Models for Scholarly Analysis: An Evaluation and Critique of Current Practice.” Digital Scholarship in the Humanities 38: 530–43. https://doi.org/10.1093/llc/fqac075. Grün, Bettina, and Kurt Hornik. 2011. “Topicmodels: An r Package for Fitting Topic Models.” Journal of Statistical Software 40 (13): 1–30. https://doi.org/10.18637/jss.v040.i13. ———. 2024. Topicmodels: Topic Models. https://CRAN.R-project.org/package=topicmodels. Hyland, Conor C., Yang Tao, Lida Azizi, Martin Gerlach, Tiago P. Peixoto, and Eduardo G. Altmann. 2021. “Multilayer Networks for Text Analysis with Multiple Data Types.” EPJ Data Science 10: 33. https://doi.org/https://doi.org/10.1140/epjds/s13688-021-00288-5. Kabanoff, Boris. 1997. “Introduction: Computers Can Read as Well as Count: Computer-Aided Text Analysis in Organizational Research.” Journal of Organizational Behavior, 507–11. Mohammad, Saif M, and Peter D Turney. 2013. “Crowdsourcing a Word-Emotion Association Lexicon.” Computational Intelligence 29 (3): 436–65. https://doi.org/https://doi.org/10.1111/j.1467-8640.2012.00460.x. Popping, Roel. 2000. Computer-Assisted Text Analysis. Sage. https://doi.org/https://doi.org/10.4135/9781849208741. Roberts, Margaret E., Brandon M. Stewart, and Dustin Tingley. 2016. “Navigating the Local Modes of Big Data: The Case of Topic Models.” In Computational Social Science: Discovery and Prediction, edited by R. Michael Alvarez, 51–97. Cambridge University Press. https://doi.org/https://doi.org/10.1017/cbo9781316257340.004. Silge, Julia, and David Robinson. 2017. Text Mining with r: A Tidy Approach. O’Reilly Media. Sönning, Lukas. 2023. “Evaluation of Keyness Metrics: Performance and Reliability.” Corpus Linguistics and Linguistic Theory 0. https://doi.org/doi:10.1515/cllt-2022-0116. Watanabe, Kohei, and Phan Xuan-Hieu. 2024. Seededlda: Seeded Sequential LDA for Topic Modeling. https://CRAN.R-project.org/package=seededlda. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
