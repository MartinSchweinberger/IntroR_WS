[["introduction-to-text-analysis-with-r.html", "Section 3 Introduction to Text Analysis with R 3.1 What is Text Analysis? 3.2 Preparation and session set up", " Section 3 Introduction to Text Analysis with R UNFINISHED! WORK IN PROGRESS! I’M STILL WORKING ON THE CONTENT This tutorial introduces basic methods of Text Analysis (see Bernard and Ryan 1998; Kabanoff 1997; Popping 2000), i.e. computer-based analysis of language data or the (semi-)automated extraction of information from text. In the following, we will explore selected methods. The methods we will focus on are: Sentiment Analysis Keyword Detection Topic Modelling Network Analysis 3.1 What is Text Analysis? Text Analysis (TA) refers to the process of examining, processing, and interpreting unstructured data (texts) to uncover actionable knowledge using computational methods. Unstructured data (text) can, for example, include emails, literary texts, letters, articles, advertisements, official documents, social media content, transcripts, and product reviews. Actionable knowledge refers to insights and patterns used to classify, sort, extract information, determine relationships, identify trends, and make informed decisions. Sometimes, Text Analysis is distinguished from Text Analytics. In this context, Text Analysis refers to manual, close-reading, and qualitative interpretative approaches, while Text Analytics refers to quantitative, computational analysis of text. However, in this tutorial, we consider Text Analysis and Text Analytics to be synonymous, encompassing any computer-based qualitative or quantitative method for analyzing text. 3.2 Preparation and session set up To ensure the scripts below run smoothly, we need to install specific R packages from a library. If you’ve already installed these packages, you can skip this section. To install them, run the code below (which may take 1 to 5 minutes). install.packages(&quot;flextable&quot;) install.packages(&quot;GGally&quot;) install.packages(&quot;ggraph&quot;) install.packages(&quot;gutenbergr&quot;) install.packages(&quot;igraph&quot;) install.packages(&quot;lda&quot;) install.packages(&quot;ldatuning&quot;) install.packages(&quot;Matrix&quot;) install.packages(&quot;network&quot;) install.packages(&quot;quanteda&quot;) install.packages(&quot;quanteda.textplots&quot;) install.packages(&quot;quanteda.textstats&quot;) install.packages(&quot;RColorBrewer&quot;) install.packages(&quot;reshape2&quot;) install.packages(&quot;slam&quot;) install.packages(&quot;sna&quot;) install.packages(&quot;textdata&quot;) install.packages(&quot;tidygraph&quot;) install.packages(&quot;tidytext&quot;) install.packages(&quot;tm&quot;) install.packages(&quot;topicmodels&quot;) install.packages(&quot;udpipe&quot;) install.packages(&quot;wordcloud&quot;) install.packages(&quot;wordcloud2&quot;) Once all packages are installed, you can activate them by executing (running) the code chunk below. # load packages library(flextable) library(GGally) library(ggraph) library(gutenbergr) library(igraph) library(lda) library(ldatuning) library(Matrix) library(network) library(quanteda) library(quanteda.textplots) library(quanteda.textstats) library(RColorBrewer) library(reshape2) library(slam) library(sna) library(textdata) library(tidygraph) library(tidytext) library(tidyverse) library(tm) library(topicmodels) library(udpipe) library(wordcloud) library(wordcloud2) # activate klippy for copy-to-clipboard button klippy::klippy() Next, we need to load our data. For the first part which focuses on sentiment analysis, we will use the IMDB data consisting of positive and negative reviews which we load by executing the code chunk below. # load reviews posreviews &lt;- list.files(here::here(&quot;data/reviews_pos&quot;), full.names = T, pattern = &quot;.*txt&quot;) %&gt;% purrr::map_chr(~ readr::read_file(.)) %&gt;% str_c(collapse = &quot; &quot;) %&gt;% str_remove_all(&quot;&lt;.*?&gt;&quot;) negreviews &lt;- list.files(here::here(&quot;data/reviews_neg&quot;), full.names = T, pattern = &quot;.*txt&quot;) %&gt;% purrr::map_chr(~ readr::read_file(.))%&gt;% str_c(collapse = &quot; &quot;) %&gt;% str_remove_all(&quot;&lt;.*?&gt;&quot;) # inspect str(posreviews); str(negreviews) ## chr &quot;One of the other reviewers has mentioned that after watching just 1 Oz episode you&#39;ll be hooked. They are right&quot;| __truncated__ ## chr &quot;Basically there&#39;s a family where a little boy (Jake) thinks there&#39;s a zombie in his closet &amp; his parents are fi&quot;| __truncated__ "],["sentiment-analysis.html", "Section 4 Sentiment Analysis 4.1 What is Sentiment Analysis? 4.2 Exporting the results 4.3 Summarizing results 4.4 Visualizing results 4.5 Identifying important emotives", " Section 4 Sentiment Analysis This part of the workshop showcases how to perform SA on textual data using R. The analysis shown here is in parts based on the 2nd chapter of Text Mining with R - the e-version of this chapter on sentiment analysis can be found here. 4.1 What is Sentiment Analysis? Sentiment Analysis (SA) extracts information on emotion or opinion from natural language (Silge and Robinson 2017). Most forms of SA provides information about positive or negative polarity, e.g. whether a tweet is positive or negative. As such, SA represents a type of classifier that assigns values to texts. As most SA only provide information about polarity, SA is often regarded as rather coarse-grained and, thus, rather irrelevant for the types of research questions in linguistics. In the language sciences, SA can also be a very helpful tool if the type of SA provides more fine-grained information. In the following, we will perform such a information-rich SA. The SA used here does not only provide information about polarity but it will also provide association values for eight core emotions. The more fine-grained output is made possible by relying on the Word-Emotion Association Lexicon (Mohammad and Turney 2013), which comprises 10,170 terms, and in which lexical elements are assigned scores based on ratings gathered through the crowd-sourced Amazon Mechanical Turk service. For the Word-Emotion Association Lexicon raters were asked whether a given word was associated with one of eight emotions. The resulting associations between terms and emotions are based on 38,726 ratings from 2,216 raters who answered a sequence of questions for each word which were then fed into the emotion association rating (cf. Mohammad and Turney 2013). Each term was rated 5 times. For 85 percent of words, at least 4 raters provided identical ratings. For instance, the word cry or tragedy are more readily associated with SADNESS while words such as happy or beautiful are indicative of JOY and words like fit or burst may indicate ANGER. This means that the SA here allows us to investigate the expression of certain core emotions rather than merely classifying statements along the lines of a crude positive-negative distinction. We start by writing a function that clean the data. This allows us to feed our texts into the function and avoids duplicating code. Also, this showcases how you can write functions. txtclean &lt;- function(x, title){ require(dplyr) require(stringr) require(tibble) x &lt;- x %&gt;% iconv(to = &quot;UTF-8&quot;) %&gt;% base::tolower() %&gt;% paste0(collapse = &quot; &quot;) %&gt;% stringr::str_squish()%&gt;% stringr::str_split(&quot; &quot;) %&gt;% unlist() %&gt;% tibble::tibble() %&gt;% dplyr::select(word = 1, everything()) %&gt;% dplyr::mutate(type = title) %&gt;% dplyr::anti_join(stop_words) %&gt;% dplyr::mutate(word = str_remove_all(word, &quot;\\\\W&quot;)) %&gt;% dplyr::filter(word != &quot;&quot;) } Process and clean texts. # process text data posreviews_clean &lt;- txtclean(posreviews, &quot;Positive Review&quot;) negreviews_clean &lt;- txtclean(negreviews, &quot;Negative Review&quot;) # inspect str(posreviews_clean); str(negreviews_clean) ## tibble [95,046 × 2] (S3: tbl_df/tbl/data.frame) ## $ word: chr [1:95046] &quot;reviewers&quot; &quot;mentioned&quot; &quot;watching&quot; &quot;1&quot; ... ## $ type: chr [1:95046] &quot;Positive Review&quot; &quot;Positive Review&quot; &quot;Positive Review&quot; &quot;Positive Review&quot; ... ## tibble [90,190 × 2] (S3: tbl_df/tbl/data.frame) ## $ word: chr [1:90190] &quot;basically&quot; &quot;family&quot; &quot;boy&quot; &quot;jake&quot; ... ## $ type: chr [1:90190] &quot;Negative Review&quot; &quot;Negative Review&quot; &quot;Negative Review&quot; &quot;Negative Review&quot; ... Now, we combine the data with the Word-Emotion Association Lexicon (Mohammad and Turney 2013). reviews_annotated &lt;- rbind(posreviews_clean, negreviews_clean) %&gt;% dplyr::group_by(type) %&gt;% dplyr::mutate(words = n()) %&gt;% dplyr::left_join(tidytext::get_sentiments(&quot;nrc&quot;)) %&gt;% dplyr::mutate(type = factor(type), sentiment = factor(sentiment)) # inspect data reviews_annotated %&gt;% as.data.frame() %&gt;% head(10) ## word type words sentiment ## 1 reviewers Positive Review 95046 &lt;NA&gt; ## 2 mentioned Positive Review 95046 &lt;NA&gt; ## 3 watching Positive Review 95046 &lt;NA&gt; ## 4 1 Positive Review 95046 &lt;NA&gt; ## 5 oz Positive Review 95046 &lt;NA&gt; ## 6 episode Positive Review 95046 &lt;NA&gt; ## 7 hooked Positive Review 95046 negative ## 8 right Positive Review 95046 &lt;NA&gt; ## 9 happened Positive Review 95046 &lt;NA&gt; ## 10 methe Positive Review 95046 &lt;NA&gt; The resulting table shows each word token by the type of review in which it occurred, the overall number of tokens in the type of review, and the sentiment with which a token is associated. 4.2 Exporting the results To export the table with the results as an MS Excel spreadsheet, we use write_xlsx. Be aware that we use the here function to save the file in the current working directory. # save data write_xlsx(reviews_annotated, here::here(&quot;data/reviews_annotated.xlsx&quot;)) 4.3 Summarizing results After preforming the sentiment analysis, we can now display and summarize the results of the SA visually and add information to the table produced by the sentiment analysis (such as calculating the percentages of the prevalence of emotions across the review type and the rate of emotions across review types). reviews_summarised &lt;- reviews_annotated %&gt;% dplyr::group_by(type) %&gt;% dplyr::group_by(type, sentiment) %&gt;% dplyr::summarise(sentiment = unique(sentiment), sentiment_freq = n(), words = unique(words)) %&gt;% dplyr::filter(is.na(sentiment) == F) %&gt;% dplyr::mutate(percentage = round(sentiment_freq/words*100, 1), sentiment = factor(sentiment, levels = c(&quot;anger&quot;, &quot;fear&quot;, &quot;disgust&quot;, &quot;sadness&quot;, &quot;anticipation&quot;, &quot;surprise&quot;, &quot;trust&quot;, &quot;joy&quot;, &quot;negative&quot;, &quot;positive&quot;))) %&gt;% dplyr::group_by(sentiment) %&gt;% dplyr::mutate(total = sum(percentage)) %&gt;% dplyr::group_by(sentiment, type) %&gt;% dplyr::mutate(ratio = round(percentage/total*100, 1)) # inspect data reviews_summarised %&gt;% as.data.frame() %&gt;% head(10) ## type sentiment sentiment_freq words percentage total ratio ## 1 Negative Review anger 4537 90190 5.0 8.4 59.5 ## 2 Negative Review anticipation 4491 90190 5.0 10.5 47.6 ## 3 Negative Review disgust 3992 90190 4.4 6.8 64.7 ## 4 Negative Review fear 5512 90190 6.1 10.8 56.5 ## 5 Negative Review joy 3657 90190 4.1 9.8 41.8 ## 6 Negative Review negative 9245 90190 10.3 17.3 59.5 ## 7 Negative Review positive 9153 90190 10.1 22.4 45.1 ## 8 Negative Review sadness 4811 90190 5.3 9.0 58.9 ## 9 Negative Review surprise 2546 90190 2.8 5.8 48.3 ## 10 Negative Review trust 5255 90190 5.8 12.7 45.7 To export the table with the results as an MS Excel spreadsheet, we use write_xlsx. Be aware that we use the here function to save the file in the current working directory. # save data write_xlsx(reviews_summarised, here::here(&quot;data/reviews_summarised.xlsx&quot;)) 4.4 Visualizing results After performing the SA, we can display the emotions by review type ordered from more negative (red) to more positive (blue). reviews_summarised %&gt;% dplyr::filter(sentiment != &quot;positive&quot;, sentiment != &quot;negative&quot;) %&gt;% ggplot(aes(type, percentage, fill = sentiment, label = percentage)) + geom_bar(stat=&quot;identity&quot;, position=position_dodge()) + geom_text(hjust=1.5, position = position_dodge(0.9)) + scale_fill_brewer(palette = &quot;RdBu&quot;) + theme_bw() + theme(legend.position = &quot;right&quot;) + coord_flip() We can also visualize the results and show the rate to identify what type is more “positive” and what type is more “negative”. reviews_summarised %&gt;% dplyr::filter(sentiment != &quot;positive&quot;, sentiment != &quot;negative&quot;) %&gt;% ggplot(aes(sentiment, ratio, fill = type, label = ratio)) + geom_bar(stat=&quot;identity&quot;, position=position_fill()) + geom_text(position = position_fill(vjust = 0.5)) + scale_fill_manual(name = &quot;&quot;, values=c(&quot;orange&quot;, &quot;gray70&quot;)) + scale_y_continuous(name =&quot;Percent&quot;, breaks = seq(0, 1, .2), labels = seq(0, 100, 20)) + theme_bw() + theme(legend.position = &quot;top&quot;) 4.5 Identifying important emotives We now check, which words have contributed to the emotionality scores. In other words, we investigate, which words are most important for the emotion scores within each review type. For the sake of interpretability, we will remove several core emotion categories and also the polarity. reviews_importance &lt;- reviews_annotated %&gt;% dplyr::filter(!is.na(sentiment), sentiment != &quot;anticipation&quot;, sentiment != &quot;surprise&quot;, sentiment != &quot;disgust&quot;, sentiment != &quot;negative&quot;, sentiment != &quot;sadness&quot;, sentiment != &quot;positive&quot;) %&gt;% dplyr::mutate(sentiment = factor(sentiment, levels = c(&quot;anger&quot;, &quot;fear&quot;, &quot;trust&quot;, &quot;joy&quot;))) %&gt;% dplyr::group_by(type) %&gt;% dplyr::count(word, sentiment, sort = TRUE) %&gt;% dplyr::group_by(type, sentiment) %&gt;% dplyr::top_n(5) %&gt;% dplyr::mutate(score = n/sum(n)) # inspect data reviews_importance %&gt;% as.data.frame() %&gt;% head(10) ## type word sentiment n score ## 1 Negative Review bad anger 563 0.5198523 ## 2 Negative Review bad fear 563 0.4622332 ## 3 Positive Review love joy 323 0.3886883 ## 4 Positive Review watch fear 286 0.4056738 ## 5 Negative Review watch fear 278 0.2282430 ## 6 Positive Review real trust 224 0.3072702 ## 7 Negative Review pretty trust 174 0.2360923 ## 8 Negative Review pretty joy 174 0.2668712 ## 9 Negative Review director trust 154 0.2089552 ## 10 Negative Review real trust 148 0.2008141 We can now visualize the top three words for the remaining core emotion categories. reviews_importance %&gt;% dplyr::group_by(type) %&gt;% slice_max(score, n = 20) %&gt;% dplyr::arrange(desc(score)) %&gt;% dplyr::ungroup() %&gt;% ggplot(aes(x = reorder(word, score), y = score, fill = word)) + facet_wrap(type~sentiment, ncol = 4, scales = &quot;free_y&quot;) + geom_col(show.legend = FALSE) + coord_flip() + labs(x = &quot;Words&quot;) If you are interested in learning more about SA in R, Silge and Robinson (2017) is highly recommended as it goes more into detail and offers additional information. "],["keyword-detection.html", "Section 5 Keyword Detection 5.1 Dimensions of keyness 5.2 Identifying keywords 5.3 Visualising keywords", " Section 5 Keyword Detection Keywords play a crucial role in text analysis, serving as distinctive terms that hold particular significance within a given text, context, or collection. These words stand out due to their heightened frequency in a specific text or context, setting them apart from their occurrence in another. In essence, keywords are linguistic markers that encapsulate the essence or topical focus of a document or data set. The process of identifying keywords involves a methodology akin to the one employed for detecting collocations using kwics. This entails comparing the use of a particular word in corpus A, against its use in corpus B. By discerning the frequency disparities, we gain valuable insights into the salient terms that contribute significantly to the unique character and thematic emphasis of a given text or context. 5.1 Dimensions of keyness Before we start with the practical part of this tutorial, it is important to talk about the different dimensions of keyness (see Sönning 2023). Keyness analysis identifies typical items in a discourse domain, where typicalness traditionally relates to frequency of occurrence. The emphasis is on items used more frequently in the target corpus compared to a reference corpus. Egbert and Biber (2019) expanded this notion, highlighting two criteria for typicalness: content-distinctiveness and content-generalizability. Content-distinctiveness refers to an item’s association with the domain and its topical relevance. Content-generalizability pertains to an item’s widespread usage across various texts within the domain. These criteria bridge traditional keyness approaches with broader linguistic perspectives, emphasizing both the distinctiveness and generalizability of key items within a corpus. Following Sönning (2023), we adopt Egbert and Biber (2019) keyness criteria, distinguishing between frequency-oriented and dispersion-oriented approaches to assess keyness. These perspectives capture distinct, linguistically meaningful attributes of typicalness. We also differentiate between keyness features inherent to the target variety and those that emerge from comparing it to a reference variety. This four-way classification, detailed in the table below, links methodological choices to the linguistic meaning conveyed by quantitative measures. Typical items exhibit a sufficiently high occurrence rate to be discernible in the target variety, with discernibility measured solely within the target corpus. Key items are also distinct, being used more frequently than in reference domains of language use. While discernibility and distinctiveness both rely on frequency, they measure different aspects of typicalness. .cl-efd7d1d0{table-layout:auto;width:75%;}.cl-efbf03e4{font-family:'Arial';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-efbf040c{font-family:'Arial';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-efc7b28c{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-efc7e932{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-efc7e93c{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-efc7e946{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-efc7e950{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-efc7e951{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-efc7e95a{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-efc7e95b{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-efc7e964{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-efc7e96e{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 5.1: Dimensions of keyness (see Soenning, 2023: 3) AnalysisFrequency.orientedDispersion.orientedTarget variety in isolationDiscernibility of item in the target varietyGenerality across texts in the target varietyComparison to reference varietyDistinctiveness relative to the reference varietyComparative generality relative to the reference variety The second aspect of keyness involves an item’s dispersion across texts in the target domain, indicating its widespread use. A typical item should appear evenly across various texts within the target domain, reflecting its generality. This breadth of usage can be compared to its occurrence in the reference domain, termed as comparative generality. Therefore, a key item should exhibit greater prevalence across target texts compared to those in the reference domain. 5.2 Identifying keywords Here, we focus on a frequency-based approach that assesses distinctiveness relative to the reference variety. To identify these keywords, we can follow the procedure we have used to identify collocations using kwics - the idea is essentially identical: we compare the use of a word in a target corpus A to its use in a reference corpus. To determine if a token is a keyword and if it occurs significantly more frequently in a target corpus compared to a reference corpus, we use the following information (that is provided by the table above): O11 = Number of times wordx occurs in target corpus O12 = Number of times wordx occurs in reference corpus (without target corpus) O21 = Number of times other words occur in target corpus O22 = Number of times other words occur in reference corpus Example: target corpus reference corpus token O11 O12 = R1 other tokens O21 O22 = R2 = C1 = C2 = N We begin with loading two texts (posreviews is our target and negreviews is our reference). As a first step, we create a frequency table of first text. positive_words &lt;- posreviews %&gt;% # remove non-word characters stringr::str_remove_all(&quot;[^[:alpha:] ]&quot;) %&gt;% # convert to lower tolower() %&gt;% # tokenize the corpus files quanteda::tokens(remove_punct = T, remove_symbols = T, remove_numbers = T) %&gt;% # unlist the tokens to create a data frame unlist() %&gt;% as.data.frame() %&gt;% # rename the column to &#39;token&#39; dplyr::rename(token = 1) %&gt;% # group by &#39;token&#39; and count the occurrences dplyr::group_by(token) %&gt;% dplyr::summarise(n = n()) %&gt;% # add column stating where the frequency list is &#39;from&#39; dplyr::mutate(type = &quot;positive&quot;) # inspect head(positive_words) ## # A tibble: 6 × 3 ## token n type ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 a 6434 positive ## 2 aaliyah 1 positive ## 3 aames 1 positive ## 4 aamir 1 positive ## 5 aardman 2 positive ## 6 aaron 2 positive Now, we create a frequency table of second text. negative_words &lt;- negreviews %&gt;% # remove non-word characters stringr::str_remove_all(&quot;[^[:alpha:] ]&quot;) %&gt;% # convert to lower tolower() %&gt;% # tokenize the corpus files quanteda::tokens(remove_punct = T, remove_symbols = T, remove_numbers = T) %&gt;% # unlist the tokens to create a data frame unlist() %&gt;% as.data.frame() %&gt;% # rename the column to &#39;token&#39; dplyr::rename(token = 1) %&gt;% # group by &#39;token&#39; and count the occurrences dplyr::group_by(token) %&gt;% dplyr::summarise(n = n()) %&gt;% # add column stating where the frequency list is &#39;from&#39; dplyr::mutate(type = &quot;negative&quot;) # inspect head(negative_words) ## # A tibble: 6 × 3 ## token n type ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 a 6102 negative ## 2 aaargh 1 negative ## 3 aap 1 negative ## 4 aaron 2 negative ## 5 abandoned 8 negative ## 6 abandoning 1 negative In a next step, we combine the tables. texts_df &lt;- dplyr::left_join(positive_words, negative_words, by = c(&quot;token&quot;)) %&gt;% # rename columns and select relevant columns dplyr::rename(positive = n.x, negative = n.y) %&gt;% dplyr::select(-type.x, -type.y) %&gt;% # replace NA values with 0 in &#39;corpus&#39; and &#39;kwic&#39; columns tidyr::replace_na(list(positive = 0, negative = 0)) # inspect texts_df %&gt;% as.data.frame() %&gt;% head(10) ## token positive negative ## 1 a 6434 6102 ## 2 aaliyah 1 0 ## 3 aames 1 0 ## 4 aamir 1 0 ## 5 aardman 2 0 ## 6 aaron 2 2 ## 7 aawip 1 0 ## 8 ab 2 0 ## 9 abandon 2 0 ## 10 abandoned 4 8 We now calculate the frequencies of the observed and expected frequencies as well as the row and column totals. texts_df %&gt;% dplyr::mutate(positive = as.numeric(positive), negative = as.numeric(negative)) %&gt;% dplyr::mutate(C1 = sum(positive), C2 = sum(negative), N = C1 + C2) %&gt;% dplyr::rowwise() %&gt;% dplyr::mutate(R1 = positive+negative, R2 = N - R1, O11 = positive, O12 = R1-O11, O21 = C1-O11, O22 = C2-O12) %&gt;% dplyr::mutate(E11 = (R1 * C1) / N, E12 = (R1 * C2) / N, E21 = (R2 * C1) / N, E22 = (R2 * C2) / N) %&gt;% dplyr::select(-positive, -negative) -&gt; stats_raw # inspect stats_raw %&gt;% as.data.frame() %&gt;% head(10) ## token C1 C2 N R1 R2 O11 O12 O21 O22 ## 1 a 224093 203391 427484 12536 414948 6434 6102 217659 197289 ## 2 aaliyah 224093 203391 427484 1 427483 1 0 224092 203391 ## 3 aames 224093 203391 427484 1 427483 1 0 224092 203391 ## 4 aamir 224093 203391 427484 1 427483 1 0 224092 203391 ## 5 aardman 224093 203391 427484 2 427482 2 0 224091 203391 ## 6 aaron 224093 203391 427484 4 427480 2 2 224091 203389 ## 7 aawip 224093 203391 427484 1 427483 1 0 224092 203391 ## 8 ab 224093 203391 427484 2 427482 2 0 224091 203391 ## 9 abandon 224093 203391 427484 2 427482 2 0 224091 203391 ## 10 abandoned 224093 203391 427484 12 427472 4 8 224089 203383 ## E11 E12 E21 E22 ## 1 6571.5438426 5964.4561574 217521.5 197426.5 ## 2 0.5242138 0.4757862 224092.5 203390.5 ## 3 0.5242138 0.4757862 224092.5 203390.5 ## 4 0.5242138 0.4757862 224092.5 203390.5 ## 5 1.0484275 0.9515725 224092.0 203390.0 ## 6 2.0968551 1.9031449 224090.9 203389.1 ## 7 0.5242138 0.4757862 224092.5 203390.5 ## 8 1.0484275 0.9515725 224092.0 203390.0 ## 9 1.0484275 0.9515725 224092.0 203390.0 ## 10 6.2905653 5.7094347 224086.7 203385.3 We could now calculate the keyness statistics for ll words in the reviews. However, this takes a few minutes an do we will exclude tokens that occur less than 10 times. stats_redux &lt;- stats_raw %&gt;% dplyr::filter(R1 &gt; 10) We can now calculate the keyness measures. stats_redux %&gt;% # determine number of rows dplyr::mutate(Rws = nrow(.)) %&gt;% # work row-wise dplyr::rowwise() %&gt;% # calculate fishers&#39; exact test dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(O11, O12, O21, O22), ncol = 2, byrow = T))[1]))) %&gt;% # extract descriptives dplyr::mutate(ptw_target = O11/C1*1000, ptw_ref = O12/C2*1000) %&gt;% # extract x2 statistics dplyr::mutate(X2 = (O11-E11)^2/E11 + (O12-E12)^2/E12 + (O21-E21)^2/E21 + (O22-E22)^2/E22) %&gt;% # extract keyness measures dplyr::mutate(phi = sqrt((X2 / N)), MI = log2(O11 / E11), t.score = (O11 - E11) / sqrt(O11), PMI = log2( (O11 / N) / ((O11+O12) / N) * ((O11+O21) / N) ), DeltaP = (O11 / R1) - (O21 / R2), LogOddsRatio = log(((O11 + 0.5) * (O22 + 0.5)) / ( (O12 + 0.5) * (O21 + 0.5) )), G2 = 2 * ((O11+ 0.001) * log((O11+ 0.001) / E11) + (O12+ 0.001) * log((O12+ 0.001) / E12) + O21 * log(O21 / E21) + O22 * log(O22 / E22)), # traditional keyness measures RateRatio = ((O11+ 0.001)/(C1*1000)) / ((O12+ 0.001)/(C2*1000)), RateDifference = (O11/(C1*1000)) - (O12/(C2*1000)), DifferenceCoefficient = RateDifference / sum((O11/(C1*1000)), (O12/(C2*1000))), OddsRatio = ((O11 + 0.5) * (O22 + 0.5)) / ( (O12 + 0.5) * (O21 + 0.5) ), LLR = 2 * (O11 * (log((O11 / E11)))), RDF = abs((O11 / C1) - (O12 / C2)), PDiff = abs(ptw_target - ptw_ref) / ((ptw_target + ptw_ref) / 2) * 100, SignedDKL = sum(ifelse(O11 &gt; 0, O11 * log(O11 / ((O11 + O12) / 2)), 0) - ifelse(O12 &gt; 0, O12 * log(O12 / ((O11 + O12) / 2)), 0))) %&gt;% # determine Bonferroni corrected significance dplyr::mutate(Sig_corrected = dplyr::case_when(p / Rws &gt; .05 ~ &quot;n.s.&quot;, p / Rws &gt; .01 ~ &quot;p &lt; .05*&quot;, p / Rws &gt; .001 ~ &quot;p &lt; .01**&quot;, p / Rws &lt;= .001 ~ &quot;p &lt; .001***&quot;, T ~ &quot;N.A.&quot;)) %&gt;% # round p-value dplyr::mutate(p = round(p, 5), type = ifelse(E11 &gt; O11, &quot;antitype&quot;, &quot;type&quot;), phi = ifelse(E11 &gt; O11, -phi, phi), G2 = ifelse(E11 &gt; O11, -G2, G2)) %&gt;% # filter out non significant results dplyr::filter(Sig_corrected != &quot;n.s.&quot;) %&gt;% # arrange by G2 dplyr::arrange(-G2) %&gt;% # remove superfluous columns dplyr::select(-any_of(c(&quot;TermCoocFreq&quot;, &quot;AllFreq&quot;, &quot;NRows&quot;, &quot;R1&quot;, &quot;R2&quot;, &quot;C1&quot;, &quot;C2&quot;, &quot;E12&quot;, &quot;E21&quot;, &quot;E22&quot;, &quot;upp&quot;, &quot;low&quot;, &quot;op&quot;, &quot;t.score&quot;, &quot;z.score&quot;, &quot;Rws&quot;))) %&gt;% dplyr::relocate(any_of(c(&quot;token&quot;, &quot;type&quot;, &quot;Sig_corrected&quot;, &quot;O11&quot;, &quot;O12&quot;, &quot;ptw_target&quot;, &quot;ptw_ref&quot;, &quot;G2&quot;, &quot;RDF&quot;, &quot;RateRatio&quot;, &quot;RateDifference&quot;, &quot;DifferenceCoefficient&quot;, &quot;LLR&quot;, &quot;SignedDKL&quot;, &quot;PDiff&quot;, &quot;LogOddsRatio&quot;, &quot;MI&quot;, &quot;PMI&quot;, &quot;phi&quot;, &quot;X2&quot;, &quot;OddsRatio&quot;, &quot;DeltaP&quot;, &quot;p&quot;, &quot;E11&quot;, &quot;O21&quot;, &quot;O22&quot;))) -&gt; keys keys %&gt;% as.data.frame() %&gt;% head(10) ## token type Sig_corrected O11 O12 ptw_target ptw_ref G2 ## 1 great type p &lt; .001*** 472 187 2.1062684 0.9194114 101.49317 ## 2 excellent type p &lt; .001*** 145 21 0.6470528 0.1032494 92.47189 ## 3 love type p &lt; .001*** 323 128 1.4413659 0.6293297 69.38975 ## 4 wonderful type p &lt; .001*** 120 27 0.5354920 0.1327492 54.92216 ## 5 loved type p &lt; .001*** 102 21 0.4551682 0.1032494 50.53473 ## 6 best type p &lt; .001*** 326 156 1.4547532 0.7669956 45.97562 ## 7 still type p &lt; .001*** 271 129 1.2093193 0.6342464 38.74014 ## 8 world type p &lt; .001*** 207 88 0.9237236 0.4326642 38.58549 ## 9 perfect type p &lt; .001*** 94 25 0.4194687 0.1229160 36.22459 ## 10 his type p &lt; .001*** 1262 882 5.6315905 4.3364751 36.09188 ## RDF RateRatio RateDifference DifferenceCoefficient LLR ## 1 0.0011868570 2.290880 1.186857e-06 0.3922613 294.63294 ## 2 0.0005438034 6.266636 5.438034e-07 0.7247791 148.07448 ## 3 0.0008120361 2.290308 8.120361e-07 0.3921562 201.57829 ## 4 0.0004027427 4.033745 4.027427e-07 0.6026907 106.29957 ## 5 0.0003519188 4.408267 3.519188e-07 0.6302072 93.56341 ## 6 0.0006877576 1.896684 6.877576e-07 0.3095569 166.13546 ## 7 0.0005750730 1.906695 5.750730e-07 0.3119352 139.02841 ## 8 0.0004910595 2.134953 4.910595e-07 0.3620347 120.72205 ## 9 0.0002965527 3.412546 2.965527e-07 0.5467572 77.08508 ## 10 0.0012951154 1.298656 1.295115e-06 0.1299265 292.48172 ## SignedDKL PDiff LogOddsRatio MI PMI phi X2 ## 1 275.56751 78.45225 0.8285169 0.4502812 -1.413264 0.015108132 97.57564 ## 2 109.75519 144.95583 1.8157365 0.7366425 -1.126903 0.013784540 81.22775 ## 3 188.54940 78.43124 0.8271524 0.4501796 -1.413366 0.012492127 66.71026 ## 4 85.86387 120.53813 1.3809359 0.6389911 -1.224555 0.010848353 50.30921 ## 5 74.17024 126.04143 1.4652312 0.6616837 -1.201862 0.010362688 45.90550 ## 6 166.33553 61.91137 0.6391318 0.3676117 -1.495934 0.010234742 44.77893 ## 7 138.89733 62.38704 0.6439262 0.3700657 -1.493480 0.009393260 37.71834 ## 8 115.60160 72.40694 0.7556895 0.4206887 -1.442857 0.009338842 37.28258 ## 9 64.66545 109.35145 1.2132873 0.5915439 -1.272002 0.008877874 33.69286 ## 10 377.99144 25.98529 0.2624613 0.1671798 -1.696366 0.009156046 35.83734 ## OddsRatio DeltaP p E11 O21 O22 N ## 1 2.289920 0.19231943 0 345.45688 223621 203204 427484 ## 2 6.145601 0.34941589 0 87.01949 223948 203370 427484 ## 3 2.286798 0.19217523 0 236.42041 223770 203263 427484 ## 4 3.978624 0.29221324 0 77.05942 223973 203364 427484 ## 5 4.328544 0.30514232 0 64.47829 223991 203370 427484 ## 6 1.894835 0.15230651 0 252.67104 223767 203235 427484 ## 7 1.903941 0.15342979 0 209.68551 223822 203262 427484 ## 8 2.129079 0.17760371 0 154.64306 223886 203303 427484 ## 9 3.364527 0.26577618 0 62.38144 223999 203366 427484 ## 10 1.300126 0.06473028 0 1123.91433 222831 202509 427484 The above table shows the keywords for positive IMDB reviews. The table starts with token (word type), followed by type, which indicates whether the token is a keyword in the target data (type) or a keyword in the reference data (antitype). Next is the Bonferroni corrected significance (Sig_corrected), which accounts for repeated testing. This is followed by O11, representing the observed frequency of the token, and Exp which represents the expected frequency of the token if it were distributed evenly across the target and reference data. After this, the table provides different keyness statistics (for information about these different keyness statistics see here). 5.3 Visualising keywords We can now visualize the keyness strengths in a dot plot as shown in the code chunk below. # sort the keys data frame in descending order based on the &#39;G2&#39; column keys %&gt;% dplyr::arrange(-G2) %&gt;% # select the top 20 rows after sorting head(20) %&gt;% # create a ggplot with &#39;token&#39; on the x-axis (reordered by &#39;G2&#39;) and &#39;G2&#39; on the y-axis ggplot(aes(x = reorder(token, G2, mean), y = G2)) + # add a scatter plot with points representing the &#39;G2&#39; values geom_point() + # flip the coordinates to have horizontal points coord_flip() + # set the theme to a basic white and black theme theme_bw() + # set the x-axis label to &quot;Token&quot; and y-axis label to &quot;Keyness (G2)&quot; labs(x = &quot;Token&quot;, y = &quot;Keyness (G2)&quot;) Another option to visualize keyness is a bar plot as shown below. # get top 10 keywords for text 1 top &lt;- keys %&gt;% dplyr::ungroup() %&gt;% dplyr::slice_head(n = 12) # get top 10 keywords for text 2 bot &lt;- keys %&gt;% dplyr::ungroup() %&gt;% dplyr::slice_tail(n = 12) # combine into table rbind(top, bot) %&gt;% # create a ggplot ggplot(aes(x = reorder(token, G2, mean), y = G2, label = G2, fill = type)) + # add a bar plot using the &#39;phi&#39; values geom_bar(stat = &quot;identity&quot;) + # add text labels above the bars with rounded &#39;phi&#39; values geom_text(aes(y = ifelse(G2&gt; 0, G2 - 20, G2 + 20), label = round(G2, 1)), color = &quot;white&quot;, size = 3) + # flip the coordinates to have horizontal bars coord_flip() + # set the theme to a basic white and black theme theme_bw() + # remove legend theme(legend.position = &quot;none&quot;) + # define colors scale_fill_manual(values = c(&quot;orange&quot;,&quot;darkgray&quot;)) + # set the x-axis label to &quot;Token&quot; and y-axis label to &quot;Keyness (G2)&quot; labs(title = &quot;Top 10 keywords for positive and negative IMDB reviews&quot;, x = &quot;Keyword&quot;, y = &quot;Keyness (G2)&quot;) "],["network-analysis.html", "Section 6 Network Analysis 6.1 Data preparation 6.2 Creating a matrix 6.3 Visualizing Networks 6.4 Network Statistics", " Section 6 Network Analysis Networks are a powerful method for visualizing relationships among various elements, such as authors, characters, or words (Silge and Robinson 2017, 131–37). Network analysis goes beyond mere visualization; it’s a technique for uncovering patterns and structures within complex systems. In essence, network analysis represents relationships as nodes (elements) connected by edges (relationships) which provides a unique perspective for understanding the connections and interactions within your data. Networks, also known as graphs, are powerful tools that represent relationships among entities. They consist of nodes (often depicted as dots) and edges (typically represented as lines) and can be categorized as directed or undirected networks. In directed networks, the direction of edges is captured, signifying the flow or relationship from one node to another. An example of a directed network is the trade relationships between countries, where arrows on the edges indicate the direction of exports. The thickness of these arrows can also encode additional information, such as the frequency or strength of the relationship. Undirected networks, on the other hand, represent symmetric relationships where the connection between two nodes is mutual. For example, in a social network, the connections between individuals are often undirected, as the relationship between friends is reciprocal. Network analysis involves exploring the structure and properties of these networks. One key concept is centrality, which identifies the most important nodes in a network. Centrality metrics, such as degree centrality (number of connections) and betweenness centrality (importance in connecting other nodes), help unveil the significance of specific nodes. In R, there are several packages that provide essential tools for constructing, analyzing, and visualizing networks but here, we will focus on the quanteda.textplots, igraph, tidygraph, and ggraph packages. To showcase how to prepare and generate network graphs, we will visualize the network that the characters in William Shakespeare’s Romeo and Juliet form. 6.1 Data preparation In network analysis, it’s crucial to have at least one table indicating the start and end points of edges (lines connecting nodes). Additionally, two additional tables providing information on node size/type and edge size/type are valuable. In the upcoming sections, we’ll create these tables from raw data. Alternatively, you can generate network graphs by uploading tables containing the necessary information. We’ll generate a network showing the frequency of characters in William Shakespeare’s Romeo and Juliet appearing in the same scene. Our focus is on investigating the networks of personas in Shakespeare’s Romeo and Juliet, and thus, we’ll load this renowned work of fiction. 6.2 Creating a matrix We start by loading the data which represents a table that contains the personas that are present during a sub-scene as well as how many contributions they make and how often they occur. # load data net_dat &lt;- read.delim(&quot;https://slcladal.github.io/data/romeo_tidy.txt&quot;, sep = &quot;\\t&quot;) # inspect data net_dat %&gt;% as.data.frame() %&gt;% head(15) ## actscene person contrib occurrences ## 1 ACT I_SCENE I BENVOLIO 24 7 ## 2 ACT I_SCENE I CAPULET 2 9 ## 3 ACT I_SCENE I FIRST CITIZEN 1 2 ## 4 ACT I_SCENE I LADY CAPULET 1 10 ## 5 ACT I_SCENE I MONTAGUE 6 3 ## 6 ACT I_SCENE I PRINCE 1 3 ## 7 ACT I_SCENE I ROMEO 16 14 ## 8 ACT I_SCENE I TYBALT 2 3 ## 9 ACT I_SCENE II BENVOLIO 5 7 ## 10 ACT I_SCENE II CAPULET 3 9 ## 11 ACT I_SCENE II PARIS 2 5 ## 12 ACT I_SCENE II ROMEO 11 14 ## 13 ACT I_SCENE II SERVANT 8 3 ## 14 ACT I_SCENE III JULIET 5 11 ## 15 ACT I_SCENE III LADY CAPULET 11 10 We now transform that table into a co-occurrence matrix. net_cmx &lt;- crossprod(table(net_dat[1:2])) diag(net_cmx) &lt;- 0 net_df &lt;- as.data.frame(net_cmx) # inspect data net_df[1:5, 1:5]%&gt;% as.data.frame() %&gt;% tibble::rownames_to_column(&quot;Persona&quot;) ## Persona BALTHASAR BENVOLIO CAPULET FIRST CITIZEN FIRST SERVANT ## 1 BALTHASAR 0 0 1 0 0 ## 2 BENVOLIO 0 0 3 2 1 ## 3 CAPULET 1 3 0 1 2 ## 4 FIRST CITIZEN 0 2 1 0 0 ## 5 FIRST SERVANT 0 1 2 0 0 The data shows how often a character has appeared with each other character in the play - only Friar Lawrence and Friar John were excluded because they only appear in one scene where they talk to each other. 6.3 Visualizing Networks There are various different ways to visualize a network structure. We will focus on two packages for network visualization here and exemplify how you can visualize networks in R. 6.3.1 Quanteda Networks The quanteda package contains many very useful functions for analyzing texts. Among these functions is the textplot_network function which provides a very handy way to display networks. The advantage of the network plots provided by or generated with the quanteda package is that you can create them with very little code. However, this comes at a cost as these visualizations cannot be modified easily (which means that their design is not very flexible compared to other methods for generating network visualizations). In a first step, we transform the text vectors of the romeo data into a document-feature matrix using the dfm function. # create a document feature matrix net_dfm &lt;- quanteda::as.dfm(net_df) # create feature co-occurrence matrix net_fcm &lt;- quanteda::fcm(net_dfm, tri = F) # inspect data head(net_fcm) ## Feature co-occurrence matrix of: 6 by 18 features. ## features ## features BALTHASAR BENVOLIO CAPULET FIRST CITIZEN FIRST SERVANT ## BALTHASAR 1 25 31 11 6 ## BENVOLIO 25 39 93 39 27 ## CAPULET 31 93 65 42 39 ## FIRST CITIZEN 11 39 42 6 10 ## FIRST SERVANT 6 27 39 10 3 ## FRIAR LAWRENCE 20 53 74 18 17 ## features ## features FRIAR LAWRENCE JULIET LADY CAPULET MERCUTIO MONTAGUE ## BALTHASAR 20 26 31 11 17 ## BENVOLIO 53 87 99 42 55 ## CAPULET 74 131 117 52 65 ## FIRST CITIZEN 18 32 36 24 29 ## FIRST SERVANT 17 40 42 12 15 ## FRIAR LAWRENCE 15 61 72 23 32 ## [ reached max_nfeat ... 8 more features ] This feature-co-occurrence matrix can then serve as the input for the textplot_network function which already generates a nice network graph. Now we generate a network graph using the textplot_network function from the quanteda.textplots package. This function has the following arguments: x: a fcm or dfm object min_freq: a frequency count threshold or proportion for co-occurrence frequencies of features to be included (default = 0.5), omit_isolated: if TRUE, features do not occur more frequent than min_freq will be omitted (default = TRUE), edge_color: color of edges that connect vertices (default = “#1F78B4”), edge_alpha: opacity of edges ranging from 0 to 1.0 (default = 0.5), edge_size: size of edges for most frequent co-occurrence (default = 2), vertex_color: color of vertices (default = “#4D4D4D”), vertex_size: size of vertices (default = 2), vertex_labelcolor: color of texts. Defaults to the same as vertex_color, vertex_labelfont: font-family of texts, vertex_labelsize: size of vertex labels in mm. Defaults to size 5. Supports both integer values and vector values (default = 5), offset: if NULL (default), the distance between vertices and texts are determined automatically, quanteda.textplots::textplot_network( x = net_fcm, # a fcm or dfm object min_freq = 0.5, # frequency count threshold or proportion for co-occurrence frequencies (default = 0.5) edge_alpha = 0.5, # opacity of edges ranging from 0 to 1.0 (default = 0.5) edge_color = &quot;gray&quot;, # color of edges that connect vertices (default = &quot;#1F78B4&quot;) edge_size = 2, # size of edges for most frequent co-occurrence (default = 2) # calculate the size of vertex labels for the network plot vertex_labelsize = net_dfm %&gt;% # convert the dfm object to a data frame quanteda::convert(to = &quot;data.frame&quot;) %&gt;% # exclude the &#39;doc_id&#39; column dplyr::select(-doc_id) %&gt;% # calculate the sum of row values for each row rowSums() %&gt;% # apply the natural logarithm to the resulting sums log(), vertex_color = &quot;#4D4D4D&quot;, # color of vertices (default = &quot;#4D4D4D&quot;) vertex_size = 2 # size of vertices (default = 2) ) We now turn to generating tidy networks with is more complex but also offers more flexibility and options for customization. 6.3.2 Tidy Networks We now turn to a different method for generating networks that is extremely flexible. First, we define the nodes and we can also add information about the nodes that we can use later on (such as frequency information). # create a new data frame &#39;va&#39; using the &#39;net_dat&#39; data net_dat %&gt;% # rename the &#39;person&#39; column to &#39;node&#39; and &#39;occurrences&#39; column to &#39;n&#39; dplyr::rename(node = person, n = occurrences) %&gt;% # group the data by the &#39;node&#39; column dplyr::group_by(node) %&gt;% # summarize the data, calculating the total occurrences (&#39;n&#39;) for each &#39;node&#39; dplyr::summarise(n = sum(n)) -&gt; va # inspect va %&gt;% as.data.frame() %&gt;% head(10) ## node n ## 1 BALTHASAR 4 ## 2 BENVOLIO 49 ## 3 CAPULET 81 ## 4 FIRST CITIZEN 4 ## 5 FIRST SERVANT 4 ## 6 FRIAR LAWRENCE 49 ## 7 JULIET 121 ## 8 LADY CAPULET 100 ## 9 MERCUTIO 16 ## 10 MONTAGUE 9 The next part is optional but it can help highlight important information. We add a column with additional information to our nodes table. # define family mon &lt;- c(&quot;ABRAM&quot;, &quot;BALTHASAR&quot;, &quot;BENVOLIO&quot;, &quot;LADY MONTAGUE&quot;, &quot;MONTAGUE&quot;, &quot;ROMEO&quot;) cap &lt;- c(&quot;CAPULET&quot;, &quot;CAPULET’S COUSIN&quot;, &quot;FIRST SERVANT&quot;, &quot;GREGORY&quot;, &quot;JULIET&quot;, &quot;LADY CAPULET&quot;, &quot;NURSE&quot;, &quot;PETER&quot;, &quot;SAMPSON&quot;, &quot;TYBALT&quot;) oth &lt;- c(&quot;APOTHECARY&quot;, &quot;CHORUS&quot;, &quot;FIRST CITIZEN&quot;, &quot;FIRST MUSICIAN&quot;, &quot;FIRST WATCH&quot;, &quot;FRIAR JOHN&quot; , &quot;FRIAR LAWRENCE&quot;, &quot;MERCUTIO&quot;, &quot;PAGE&quot;, &quot;PARIS&quot;, &quot;PRINCE&quot;, &quot;SECOND MUSICIAN&quot;, &quot;SECOND SERVANT&quot;, &quot;SECOND WATCH&quot;, &quot;SERVANT&quot;, &quot;THIRD MUSICIAN&quot;) # create color vectors va &lt;- va %&gt;% dplyr::mutate(type = dplyr::case_when(node %in% mon ~ &quot;MONTAGUE&quot;, node %in% cap ~ &quot;CAPULET&quot;, TRUE ~ &quot;Other&quot;)) # inspect updates nodes table head(va, 10) ## # A tibble: 10 × 3 ## node n type ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 BALTHASAR 4 MONTAGUE ## 2 BENVOLIO 49 MONTAGUE ## 3 CAPULET 81 CAPULET ## 4 FIRST CITIZEN 4 Other ## 5 FIRST SERVANT 4 CAPULET ## 6 FRIAR LAWRENCE 49 Other ## 7 JULIET 121 CAPULET ## 8 LADY CAPULET 100 CAPULET ## 9 MERCUTIO 16 Other ## 10 MONTAGUE 9 MONTAGUE Now, we define the edges, i.e., the connections between nodes and, again, we can add information in separate variables that we can use later on. # create a new data frame &#39;ed&#39; using the &#39;dat&#39; data ed &lt;- net_df %&gt;% # add a new column &#39;from&#39; with row names dplyr::mutate(from = rownames(.)) %&gt;% # reshape the data from wide to long format using &#39;gather&#39; tidyr::gather(to, n, BALTHASAR:TYBALT) %&gt;% # remove zero frequencies dplyr::filter(n != 0) # inspect ed %&gt;% as.data.frame() %&gt;% head(10) ## from to n ## 1 CAPULET BALTHASAR 1 ## 2 FRIAR LAWRENCE BALTHASAR 1 ## 3 JULIET BALTHASAR 1 ## 4 LADY CAPULET BALTHASAR 1 ## 5 MONTAGUE BALTHASAR 1 ## 6 PARIS BALTHASAR 1 ## 7 PRINCE BALTHASAR 1 ## 8 ROMEO BALTHASAR 2 ## 9 CAPULET BENVOLIO 3 ## 10 FIRST CITIZEN BENVOLIO 2 Now that we have generated tables for the edges and the nodes, we can generate a graph object. ig &lt;- igraph::graph_from_data_frame(d=ed, vertices=va, directed = FALSE) We will also add labels to the nodes as follows: tg &lt;- tidygraph::as_tbl_graph(ig) %&gt;% tidygraph::activate(nodes) %&gt;% dplyr::mutate(label=name) When we now plot our network, it looks as shown below. # set seed (so that the exact same network graph is created every time) set.seed(12345) # create a graph using the &#39;tg&#39; data frame with the Fruchterman-Reingold layout tg %&gt;% ggraph::ggraph(layout = &quot;fr&quot;) + # add arcs for edges with various aesthetics geom_edge_arc(colour = &quot;gray50&quot;, lineend = &quot;round&quot;, strength = .1, aes(edge_width = ed$n, alpha = ed$n)) + # add points for nodes with size based on log-transformed &#39;v.size&#39; and color based on &#39;va$Family&#39; geom_node_point(size = log(va$n) * 2, aes(color = va$type)) + # add text labels for nodes with various aesthetics geom_node_text(aes(label = name), repel = TRUE, point.padding = unit(0.2, &quot;lines&quot;), size = sqrt(va$n), colour = &quot;gray10&quot;) + # adjust edge width and alpha scales scale_edge_width(range = c(0, 2.5)) + scale_edge_alpha(range = c(0, .3)) + # set graph background color to white theme_graph(background = &quot;white&quot;) + # adjust legend position to the top theme(legend.position = &quot;top&quot;, # suppress legend title legend.title = element_blank()) + # remove edge width and alpha guides from the legend guides(edge_width = FALSE, edge_alpha = FALSE) 6.4 Network Statistics In addition to visualizing networks, we will analyze the network and extract certain statistics about the network that tell us about structural properties of networks. To extract the statistics, we use the edge object generated above (called ed) and then repeat each combination as often as it occurred based on the value in the Frequency column. dg &lt;- ed[rep(seq_along(ed$n), ed$n), 1:2] rownames(dg) &lt;- NULL # inspect data dg %&gt;% as.data.frame() %&gt;% head(10) ## from to ## 1 CAPULET BALTHASAR ## 2 FRIAR LAWRENCE BALTHASAR ## 3 JULIET BALTHASAR ## 4 LADY CAPULET BALTHASAR ## 5 MONTAGUE BALTHASAR ## 6 PARIS BALTHASAR ## 7 PRINCE BALTHASAR ## 8 ROMEO BALTHASAR ## 9 ROMEO BALTHASAR ## 10 CAPULET BENVOLIO 6.4.1 Degree centrality We now generate an edge list from the dg object and then extract the degree centrality. The degree centrality reflects how many edges each node has with the most central node having the highest value. dgg &lt;- graph.edgelist(as.matrix(dg), directed = T) # extract degree centrality igraph::degree(dgg) %&gt;% as.data.frame() %&gt;% tibble::rownames_to_column(&quot;node&quot;) %&gt;% dplyr::rename(`degree centrality` = 2) %&gt;% dplyr::arrange(-`degree centrality`) -&gt; dc_tbl # inspect data dc_tbl %&gt;% as.data.frame() %&gt;% head(10) ## node degree centrality ## 1 ROMEO 108 ## 2 CAPULET 92 ## 3 LADY CAPULET 90 ## 4 NURSE 76 ## 5 JULIET 72 ## 6 BENVOLIO 68 ## 7 MONTAGUE 44 ## 8 PRINCE 44 ## 9 TYBALT 44 ## 10 PARIS 42 6.4.2 Central node Next, we extract the most central node. names(igraph::degree(dgg))[which(igraph::degree(dgg) == max(igraph::degree(dgg)))] ## [1] &quot;ROMEO&quot; 6.4.3 Betweenness centrality We now extract the betweenness centrality. Betweenness centrality provides a measure of how important nodes are for information flow between nodes in a network. The node with the highest betweenness centrality creates the shortest paths in the network. The higher a node’s betweenness centrality, the more important it is for the efficient flow of goods in a network. igraph::betweenness(dgg) %&gt;% as.data.frame() %&gt;% tibble::rownames_to_column(&quot;node&quot;) %&gt;% dplyr::rename(`betweenness centrality` = 2) %&gt;% dplyr::arrange(-`betweenness centrality`) -&gt; bc_tbl # inspect data bc_tbl %&gt;% as.data.frame() %&gt;% head(10) ## node betweenness centrality ## 1 ROMEO 27.624370 ## 2 LADY CAPULET 16.276864 ## 3 CAPULET 15.623219 ## 4 BENVOLIO 9.615121 ## 5 NURSE 7.401454 ## 6 JULIET 5.554710 ## 7 TYBALT 3.199408 ## 8 MONTAGUE 2.182203 ## 9 PRINCE 2.182203 ## 10 PARIS 1.859429 6.4.4 Closeness In addition, we extract the closeness statistic of all edges in the dg object by using the closeness function from the igraph package. Closeness centrality refers to the shortest paths between nodes. The distance between two nodes represents the length of the shortest path between them. The closeness of a node is the average distance from that node to all other nodes. igraph::closeness(dgg) %&gt;% as.data.frame() %&gt;% tibble::rownames_to_column(&quot;node&quot;) %&gt;% dplyr::rename(closeness = 2) %&gt;% dplyr::arrange(-closeness) -&gt; c_tbl # inspect data c_tbl %&gt;% as.data.frame() %&gt;% head(10) ## node closeness ## 1 LADY CAPULET 0.05882353 ## 2 ROMEO 0.05882353 ## 3 CAPULET 0.05555556 ## 4 BENVOLIO 0.05263158 ## 5 JULIET 0.05000000 ## 6 NURSE 0.04761905 ## 7 TYBALT 0.04761905 ## 8 MONTAGUE 0.04545455 ## 9 PARIS 0.04545455 ## 10 PRINCE 0.04545455 "],["topic-modelling.html", "Section 7 Topic Modelling 7.1 Getting started with Topic Modelling 7.2 Human-in-the-loop Topic Modelling 7.3 Loading and preparing data 7.4 Initial unsupervised topic model 7.5 Supervised, seeded topic model", " Section 7 Topic Modelling Topic models refers to a suit of methods employed to uncover latent structures within a corpus of text. These models operate on the premise of identifying abstract topics that recur across documents. In essence, topic models sift through the textual data to discern recurring patterns of word co-occurrence, revealing underlying semantic themes (Busso et al. 2022; Blei, Ng, and Jordan 2003). This technique is particularly prevalent in text mining, where it serves to unveil hidden semantic structures in large volumes of textual data. Conceptually, topics can be understood as clusters of co-occurring terms, indicative of shared semantic domains within the text. The underlying assumption is that if a document pertains to a specific topic, words related to that topic will exhibit higher frequency compared to documents addressing other subjects. For example, in documents discussing dogs, terms like dog and bone are likely to feature prominently, while in documents focusing on cats, cat and meow would be more prevalent. Meanwhile, ubiquitous terms such as the and is are expected to occur with similar frequency across diverse topics, serving as noise rather than indicative signals of topic specificity. Various methods exist for determining topics within topic models. For instance, Gerlach, Peixoto, and Altmann (2018) and Hyland et al. (2021) advocate for an approach grounded in stochastic block models. However, most applications of topic models use Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) or Structural Topic Modeling (Roberts, Stewart, and Tingley 2016). LDA, in particular, emerges as a widely embraced technique for fitting topic models. It operates by treating each document as a blend of topics and each topic as a blend of words. Consequently, documents can exhibit content overlaps, akin to the fluidity observed in natural language usage, rather than being strictly segregated into distinct groups. Gillings and Hardie (2022) state that topic modelling is based on the following key assumptions: The corpus comprises a substantial number of documents. A topic is delineated as a set of words with varying probabilities of occurrence across the documents. Each document exhibits diverse degrees of association with multiple topics. The collection is structured by underlying topics, which are finite in number, organizing the corpus. Given the availability of vast amounts of textual data, topic models can help to organize and offer insights and assist in understanding large collections of unstructured text and they are widely used in natural language processing and computational text analytics. However, the use of topic modelling in discourse studies has received criticism (Brookes and McEnery 2019) due to the following issues: Thematic Coherence: While topic modeling can group texts into topics, the degree of thematic coherence varies. Some topics may be thematically coherent, but others may lack cohesion or accuracy in capturing the underlying themes present in the texts. Nuanced Perspective: Compared to more traditional approaches to discourse analysis, topic modeling often provides a less nuanced perspective on the data. The automatically generated topics may overlook subtle nuances and intricacies present in the texts, leading to a less accurate representation of the discourse. Distance from Reality: Brookes and McEnery (2019) suggest that the insights derived from topic modeling may not fully capture the “reality” of the texts. The topics generated by the model may not accurately reflect the complex nature of the discourse, leading to potential misinterpretations or oversimplifications of the data. Utility for Discourse Analysts: While topic modeling may offer a method for organizing and studying sizable data sets, Brookes and McEnery (2019) questions the utility for discourse analysts and suggests that traditional discourse analysis methods consistently provide a more nuanced and accurate perspective on the data compared to topic modeling approaches. This criticism is certainly valid if topic modeling is solely reliant on a purely data-driven approach without human intervention. In this tutorial, we will demonstrate how to combine data-driven topic modeling with human-supervised seeded methods to arrive at more reliable and accurate topics. 7.1 Getting started with Topic Modelling In this tutorial, we’ll explore a two-step approach to topic modeling. Initially, we’ll employ an unsupervised method to generate a preliminary topic model, uncovering inherent topics within the data. Subsequently, we’ll introduce a human-supervised, seeded model, informed by the outcomes of the initial data-driven approach. Following this (recommended) procedure, we’ll then delve into an alternative purely data-driven approach. Our tutorial begins by gathering the necessary corpus data. We’ll be focusing on analyzing the State of the Union Addresses (SOTU) delivered by US presidents, with the aim of understanding how the addressed topics have evolved over time. Given the length of these addresses (amounting to 231 in total), it’s important to acknowledge that document length can influence topic modeling outcomes. In cases where texts are exceptionally short (like Twitter posts) or long (such as books), adjusting the document units for modeling purposes can be beneficial—either by combining or splitting them accordingly. To tailor our approach to the SOTU speeches, we’ve chosen to model at the paragraph level instead of analyzing entire speeches at once. This allows for a more detailed analysis, potentially leading to clearer and more interpretable topics. We’ve provided a data set named sotu_paragraphs.rda, which contains the speeches segmented into paragraphs for easier analysis. 7.2 Human-in-the-loop Topic Modelling In this human-in-the-loop approach to topic modelling which mainly uses and combines the quanteda package (Benoit et al. 2018), the topicmodels package (Grün and Hornik 2024, 2011), and the seededlda package (Watanabe and Xuan-Hieu 2024). Now that we have cleaned the data, we can perform the topic modelling. This consists of two steps: First, we perform an unsupervised LDA. We do this to check what topics are in our corpus. Then, we perform a supervised LDA (based on the results of the unsupervised LDA) to identify meaningful topics in our data. For the supervised LDA, we define so-called seed terms that help in generating coherent topics. 7.3 Loading and preparing data When preparing the data for analysis, we employ several preprocessing steps to ensure its cleanliness and readiness for analysis. Initially, we load the data and then remove punctuation, symbols, and numerical characters. Additionally, we eliminate common stop words, such as the and and, which can introduce noise and hinder the topic modeling process. To standardize the text, we convert it to lowercase and, lastly, we apply stemming to reduce words to their base form. txts &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/sotu_paragraphs.rda&quot;, &quot;rb&quot;)) # inspect str(txts) ## &#39;data.frame&#39;: 8833 obs. of 6 variables: ## $ doc_id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ speech_doc_id: int 1 1 1 1 1 1 1 1 1 1 ... ## $ speech_type : Factor w/ 1 level &quot;State of the Union Address&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ president : Factor w/ 23 levels &quot;Abraham Lincoln&quot;,..: 7 7 7 7 7 7 7 7 7 7 ... ## $ date : chr &quot;1790-01-08&quot; &quot;1790-01-08&quot; &quot;1790-01-08&quot; &quot;1790-01-08&quot; ... ## $ text : chr &quot;Fellow-Citizens of the Senate and House of Representatives:&quot; &quot;I embrace with great satisfaction the opportunity which now presents itself\\nof congratulating you on the prese&quot;| __truncated__ &quot;In resuming your consultations for the general good you can not but derive\\nencouragement from the reflection t&quot;| __truncated__ &quot;Among the many interesting objects which will engage your attention that of\\nproviding for the common defense w&quot;| __truncated__ ... # load data txts &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/sotu_paragraphs.rda&quot;, &quot;rb&quot;)) txts$text %&gt;% # tokenize quanteda::tokens(remove_punct = TRUE, # remove punctuation remove_symbols = TRUE, # remove symbols remove_number = TRUE) %&gt;% # remove numbers # remove stop words quanteda::tokens_select(pattern = stopwords(&quot;en&quot;), selection = &quot;remove&quot;) %&gt;% # stemming quanteda::tokens_wordstem() %&gt;% # convert to document-frequency matrix quanteda::dfm(tolower = T) -&gt; ctxts # add docvars docvars(ctxts, &quot;president&quot;) &lt;- txts$president docvars(ctxts, &quot;date&quot;) &lt;- txts$date docvars(ctxts, &quot;speechid&quot;) &lt;- txts$speech_doc_id docvars(ctxts, &quot;docid&quot;) &lt;- txts$doc_id # clean data ctxts &lt;- dfm_subset(ctxts, ntoken(ctxts) &gt; 0) # inspect data ctxts[1:5, 1:5] ## Document-feature matrix of: 5 documents, 5 features (80.00% sparse) and 4 docvars. ## features ## docs fellow-citizen senat hous repres embrac ## text1 1 1 1 1 0 ## text2 0 0 0 0 1 ## text3 0 0 0 0 0 ## text4 0 0 0 0 0 ## text5 0 0 0 0 0 7.4 Initial unsupervised topic model Now that we have loaded and prepared the data for analysis, we will follow a two-step approach. First, we perform an unsupervised topic model using Latent Dirichlet Allocation (LDA) to identify the topics present in our data. This initial step helps us understand the broad themes and structure within the data set. Then, based on the results of the unsupervised topic model, we conduct a supervised topic model using LDA to refine and identify more meaningful topics in our data. This combined approach allows us to leverage both data-driven insights and expert supervision to enhance the accuracy and interpretability of the topics. In the initial step that implements a unsupervised, data-driven topic model, we vary the number of topics the LDA algorithm looks for until we identify coherent topics in the data. We use the LDA function from the topicmodels package instead of the textmodel_lda function from the seededlda package because the former allows us to include a seed. Including a seed ensures that the results of this unsupervised topic model are reproducible, which is not the case if we do not seed the model, as each model will produce different results without setting a seed. # generate model: change k to different numbers, e.g. 10 or 20 and look for consistencies in the keywords for the topics below. topicmodels::LDA(ctxts, k = 15, control = list(seed = 1234)) -&gt; ddlda Now that we have generated an initial data-driven model, the next step is to inspect it to evaluate its performance and understand the topics it has identified. To do this, we need to examine the terms associated with each detected topic. By analyzing these terms, we can gain insights into the themes represented by each topic and assess the coherence and relevance of the model’s output. # define number of topics ntopics = 15 # define number of terms nterms = 10 # generate table tidytext::tidy(ddlda, matrix = &quot;beta&quot;) %&gt;% dplyr::group_by(topic) %&gt;% dplyr::slice_max(beta, n = nterms) %&gt;% dplyr::ungroup() %&gt;% dplyr::arrange(topic, -beta) %&gt;% dplyr::mutate(term = paste(term, &quot; (&quot;, round(beta, 3), &quot;)&quot;, sep = &quot;&quot;), topic = paste(&quot;topic&quot;, topic), topic = factor(topic, levels = c(paste(&quot;topic&quot;, 1:ntopics))), top = rep(paste(&quot;top&quot;, 1:nterms), nrow(.)/nterms), top = factor(top, levels = c(paste(&quot;top&quot;, 1:nterms)))) %&gt;% dplyr::select(-beta) %&gt;% tidyr::spread(topic, term) -&gt; ddlda_top_terms ddlda_top_terms ## # A tibble: 10 × 16 ## top `topic 1` `topic 2` `topic 3` `topic 4` `topic 5` `topic 6` `topic 7` ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 top 1 state (0.… countri … state (0… govern (… state (0… state (0… state (0… ## 2 top 2 countri (… upon (0.… unite (0… will (0.… countri … will (0.… upon (0.… ## 3 top 3 will (0.0… present … congress… year (0.… will (0.… unite (0… will (0.… ## 4 top 4 congress … war (0.0… may (0.0… unite (0… congress… govern (… congress… ## 5 top 5 nation (0… can (0.0… treati (… law (0.0… public (… power (0… may (0.0… ## 6 top 6 can (0.00… unite (0… citizen … may (0.0… year (0.… law (0.0… govern (… ## 7 top 7 subject (… nation (… nation (… upon (0.… nation (… peopl (0… citizen … ## 8 top 8 govern (0… author (… great (0… act (0.0… can (0.0… last (0.… nation (… ## 9 top 9 land (0.0… may (0.0… territor… public (… law (0.0… duti (0.… import (… ## 10 top 10 made (0.0… subject … made (0.… last (0.… import (… part (0.… great (0… ## # ℹ 8 more variables: `topic 8` &lt;chr&gt;, `topic 9` &lt;chr&gt;, `topic 10` &lt;chr&gt;, ## # `topic 11` &lt;chr&gt;, `topic 12` &lt;chr&gt;, `topic 13` &lt;chr&gt;, `topic 14` &lt;chr&gt;, ## # `topic 15` &lt;chr&gt; In a real analysis, we would re-run the unsupervised model multiple times, adjusting the number of topics that the Latent Dirichlet Allocation (LDA) algorithm “looks for.” For each iteration, we would inspect the key terms associated with the identified topics to check their thematic consistency. This evaluation helps us determine whether the results of the topic model make sense and accurately reflect the themes present in the data. By varying the number of topics and examining the corresponding key terms, we can identify the optimal number of topics that best represent the underlying themes in our data set. However, we will skip re-running the model here, as this is just a tutorial intended to showcase the process rather than a comprehensive analysis. To obtain a comprehensive table of terms and their association strengths with topics (the beta values), follow the steps outlined below. This table can help verify if the data contains thematically distinct topics. Additionally, visualizations and statistical modeling can be employed to compare the distinctness of topics and determine the ideal number of topics. However, I strongly recommend not solely relying on statistical measures when identifying the optimal number of topics. In my experience, human intuition is still essential for evaluating topic coherence and consistency. # extract topics ddlda_topics &lt;- tidy(ddlda, matrix = &quot;beta&quot;) # inspect head(ddlda_topics, 20) ## # A tibble: 20 × 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 fellow-citizen 0.000249 ## 2 2 fellow-citizen 0.000351 ## 3 3 fellow-citizen 0.000416 ## 4 4 fellow-citizen 0.0000333 ## 5 5 fellow-citizen 0.0000797 ## 6 6 fellow-citizen 0.000183 ## 7 7 fellow-citizen 0.000445 ## 8 8 fellow-citizen 0.000306 ## 9 9 fellow-citizen 0.000381 ## 10 10 fellow-citizen 0.000332 ## 11 11 fellow-citizen 0.000187 ## 12 12 fellow-citizen 0.000147 ## 13 13 fellow-citizen 0.000278 ## 14 14 fellow-citizen 0.000336 ## 15 15 fellow-citizen 0.000205 ## 16 1 senat 0.000708 ## 17 2 senat 0.000477 ## 18 3 senat 0.00263 ## 19 4 senat 0.00118 ## 20 5 senat 0.000436 The purpose of this initial step, in which we generate data-driven unsupervised topic models, is to identify the number of coherent topics present in the data and to determine the key terms associated with these topics. These key terms will then be used as seed terms in the next step: the supervised, seeded topic model. This approach ensures that the supervised model is grounded in the actual thematic structure of the data set, enhancing the accuracy and relevance of the identified topics. 7.5 Supervised, seeded topic model To implement the supervised, seeded topic model, we start by creating a dictionary containing the seed terms we have identified in the first step. To check terms (to see if ), you can use the following code chunk: ddlda_topics %&gt;% select(term) %&gt;% unique() %&gt;% filter(str_detect(term, &quot;agri&quot;)) ## # A tibble: 3 × 1 ## term ## &lt;chr&gt; ## 1 agricultur ## 2 agriculturist ## 3 agricultural-colleg # semisupervised LDA dict &lt;- dictionary(list(military = c(&quot;armi&quot;, &quot;war&quot;, &quot;militari&quot;, &quot;conflict&quot;), liberty = c(&quot;freedom&quot;, &quot;liberti&quot;, &quot;free&quot;), nation = c(&quot;nation&quot;, &quot;countri&quot;, &quot;citizen&quot;), law = c(&quot;law&quot;, &quot;court&quot;, &quot;prison&quot;), treaty = c(&quot;claim&quot;, &quot;treati&quot;, &quot;negoti&quot;), indian = c(&quot;indian&quot;, &quot;tribe&quot;, &quot;territori&quot;), labor = c(&quot;labor&quot;, &quot;work&quot;, &quot;condit&quot;), money = c(&quot;bank&quot;, &quot;silver&quot;, &quot;gold&quot;, &quot;currenc&quot;, &quot;money&quot;), finance = c(&quot;debt&quot;, &quot;invest&quot;, &quot;financ&quot;), wealth = c(&quot;prosper&quot;, &quot;peac&quot;, &quot;wealth&quot;), industry = c(&quot;produc&quot;, &quot;industri&quot;, &quot;manufactur&quot;), navy = c(&quot;navi&quot;, &quot;ship&quot;, &quot;vessel&quot;, &quot;naval&quot;), consitution = c(&quot;constitut&quot;, &quot;power&quot;, &quot;state&quot;), agriculture = c(&quot;agricultur&quot;, &quot;grow&quot;, &quot;land&quot;), office = c(&quot;office&quot;, &quot;serv&quot;, &quot;duti&quot;))) tmod_slda &lt;- seededlda::textmodel_seededlda(ctxts, dict, residual = TRUE, min_termfreq = 2) # inspect seededlda::terms(tmod_slda) ## military liberty nation law treaty indian ## [1,] &quot;war&quot; &quot;free&quot; &quot;countri&quot; &quot;law&quot; &quot;treati&quot; &quot;territori&quot; ## [2,] &quot;militari&quot; &quot;peopl&quot; &quot;nation&quot; &quot;court&quot; &quot;claim&quot; &quot;indian&quot; ## [3,] &quot;armi&quot; &quot;can&quot; &quot;citizen&quot; &quot;case&quot; &quot;govern&quot; &quot;tribe&quot; ## [4,] &quot;forc&quot; &quot;must&quot; &quot;govern&quot; &quot;person&quot; &quot;negoti&quot; &quot;mexico&quot; ## [5,] &quot;offic&quot; &quot;govern&quot; &quot;foreign&quot; &quot;upon&quot; &quot;unite&quot; &quot;part&quot; ## [6,] &quot;servic&quot; &quot;upon&quot; &quot;american&quot; &quot;offic&quot; &quot;minist&quot; &quot;will&quot; ## [7,] &quot;men&quot; &quot;liberti&quot; &quot;right&quot; &quot;provis&quot; &quot;two&quot; &quot;texa&quot; ## [8,] &quot;command&quot; &quot;everi&quot; &quot;time&quot; &quot;may&quot; &quot;relat&quot; &quot;new&quot; ## [9,] &quot;conflict&quot; &quot;public&quot; &quot;upon&quot; &quot;execut&quot; &quot;british&quot; &quot;line&quot; ## [10,] &quot;order&quot; &quot;will&quot; &quot;properti&quot; &quot;author&quot; &quot;britain&quot; &quot;within&quot; ## labor money finance wealth industry navy ## [1,] &quot;condit&quot; &quot;bank&quot; &quot;year&quot; &quot;peac&quot; &quot;produc&quot; &quot;vessel&quot; ## [2,] &quot;work&quot; &quot;money&quot; &quot;debt&quot; &quot;prosper&quot; &quot;industri&quot; &quot;navi&quot; ## [3,] &quot;labor&quot; &quot;gold&quot; &quot;amount&quot; &quot;will&quot; &quot;manufactur&quot; &quot;ship&quot; ## [4,] &quot;report&quot; &quot;currenc&quot; &quot;expenditur&quot; &quot;us&quot; &quot;import&quot; &quot;naval&quot; ## [5,] &quot;depart&quot; &quot;silver&quot; &quot;treasuri&quot; &quot;peopl&quot; &quot;product&quot; &quot;coast&quot; ## [6,] &quot;congress&quot; &quot;govern&quot; &quot;increas&quot; &quot;great&quot; &quot;foreign&quot; &quot;construct&quot; ## [7,] &quot;secretari&quot; &quot;treasuri&quot; &quot;last&quot; &quot;interest&quot; &quot;trade&quot; &quot;port&quot; ## [8,] &quot;servic&quot; &quot;note&quot; &quot;fiscal&quot; &quot;everi&quot; &quot;increas&quot; &quot;sea&quot; ## [9,] &quot;recommend&quot; &quot;issu&quot; &quot;estim&quot; &quot;may&quot; &quot;articl&quot; &quot;commerc&quot; ## [10,] &quot;attent&quot; &quot;public&quot; &quot;revenu&quot; &quot;happi&quot; &quot;upon&quot; &quot;great&quot; ## consitution agriculture office other ## [1,] &quot;state&quot; &quot;land&quot; &quot;duti&quot; &quot;congress&quot; ## [2,] &quot;power&quot; &quot;agricultur&quot; &quot;will&quot; &quot;act&quot; ## [3,] &quot;constitut&quot; &quot;public&quot; &quot;may&quot; &quot;last&quot; ## [4,] &quot;unite&quot; &quot;grow&quot; &quot;subject&quot; &quot;session&quot; ## [5,] &quot;govern&quot; &quot;larg&quot; &quot;can&quot; &quot;repres&quot; ## [6,] &quot;right&quot; &quot;improv&quot; &quot;congress&quot; &quot;presid&quot; ## [7,] &quot;union&quot; &quot;year&quot; &quot;consider&quot; &quot;senat&quot; ## [8,] &quot;shall&quot; &quot;now&quot; &quot;object&quot; &quot;hous&quot; ## [9,] &quot;act&quot; &quot;acr&quot; &quot;measur&quot; &quot;day&quot; ## [10,] &quot;one&quot; &quot;time&quot; &quot;shall&quot; &quot;author&quot; Now, we extract files and create a data frame of topics and documents. This shows what topic is dominant in which file in tabular form. # generate data frame data.frame(tmod_slda$data$date, tmod_slda$data$president, seededlda::topics(tmod_slda)) %&gt;% dplyr::rename(Date = 1, President = 2, Topic = 3) %&gt;% dplyr::mutate(Date = stringr::str_remove_all(Date, &quot;-.*&quot;), Date = stringr::str_replace_all(Date, &quot;.$&quot;, &quot;0&quot;)) %&gt;% dplyr::mutate_if(is.character, factor) -&gt; topic_df # inspect head(topic_df) ## Date President Topic ## text1 1790 George Washington other ## text2 1790 George Washington wealth ## text3 1790 George Washington office ## text4 1790 George Washington labor ## text5 1790 George Washington wealth ## text6 1790 George Washington office Using the table (or data frame) we have just created, we can visualize the use of topics over time. topic_df %&gt;% dplyr::group_by(Date, Topic) %&gt;% dplyr::summarise(freq = n()) %&gt;% ggplot(aes(x = Date, y = freq, fill = Topic)) + geom_bar(stat=&quot;identity&quot;, position=&quot;fill&quot;, color = &quot;black&quot;) + theme_bw() + labs(x = &quot;Decade&quot;) + scale_fill_manual(values = rev(colorRampPalette(brewer.pal(8, &quot;RdBu&quot;))(ntopics+1))) + scale_y_continuous(name =&quot;Percent of paragraphs&quot;, labels = seq(0, 100, 25)) ## `summarise()` has grouped output by &#39;Date&#39;. You can override using the ## `.groups` argument. The figure illustrates the relative frequency of topics over time in the State of the Union (SOTU) texts. Notably, paragraphs discussing the topic of “office,” characterized by key terms such as office, serv, and duti, have become less prominent over time. This trend suggests a decreasing emphasis on this particular theme, as evidenced by the diminishing number of paragraphs dedicated to it. "],["citation-session-info.html", "Section 8 Citation &amp; Session Info", " Section 8 Citation &amp; Session Info Schweinberger, Martin. 2024. Introduction to R for Social Science. University of Eastern Finland, Joensuu. url: https://martinschweinberger.github.io/IntroR_WS (Version 2024.06.11). @manual{schweinberger2024introrss, author = {Schweinberger, Martin}, title = {Introduction to R for Social Science}, note = {https://martinschweinberger.github.io/IntroR_WS}, year = {2024}, organization = {University of Eastern Finland}, address = {Joensuu}, edition = {2024.06.11} } sessionInfo() ## R version 4.3.2 (2023-10-31 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 11 x64 (build 22621) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=English_Australia.utf8 LC_CTYPE=English_Australia.utf8 ## [3] LC_MONETARY=English_Australia.utf8 LC_NUMERIC=C ## [5] LC_TIME=English_Australia.utf8 ## ## time zone: Australia/Brisbane ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices datasets utils methods base ## ## other attached packages: ## [1] wordcloud2_0.2.1 wordcloud_2.6 ## [3] udpipe_0.8.11 topicmodels_0.2-16 ## [5] tm_0.7-13 NLP_0.2-1 ## [7] lubridate_1.9.3 forcats_1.0.0 ## [9] stringr_1.5.1 dplyr_1.1.4 ## [11] purrr_1.0.2 readr_2.1.5 ## [13] tidyr_1.3.1 tibble_3.2.1 ## [15] tidyverse_2.0.0 tidytext_0.4.2 ## [17] tidygraph_1.3.1 textdata_0.4.5 ## [19] sna_2.7-2 statnet.common_4.9.0 ## [21] slam_0.1-50 reshape2_1.4.4 ## [23] RColorBrewer_1.1-3 quanteda.textstats_0.97 ## [25] quanteda.textplots_0.94.4 quanteda_4.0.2 ## [27] network_1.18.2 Matrix_1.6-5 ## [29] ldatuning_1.0.2 lda_1.5.2 ## [31] igraph_2.0.3 gutenbergr_0.2.4 ## [33] ggraph_2.2.1 GGally_2.2.1 ## [35] ggplot2_3.5.0 flextable_0.9.6 ## ## loaded via a namespace (and not attached): ## [1] rstudioapi_0.16.0 jsonlite_1.8.8 magrittr_2.0.3 ## [4] modeltools_0.2-23 farver_2.1.1 rmarkdown_2.26 ## [7] fs_1.6.3 ragg_1.3.0 vctrs_0.6.5 ## [10] memoise_2.0.1 askpass_1.2.0 htmltools_0.5.8 ## [13] curl_5.2.1 janeaustenr_1.0.0 sass_0.4.9 ## [16] bslib_0.7.0 htmlwidgets_1.6.4 tokenizers_0.3.0 ## [19] plyr_1.8.9 cachem_1.0.8 uuid_1.2-0 ## [22] mime_0.12 lifecycle_1.0.4 pkgconfig_2.0.3 ## [25] R6_2.5.1 fastmap_1.1.1 shiny_1.8.1 ## [28] digest_0.6.35 colorspace_2.1-0 rprojroot_2.0.4 ## [31] textshaping_0.3.7 SnowballC_0.7.1 labeling_0.4.3 ## [34] fansi_1.0.6 timechange_0.3.0 polyclip_1.10-6 ## [37] compiler_4.3.2 here_1.0.1 fontquiver_0.2.1 ## [40] withr_3.0.0 viridis_0.6.5 ggstats_0.6.0 ## [43] highr_0.10 ggforce_0.4.2 MASS_7.3-60 ## [46] openssl_2.1.1 proxyC_0.3.4 rappdirs_0.3.3 ## [49] gfonts_0.2.0 tools_4.3.2 stopwords_2.3 ## [52] zip_2.3.1 httpuv_1.6.15 glue_1.7.0 ## [55] promises_1.2.1 grid_4.3.2 generics_0.1.3 ## [58] gtable_0.3.4 tzdb_0.4.0 data.table_1.15.2 ## [61] hms_1.1.3 xml2_1.3.6 utf8_1.2.4 ## [64] ggrepel_0.9.5 pillar_1.9.0 nsyllable_1.0.1 ## [67] later_1.3.2 tweenr_2.0.3 lattice_0.21-9 ## [70] klippy_0.0.0.9500 renv_1.0.5 tidyselect_1.2.1 ## [73] fontLiberation_0.1.0 knitr_1.45 fontBitstreamVera_0.1.1 ## [76] gridExtra_2.3 bookdown_0.38 crul_1.4.0 ## [79] stats4_4.3.2 xfun_0.43 graphlayouts_1.1.1 ## [82] stringi_1.8.3 yaml_2.3.8 evaluate_0.23 ## [85] httpcode_0.3.0 officer_0.6.5 gdtools_0.3.7 ## [88] cli_3.6.2 RcppParallel_5.1.7 xtable_1.8-4 ## [91] systemfonts_1.0.6 munsell_0.5.0 jquerylib_0.1.4 ## [94] Rcpp_1.0.12 coda_0.19-4.1 parallel_4.3.2 ## [97] assertthat_0.2.1 viridisLite_0.4.2 scales_1.3.0 ## [100] crayon_1.5.2 rlang_1.1.3 fastmatch_1.1-4 ## [103] seededlda_1.2.1 "],["references.html", "Section 9 References", " Section 9 References Benoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, and Akitaka Matsuo. 2018. “Quanteda: An r Package for the Quantitative Analysis of Textual Data.” Journal of Open Source Software 3 (30): 774. https://doi.org/https://doi.org/10.21105/joss.00774. Bernard, H. Russell, and Gery Ryan. 1998. “Text Analysis.” Handbook of Methods in Cultural Anthropology 613. Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. “Latent Dirichlet Allocation.” Journal of Machine Learning Research 3: 993–1022. https://doi.org/https://doi.org/10.5555/944919.944937. Brookes, Gavin, and Tony McEnery. 2019. “The Utility of Topic Modelling for Discourse Studies.” Discourse Studies 21 (1): 3–21. https://doi.org/10.1177/14614456188140. Busso, Luciana, Monika Petyko, Steven Atkins, and Tim Grant. 2022. “Operation Heron: Latent Topic Changes in an Abusive Letter Series.” Corpora 17 (2): 225–58. https://doi.org/https://doi.org/10.3366/cor.2022.0255. Egbert, Jesse, and Douglas Biber. 2019. “Incorporating Text Dispersion into Keyword Analysis.” Corpora 14 (1): 77–104. https://doi.org/10.3366/cor.2019.0162. Gerlach, Martin, Tiago P. Peixoto, and Eduardo G. Altmann. 2018. “A Network Approach to Topic Models.” Science Advances 4: eaar1360. https://doi.org/https://doi.org/10.1126/sciadv.aaq1360. Gillings, Mark, and Andrew Hardie. 2022. “The Interpretation of Topic Models for Scholarly Analysis: An Evaluation and Critique of Current Practice.” Digital Scholarship in the Humanities 38: 530–43. https://doi.org/10.1093/llc/fqac075. Grün, Bettina, and Kurt Hornik. 2011. “Topicmodels: An r Package for Fitting Topic Models.” Journal of Statistical Software 40 (13): 1–30. https://doi.org/10.18637/jss.v040.i13. ———. 2024. Topicmodels: Topic Models. https://CRAN.R-project.org/package=topicmodels. Hyland, Conor C., Yang Tao, Lida Azizi, Martin Gerlach, Tiago P. Peixoto, and Eduardo G. Altmann. 2021. “Multilayer Networks for Text Analysis with Multiple Data Types.” EPJ Data Science 10: 33. https://doi.org/https://doi.org/10.1140/epjds/s13688-021-00288-5. Kabanoff, Boris. 1997. “Introduction: Computers Can Read as Well as Count: Computer-Aided Text Analysis in Organizational Research.” Journal of Organizational Behavior, 507–11. Mohammad, Saif M, and Peter D Turney. 2013. “Crowdsourcing a Word-Emotion Association Lexicon.” Computational Intelligence 29 (3): 436–65. https://doi.org/https://doi.org/10.1111/j.1467-8640.2012.00460.x. Popping, Roel. 2000. Computer-Assisted Text Analysis. Sage. https://doi.org/https://doi.org/10.4135/9781849208741. Roberts, Margaret E., Brandon M. Stewart, and Dustin Tingley. 2016. “Navigating the Local Modes of Big Data: The Case of Topic Models.” In Computational Social Science: Discovery and Prediction, edited by R. Michael Alvarez, 51–97. Cambridge University Press. https://doi.org/https://doi.org/10.1017/cbo9781316257340.004. Silge, Julia, and David Robinson. 2017. Text Mining with r: A Tidy Approach. O’Reilly Media. Sönning, Lukas. 2023. “Evaluation of Keyness Metrics: Performance and Reliability.” Corpus Linguistics and Linguistic Theory 0. https://doi.org/doi:10.1515/cllt-2022-0116. Watanabe, Kohei, and Phan Xuan-Hieu. 2024. Seededlda: Seeded Sequential LDA for Topic Modeling. https://CRAN.R-project.org/package=seededlda. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
