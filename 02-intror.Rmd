# Introduction to R

> **UNFINISHED! WORK IN PROGRESS! I'M STILL WORKING ON THE CONTENT**

Here I want to show you how to get started with R. As such, this section is aimed at fresh users or beginners with the aim of showcasing how to set up a R session in RStudio, how to set up R projects, and how to do basic operations using R such as loading and manipulation tabular data and generating basic visualization using R. 


[Here](https://www.rstudio.com/resources/cheatsheets/) is an overview of cheat sheets for popular and frequently used packages provided by RStudio (cheat sheets are short overviews with explanations and examples of useful functions). If you already have experience with R, both @wickham2016r (see [here](https://r4ds.had.co.nz/)) and  @gillespie2016efficient (see [here](https://bookdown.org/csgillespie/efficientR/)) are highly recommendable and excellent resources for improving your coding abilities and workflows in R.


## Installing R and RStudio

* You have NOT yet installed **R** on your computer? 

  + You have a Windows computer? Then click [here](https://cran.r-project.org/bin/windows/base/R-4.0.2-win.exe) for downloading and installing R

  + You have a Mac? Then click [here](https://cran.r-project.org/bin/macosx/R-4.0.2.pkg) for downloading and installing R

* You have NOT yet installed **RStudio** on your computer?

  + Click [here](https://rstudio.com/products/rstudio/download/#download) for downloading and installing RStudio.


You can find a more elaborate explanation of how to download and install R and RStudio [here](https://gitlab.com/stragu/DSH/blob/master/R/Installation.md) that was created by the UQ library.



## Preparation

Before you actually open R or RStudio, there things to consider that make working in R much easier and give your workflow a better structure. 

Imagine it like this: when you want to write a book, you could simply take pen and paper and start writing *or* you could think about what you want to write about, what different chapters your book would consist of, which chapters to write first, what these chapters will deal with, etc. The same is true for R: you could simply open R and start writing code *or* you can prepare you session and structure what you will be doing.

### Before you start working with R/RStudio (Folder Structure and R projects)

Before actually starting with writing code, you should prepare the session by going through the following steps:

### Create a folder for your project

In that folder, create the following sub-folders (you can, of course, adapt this folder template to match your needs)

  - data (you do not create this folder for the present workshop as you can simply use the data folder that you downloaded for this workshop instead)
  - images
  - tables
  - docs

The folder for your project could look like the the one shown below.

```{r rstudio01, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='75%'}
knitr::include_graphics("https://slcladal.github.io/images/RStudio_newfolder.png")
```   

Once you have created your project folder, you can go ahead with RStudio.

### Open RStudio

This is what RStudio looks like when you first open it: 

```{r , echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("https://slcladal.github.io/images/RStudio_empty.png")
``` 

In RStudio, click on `File` 
  
```{r rstudio02, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='50%'}
knitr::include_graphics("https://slcladal.github.io/images/RStudio_file.png")
``` 

You can use the drop-down menu to create a `R project`

### R Projects

In RStudio, click on `New Project`
  
```{r rstudio05, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='50%'}
knitr::include_graphics("https://slcladal.github.io/images/RStudio_newfile.png")
``` 
  
Next, confirm by clicking `OK` and select `Existing Directory`.

Then, navigate to where you have just created the project folder for this workshop.
  
```{r rstudio06, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='30%'}
knitr::include_graphics("https://slcladal.github.io/images/RStudio_existingdirectory.png")
```  
  
Once you click on `Open`, you have created a new `R project` 
  
### R Notebooks
  
In this project, click on `File`
  
```{r rstudio10, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='50%'}
knitr::include_graphics("https://slcladal.github.io/images/RStudio_file.png")
``` 
  
Click on `New File` and then on `R Notebook` as shown below.

```{r rstudio12, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='50%'}
knitr::include_graphics("https://slcladal.github.io/images/RStudio_newnotebook.png")
```  

This `R Notebook` will be the file in which you do all your work.



### Updating R

In case you encounter issues when opening the R Notebook (e.g., if you receive an error message saying that you need to update packages which then do not install properly), you may have to *update your R version*.

To update your current R version to the recent release please copy the code chunk shown below into the console pane (the bottom left pane) and click on `Enter` to run the code. The code will automatically update your version of R to the most recent release. During the update, you may be asked to specify some options - in that case, you can simply click on *Accept* and *Next* and accept the default settings.

```{r updater, eval=F, warning=F, message=F}
# install installr package
install.packages("installr")
# load installr package
library(installr)
# update r
updateR()
```

### Optimizing R project options

When you work with projects, it is recommendable to control the so-called *environment*. This means that you make your R Project self-contained by storing all packages that are used in project in a library *in the R Project* (instead of in the general R library on your computer). Having a library in your R Project means that you can share your project folder wit other people and they will automatically have the same package versions that you have sued which makes your code more robust and reproducible.

So, how to create such an environment? You simply click on `Tools` (at the very top right of RStudio), then click on`Project Options` then click on `Environments` and then check `Use renv with this project`. Now, when you install packages, they will be installed in the package library (rather than the general R library on your computer).


### Getting started with R Notebooks

You can now start writing in this R Notebook. For instance, you could start by changing the title of the R Notebook and describe what you are doing (what this Notebook contains).

Below is a picture of what this document looked like when I started writing it.

```{r , echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("https://slcladal.github.io/images/RStudio_editMD.png")
```  

When you write in the R Notebook, you use what is called `R Markdown` which is explained below.


### R Markdown

The Notebook is an [R Markdown document](http://rmarkdown.rstudio.com/): a Rmd (R Markdown) file is more than a flat text document: it's a program that you can run in R and which allows you to combine prose and code, so readers can see the technical aspects of your work while reading about their interpretive significance. 

You can get a nice and short overview of the formatting options in R Markdown (Rmd) files [here](https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf).


R Markdown allows you to make your research fully transparent and reproducible! If a couple of years down the line another researcher or a journal editor asked you how you have done your analysis, you can simply send them the Notebook or even the entire R-project folder. 

As such, Rmd files are a type of document that allows to 

+ include snippets of code (and any outputs such as tables or graphs) in plain text while 

+ encoding the *structure* of your document by using simple typographical symbols to encode formatting (rather than HTML tags or format types such as *Main header* or *Header level 1* in Word).  

Markdown is really quite simple to learn and these resources may help:

+ The [Markdown Wikipedia page](https://en.wikipedia.org/wiki/Markdown) includes a very handy chart of the syntax.

+ John Gruber developed Markdown and his [introduction to the syntax](https://daringfireball.net/projects/markdown/syntax) is worth browsing.

+ This [interactive Markdown tutorial](http://www.markdowntutorial.com/) will teach you the syntax in a few minutes.

## R and RStudio Basics

RStudio is a so-called IDE - Integrated Development Environment. The interface provides easy access to R. The advantage of this application is that R programs and files as well as a project directory can be managed easily. The environment is capable of editing and running program code, viewing outputs and rendering graphics. Furthermore, it is possible to view variables and data objects of an R-script directly in the interface. 

### RStudio: Panes

The GUI - Graphical User Interface - that RStudio provides divides the screen into four areas that are called **panes**:

1. File editor
2. Environment variables
3. R console
4. Management panes (File browser, plots, help display and R packages).

The two most important are the R console (bottom left) and the File editor (or Script in the top left).
The Environment variables and Management panes are on the right of the screen and they contain: 

* **Environment** (top): Lists all currently defined objects and data sets
* **History** (top): Lists all commands recently used or associated with a project
* **Plots** (bottom): Graphical output goes here
* **Help** (bottom): Find help for R packages and functions.  Don't forget you can type `?` before a function name in the console to get info in the Help section. 
* **Files** (bottom): Shows the files available to you in your working directory

These RStudio panes are shown below.

```{r intro_01_17, fig.width=10, fig.height=8, echo=FALSE, warning=FALSE}
knitr::include_graphics("https://slcladal.github.io/images/RStudioscreenshot.png")
```

#### R Console (bottom left pane)

The console pane allows you to quickly and immediately execute R code. You can experiment with functions here, or quickly print data for viewing. 

Type next to the `>` and press `Enter` to execute. 

***

<div class="warning" style='padding:0.1em; background-color:#51247a; color:#f2f2f2'>
<span>
<p style='margin-top:1em; text-align:center'>
<b>EXERCISE TIME!</b></p>
<p style='margin-left:1em;'>
</p></span>
</div>

<div class="question">` 

1. You can use R like a calculator.  Try typing `2+8` into the **R console**.

<details>
  <summary>Answer</summary>
  ```{r calculator}
  2+8
  ```
  
</details>

</div>`

***

Here, the plus sign is the **operator**.  Operators are symbols that represent some sort of action.  However, R is, of course, much more than a simple calculator.  To use R more fully, we need to understand **objects**, **functions**, and **indexing** - which we will learn about as we go.

For now, think of *objects as nouns* and *functions as verbs*. 

#### Running commands from a script

To run code from a script, insert your cursor on a line with a command, and press `CTRL/CMD+Enter`.

Or highlight some code to only run certain sections of the command, then press `CTRL/CMD+Enter` to run.

Alternatively, use the `Run` button at the top of the pane to execute the current line or selection (see below).

```{r rstudio13, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='50%'}
knitr::include_graphics("https://slcladal.github.io/images/RStudio_run.png")
```  

#### Script Editor (top left pane)

In contrast to the R console, which quickly runs code, the Script Editor (in the top left) does not automatically execute code. The Script Editor allows you to save the code essential to your analysis.  You can re-use that code in the moment, refer back to it later, or publish it for replication.  


Now, that we have explored RStudio, we are ready to get started with R!

## Getting started with R

This section introduces some basic concepts and procedures that help optimize your workflow in R. 

### Setting up an R session

At the beginning of a session, it is common practice to define some basic parameters. This is not required or even necessary, but it may just help further down the line. This session preparation may include specifying options. In the present case, we 

+ want R to show numbers as numbers up to 100 decimal points (and not show them in mathematical notation (in mathematical notation, 0.007 would be represented as 0.7e^-3^))

+ want R to show maximally 100 results (otherwise, it can happen that R prints out pages-after-pages of some numbers).

Again, the session preparation is not required or necessary but it can help avoid errors. 

```{r, message=F, warning=F}
# set options
options(stringsAsFactors = F)                           
options(scipen = 100) 
options(max.print=100) 
```

In script editor pane of RStudio, this would look like this:

```{r , echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("https://slcladal.github.io/images/RStudio_preparation.png")
``` 

### Packages

When using R, most of the functions are not loaded or even installing automatically. Instead, most functions are in contained in what are called **packages**. 

R comes with about 30 packages ("base R").  There are over 10,000 user-contributed packages; you can discover these packages online.  A prevalent collection of packages is the Tidyverse, which includes ggplot2, a package for making graphics. 

Before being able to use a package, we need to install the package (using the `install.packages` function) and load the package (using the `library` function). However, a package only needs to be installed once(!) and can then simply be loaded. When you install a package, this will likely install several other packages it depends on.  You should have already installed tidyverse before the workshop. 

You must load the package in any new R session where you want to use that package.    Below I show what you need to type when you want to install the `tidyverse`, the `tidytext`,  the `quanteda`, the `readxl`, and the `tm` packages (which are the packages that we will need in this workshop).

```{r, eval=F, message=F, warning=F}
install.packages("tidyverse")
install.packages("tidytext")
install.packages("quanteda")
install.packages("readxl")
install.packages("tm")
install.packages("tokenizers")
install.packages("here")
install.packages("flextable")
# install klippy for copy-to-clipboard button in code chunks
install.packages("remotes")
remotes::install_github("rlesur/klippy")
```

To load these packages, use the `library` function which takes the package name as its main argument.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(quanteda)
library(readxl)
library(tm)
library(tokenizers)
library(here)
library(flextable)
# activate klippy for copy-to-clipboard button
klippy::klippy()
```

The session preparation section of your Rmd file will thus also state which packages a script relies on.

In script editor pane of RStudio, the code blocks that install and activate packages would look like this:

```{r echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("https://slcladal.github.io/images/RStudio_packages.png")
``` 

### Getting help

When working with R, you will encounter issues and face challenges. A very good thing about R is that it provides various ways to get help or find information about the issues you face.

#### Finding help within R

To get help regrading what functions a package contains, which arguments a function takes or to get information about how to use a function, you can use the `help` function or the `apropos`. function or you can simply type a `?` before the package or two `??` if this does not give you any answers. 

```{r intro_01_11, eval=F, warning=F, message=F}
help(tidyverse) 
apropos("tidyverse")
?require
```

There are also other "official" help resources from R/RStudio. 

* Read official package documentation, see vignettes, e.g., Tidyverse <https://cran.r-project.org/package=tidyverse>

* Use the RStudio Cheat Sheets at <https://www.rstudio.com/resources/cheatsheets/>

* Use the RStudio Help viewer by typing `?` before a function or package

* Check out the keyboard shortcuts `Help` under `Tools` in RStudio for some good tips 

#### Finding help online

One great thing about R is that you can very often find an answer to your question online.

* Google your error! See <http://r4ds.had.co.nz/introduction.html#getting-help-and-learning-more> for excellent suggestions on how to find help for a specific question online.

# Working with Text

In this section, we will learn how to work with textual data in R and we use positive IMDB reviews as our example texts. Before we start, it is important to understand the general logic of R code which is why we start with a very brief explanation of functions and objects.

## Functions and Objects

In R, functions always have the following form: `function(argument1, argument2, ..., argumentN)`. Typically a function does something to an object (e.g. a table), so that the first argument typically specifies the data to which the function is applied. Other arguments then allow to add some information. Just as a side note, functions are also objects that do not contain data but instructions.

To assign content to an object, we use `<-` or `=` so that the we provide a name for an object, and then assign some content to it. For example, `MyObject <- 1:20` means *Create an object called `MyObject`. this object should contain the numbers 1 to 20*.

```{r, message=FALSE, warning=FALSE}
# generate an object
MyObject <- 1:20
# inspecting my object
MyObject

```

## Inspecting data

There are many ways to inspect data. We will briefly go over the most common ways to inspect data.

The `head` function takes the data-object as its first argument and automatically shows the first 6 elements of an object (or rows if the data-object has a table format). In contrast, the `str` function shows the structure of an object.

```{r, message=FALSE, warning=FALSE}
# inspect first six elements of my object
head(MyObject)
# inspect structure of my object
str(MyObject)
```

Next, we will learn how to load texts into R.

## Loading text data

There are many functions that we can use to load text data into R. For example, we can use the `readLines` function as shown below.


```{r, message=F, warning=F}
text <- readLines(here::here("data", "reviews_pos/textpos1.txt"))
# inspect first text element
text
```

To load many texts, we can use a loop to read all texts in a folder as shown below. In a first step, we define the paths of the texts and then, we use the `map_chr` function from the `purrr` package to loop over the paths and read them into R. In addition, we add names to the texts based on the paths from where the texts were loaded.


```{r load_texts_tidy,  message=F, warning=F}
# define paths
reviews_pos <- list.files(here::here("data/reviews_pos"), full.names = T, pattern = ".*txt") %>%
  # load data
  purrr::map_chr(~ readr::read_file(.))
# add names
names(reviews_pos) <- list.files(here::here("data/reviews_pos"), pattern = ".*txt") %>%
  stringr::str_remove_all(".txt")
# inspect first text element
reviews_pos[1]
```


## Saving text data

To save a single text file, we can use `writeLines` function which only needs the text and the location where the text should be saved as its arguments.


```{r eval=F, message=F, warning=F}
writeLines(reviews_pos[1], here::here("data", "review_pos_text1.txt"))
```


To save many text files on your computer, you need to first define the locations where you want to save the texts and then, in a second step, you save the files (as shown below).

> IMPORTANT: I have created a folder called `output` in my data folder in which the texts will be saved!


```{r}
# define where to save each file
outs <- file.path(paste0(here::here(), "/", "data/output", "/", names(reviews_pos), ".txt", sep = ""))
head(outs)
```




> IMPORTANT: I have set the chunk attribute eval to F (FALSE) so that this chunk is not executed automatically. To run the code chunk, please just click the green "play button" in the top right corner of the code chunk.

```{r eval = F}
# save the files
lapply(seq_along(reviews_pos), function(i) 
       writeLines(reviews_pos[[i]],  
       con = outs[i]))
```

## Piping 

Piping is done using the `%>%` sequence and it can be translated as **and then**. In the example below, we take the existing object (text) *and then* we convert it to upper case  **and then** we store the result in a new object (text2).

```{r, message=FALSE, warning=FALSE}
text %>%
  toupper() -> text2
# inspect data
text2
```


# Basic String Processing

Before turning to more advanced string processing (in the context of computation, texts are referred to as strings) using the `stringr` package, let us just  focus on some basic functions that are extremely useful when working with texts.

To practice working with texts, we will rename a shirt positive IMDB review and also generate a tokenized version of this review.

**Tokenizsation**: Tokenization is the process of breaking down text into smaller units called tokens. These tokens can be symbols, words, phrases, sentences, or other meaningful elements (e.g. paragraphs). Tokenization is a fundamental step in natural language processing (NLP) and text analysis, as it allows for the text to be analysed and processed systematically..

```{r}
review <- reviews_pos[4]
review_tok <- quanteda::tokens(review) %>% unlist() %>% as.vector()
# inspect
review; str(review_tok)
```

We can now start to check out functions that are useful and frequently applied to text data. We start by splitting text data.

To split texts, we can use the `str_split` function. However, there are two issues when using this (very useful) function:

  + the pattern that we want to split on disappears 

  + the output is a list (a special type of data format)

To remedy these issues, we 

  + combine the `str_split` function with the `unlist` function 

  + add something right at the beginning of the pattern that we use to split the text. 
To add something to the beginning of the pattern that we want to split the text by, we use the `str_replace_all` function. The `str_replace_all` function takes three arguments, 1. the **text**, 2. the **pattern** that should be replaced, 3. the **replacement**. In the example below, we add `~~~` to the sequence `movie` and then split on the `~~~` rather than on the sequence "movie" (in other words, we replace `movie` with `~~~movie` and then split on `~~~`).

```{r, message=FALSE, warning=FALSE}
# split text
str_split(
  # attach ~~~ right before where we want to split (we want to split before the token "movie")
    stringr::str_replace_all(reviews_pos, "movie", "~~~movie"),
    # define that we want to split by ~~~
    pattern = "~~~") %>%
  # unlist results
  unlist() -> reviews_pos_split
# inspect data
nchar(reviews_pos_split); str(reviews_pos_split)
```


A very useful function is, e.g. `tolower` which converts everything to lower case. 

```{r lwr, message=FALSE, warning=FALSE}
tolower(review)
```

Conversely, `toupper` converts everything to upper case. 

```{r upr, message=FALSE, warning=FALSE}
toupper(review)
```


The `stringr` package (see [here](https://raw.githubusercontent.com/rstudio/cheatsheets/main/strings.pdf) is part of the so-called *tidyverse* - a collection of packages that allows to write R code in a readable manner - and it is the most widely used package for string processing in . The advantage of using `stringr` is that it makes string processing very easy. All `stringr` functions share a common structure:

`str_function(string, pattern)`

The two arguments in the structure of `stringr` functions are:  *string* which is the character string to be processed and a pattern which is either a simple sequence of characters, a regular expression, or a combination of both. Because the *string* comes first, the `stringr` functions are ideal for piping and thus use in tidyverse style R. 

All function names of `stringr` begin with str, then an underscore and then the name of the action to be performed.  For example, to replace the first occurrence of a pattern in a string, we should use `str_replace()`. In the following, we will use `stringr` functions to perform various operations on the example text. As we have already loaded the `tidyverse` package, we can start right away with using `stringr` functions as shown below.

Like `nchar` in `base`, `str_count` provides the number of characters of a text.

```{r stringr2, message=FALSE, warning=FALSE}
str_count(reviews_pos[1:5])
```

The function `str_detect` informs about whether a pattern is present in a text and outputs a logical vector with *TRUE* if the pattern occurs and *FALSE* if it does not.

```{r stringr3, message=FALSE, warning=FALSE}
str_detect(review, "and")
```	

The function `str_extract_all` extracts all occurrences of a pattern, if that pattern is present in a text.

```{r stringr5, message=FALSE, warning=FALSE}
str_extract_all(review, "and")
```	

The function `str_locate_all` provides the start and end positions of the match of the pattern in a text and displays the result in matrix-form.

```{r stringr7, message=FALSE, warning=FALSE}
str_locate_all(review, "and")
```

The function `str_remove` removes the first occurrence of a pattern in a text.

```{r stringr10, message=FALSE, warning=FALSE}
str_remove(review, "and") 
```

The function `str_remove_all` removes all occurrences of a pattern from a text.

```{r stringr11, message=FALSE, warning=FALSE}
str_remove_all(review, "and")
```


The function `str_replace_all` replaces all occurrences of a pattern with something else in a text.

```{r stringr13, message=FALSE, warning=FALSE}
str_replace_all(review, "and", "AND")
```


Like `strsplit`, the function `str_split` splits a text when a given pattern occurs. If no pattern is provided, then the text is split into individual symbols.

```{r stringr16, message=FALSE, warning=FALSE}
str_split(review, "and") 
```

The function `str_split_fixed` splits a text when a given pattern occurs but only so often as is indicated by the argument `n`. So, even if the patter occur more often than `n`, `str_split_fixed` will only split the text `n` times.

```{r stringr17, message=FALSE, warning=FALSE}
str_split_fixed(review, "and", n = 3)
```



The function `str_subset` extracts those subsets of a text that contain a certain pattern.  

```{r stringr18, message=FALSE, warning=FALSE}
str_subset(review_tok, "and") 
```

The function `str_which` provides a vector with the indices of the texts that contain a certain pattern. 

```{r stringr19, message=FALSE, warning=FALSE}
str_which(review_tok, "and")
```

The function `str_view_all` shows the locations of all instances of a pattern in a text or vector of texts.

```{r stringr21, message=FALSE, warning=FALSE}
str_view_all(review, "and")
```

The function `str_pad` adds white spaces to a text or vector of texts so that they reach a given number of symbols.

```{r stringr22, message=FALSE, warning=FALSE}
# create text with white spaces
text <- " this    is a    text   "
str_pad(text, width = 30)
```

The function `str_trim` removes white spaces from the beginning(s) and end(s) of a text or vector of texts.

```{r stringr23, message=FALSE, warning=FALSE}
str_trim(text) 
```

The function `str_squish` removes white spaces that occur within a text or vector of texts.

```{r stringr24, message=FALSE, warning=FALSE}
str_squish(text)
```

The function `str_order` provides a vector that represents the order of a vector of texts according to the lengths of texts in that vector.

```{r stringr26, message=FALSE, warning=FALSE}
str_order(reviews_pos_split)
```

The function `str_sort` orders of a vector of texts according to the lengths of texts in that vector.

```{r stringr27, message=FALSE, warning=FALSE}
str_sort(reviews_pos_split) %>%
  head()
```


The function `str_c` combines texts into one text

```{r stringr30, message=FALSE, warning=FALSE}
str_c(review, reviews_pos[7])
```

The function `str_conv` converts a text into a certain type of encoding, e.g. into `UTF-8` or `Latin1`.

```{r stringr31, message=FALSE, warning=FALSE}
str_conv(review, encoding = "UTF-8")
```



The function `str_flatten` combines a vector of texts into one text. The argument `collapse` defines the symbol that occurs between the combined texts. If the argument `collapse` is left out, the texts will be combined without any symbol between the combined texts.


```{r stringr33, message=FALSE, warning=FALSE}
str_flatten(c(review, reviews_pos[7]), collapse = " ")
```


The function `str_length` provides the length of texts in characters.

```{r stringr35, message=FALSE, warning=FALSE}
str_length(review)
```


The function `str_sub` extracts a string from a text from a start location to an end position (expressed as character positions).

```{r stringr38, message=FALSE, warning=FALSE}
str_sub(review, 5, 25)
```

The function `word` extracts words from a text (expressed as word positions).


```{r stringr39, message=FALSE, warning=FALSE}
word(review, 2:7)
```


The function `str_glue` combines strings and allows to input variables.

```{r stringr40, message=FALSE, warning=FALSE}
name <- "Fred"
age <- 50
anniversary <- as.Date("1991-10-12")
str_glue(
  "My name is {name}, ",
  "my age next year is {age + 1}, ",
  "and my anniversary is {format(anniversary, '%A, %B %d, %Y')}."
)
```



## Extracting frequency information from text

Frequency lists are very basic but also important when analysing text. Fortunately, it is very easy to extract frequency information and to create frequency lists with R. We can do this by first using the `unnest_tokens`  function which splits texts into individual words, an then use the `count` function to get the raw frequencies of all word types in a text.

```{r, message=FALSE, warning=FALSE}
reviews_pos %>%
  # convert to data frame
  as.data.frame()%>%
  # give name column with text
  dplyr::rename(text = 1) %>%
  # tokenise
  tidytext::unnest_tokens(word, text) %>%
  # count tokens
  dplyr::count(word, sort=T) %>%
  # inspect 20 most frequent tokens
  head(20)
```

Extracting N-grams is also very easy as the `unnest_tokens`  function can an argument called `token` in which we can specify that we want to extract n-grams, If we do this, then we need to specify the `n` as a separate argument. Below we specify that we want the frequencies of all 4-grams.

```{r, message=FALSE, warning=FALSE}
reviews_pos %>%
    # convert to data frame
  as.data.frame()%>%
  # give name column with text
  dplyr::rename(text = 1) %>%
  # clean data
  dplyr::mutate(text = str_remove_all(text, "<.*?>")) %>%
  # tokenise and extract trigrams
  tidytext:: unnest_tokens(word, text, token="ngrams", n=3) %>%
  # count tokens
  dplyr::count(word, sort=T) %>%
  # inspect ten most frequent tri-grams
  head(10)
```

## Regular Expressions

In  this section, we focus on regular expressions (to learn more about regular expression, have a look at this very recommendable  [tutorial](https://stringr.tidyverse.org/articles/regular-expressions.html)). Regular expressions are powerful tools used to search and manipulate text patterns. They provide a way to find specific sequences of characters within larger bodies of text. 

There are two basic types of regular expressions:

* regular expressions that stand for  frequencies (quantifiers)

* regular expressions that stand for classes of symbols (types)

The regular expressions below show the first type of regular expressions, i.e. quantifiers.

```{r regex1, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='95%'}
knitr::include_graphics("https://slcladal.github.io/images/regex_tb1.png")
```  



The regular expressions below show the second type of regular expressions, i.e. types.

```{r regex2, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='95%'}
knitr::include_graphics("https://slcladal.github.io/images/regex_tb2.png")
```  

Types can be expanded to include structural properties as shown below.

```{r regex3, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='95%'}
knitr::include_graphics("https://slcladal.github.io/images/regex_tb3.png")
``` 

## Practice: regular expressions

We now want to show all words in the tokenized review that contain `y`.

```{r regex10, message=FALSE, warning=FALSE}
review_tok[str_detect(review_tok, "[y]")]
```

Show all words in the split tokenized review  that begin with a lower case `a`.

```{r regex11, message=FALSE, warning=FALSE}
review_tok[str_detect(review_tok, "^a")]
```

Show all words in the split tokenized review  that end in a lower case `s`.

```{r regex12,  message=FALSE, warning=FALSE}
review_tok[str_detect(review_tok, "s$")]
```

Show all words in the split tokenized review  in which there is an `e`, then any other character, and than another `n`.

```{r regex13, message=FALSE, warning=FALSE}
review_tok[str_detect(review_tok, "o..y")]
```

Show all words in the tokenized review  text in which there is an `e`, then two other characters, and than another `n`.

```{r regex14, message=FALSE, warning=FALSE}
review_tok[str_detect(review_tok, "o.{2,2}y")]
```

Show all words that consist of exactly three alphabetical characters in the tokenized review .

```{r regex15, message=FALSE, warning=FALSE}
review_tok[str_detect(review_tok, "^[:alpha:]{3,3}$")]
```

Show all words that consist of six or more alphabetical characters in the tokenized review.

```{r regex16, message=FALSE, warning=FALSE}
review_tok[str_detect(review_tok, "^[:alpha:]{6,}$")]
```

Replace all lower case `a`s with upper case `E`s in the review.

```{r regex17, message=FALSE, warning=FALSE}
str_replace_all(review, "a", "E")
```

Remove all non-alphabetical characters in the tokenized review.

```{r regex18, message=FALSE, warning=FALSE}
str_remove_all(review_tok, "\\W")
```

Remove all white spaces in the  review.

```{r regex19, message=FALSE, warning=FALSE}
str_remove_all(review, " ")
```



# Advanced String Processing

Above, we have used functions and regular expressions to extract and find patters in textual data. Here, we will focus on common methods for cleaning text data that are applied before implementing certain methods.

We start by installing and then loading some additional packages, e.g., the `quanteda` (see [here](https://raw.githubusercontent.com/rstudio/cheatsheets/main/quanteda.pdf) for a cheat sheet for the `quanteda` package), the `tm`, and the `udpipe` package, which are extremely useful when dealing with more advanced text processing.


```{r atp0, eval = F, message=F, warning=F}
install.packages("quanteda")
install.packages("tm")
install.packages("udpipe")
```


```{r atp1, message=F, warning=F}
library(quanteda)
library(tm)
library(udpipe) 
```

One common procedure is to split texts into sentences which we can do by using, e.g., the  `tokenize_sentence` function from the `quanteda` package. I also unlist the data to have a vector wot work with (rather than a list).


```{r atp3, message=F, warning=F}
et_sent <- quanteda::tokenize_sentence(review) %>%
  unlist()
# inspect
et_sent
```

Another common procedure is to remove stop words, i.e., words that do not have semantic or referential meaning (like nouns such as *tree* or *cat*, or verbs like *sit* or *speak* or adjectives such as *green* or *loud*) but that indicate syntactic relations, roles, or features.(e.g., articles and pronouns). We can remove stopwords using, e.g., the  `removeWords` function from the `tm` package


```{r atp5, message=F, warning=F}
et_wostop <-  tm::removeWords(review, tm::stopwords("english"))
# inspect
et_wostop
```

To remove the superfluous white spaces, we can use, e.g., the  `stripWhitespace` function from the `tm` package.

```{r atp6, message=F, warning=F}
et_wows <-  tm::stripWhitespace(et_wostop)
# inspect
et_wows
```

It can also be useful to remove numbers. We can do this using, e.g., the  `removeNumbers` function from the `tm` package.


```{r atp7, message=F, warning=F}
et_wonum <-  tm::removeNumbers("This is the 1 and only sentence I will write in 2022.")
# inspect
et_wonum
```

We may also want to remove any type of punctuation using, e.g., the  `removePunctuation` function from the `tm` package.



```{r atp9, message=F, warning=F}
et_wopunct <-  tm::removePunctuation(review)
# inspect
et_wopunct
```

## Stemming

Stemming is the process of reducing a word to its base or root form, typically by removing suffixes. For example, *running* becomes *run* and *jumps* becomes *jump*. This helps in normalizing words to their core meaning for text analysis and natural language processing tasks. We can stem a text using, e.g., the  `stemDocument` function from the `tm` package


```{r atp11, message=F, warning=F}
et_stem <-  tm::stemDocument(review, language = "en")
# inspect
et_stem
```

## Part-of-speech tagging and dependency parsing

A far better option than stemming is lemmatization as lemmatization is based on proper morphological information and vocabularies. For lemmatization, we can use the `udpipe` package which also tokenizes texts, adds part-of-speech tags, and provides information about dependency relations. 

Before we can tokenize, lemmatize, pos-tag and parse though, we need to download a pre-trained language model.

```{r atp12, eval = F, message=F, warning=F}
# download language model
m_eng   <- udpipe::udpipe_download_model(language = "english-ewt")
```


If you have downloaded a model once, you can also load the model directly from the place where you stored it on your computer. In my case, I have stored the model in a folder called udpipemodels

```{r atp14, message=F, warning=F}
# load language model from your computer after you have downloaded it once
m_eng <- udpipe::udpipe_load_model(file = here::here("english-ewt-ud-2.5-191206.udpipe"))
```

We can now use the model to annotate out text.

```{r atp17, message=F, warning=F}
# tokenise, tag, dependency parsing
review_annotated <- udpipe::udpipe_annotate(m_eng, x = review) %>%
  as.data.frame() %>%
  dplyr::select(-sentence)
# inspect
head(review_annotated, 10)
```

We could, of course, perform many more manipulations of textual data but this should suffice to get you started.

Now, we combine the pos-tagged text back into a text.

```{r}
review_annotated %>%
   as.data.frame() %>%
  dplyr::summarise(postxt = paste(token, "/", xpos, collapse = " ", sep = "")) %>%
  dplyr::pull(unique(postxt)) -> review_postagged
# inspect
review_postagged
```


Visualising the dependencies (syntactic )

```{r}
sent <- udpipe::udpipe_annotate(m_eng, x = "John gave the bear an apple") %>%
  as.data.frame()
# generate dependency plot
dplot <- textplot::textplot_dependencyparser(sent, size = 3) 
# show plot
dplot
```



## Example: Data processing chain

When working with texts, we usually need to clean the data. Below, we do some  basic cleaning using a pipeline or data processing chain. In a data processing chain, we combine commands to achieve an end result (in our case, a clean text).

```{r, message=FALSE, warning=FALSE}
reviews_pos_split_clean <- reviews_pos_split %>%
  # replace elements
  stringr::str_replace_all("<.*?>", " ") %>%
  # convert to lower case
  tolower() %>%
  # remove strange symbols
  stringr::str_replace_all("[^[:alnum:][:punct:]]+", " ") %>%
  # remove \"
  stringr::str_remove_all("\"") %>%
  # remove superfluous white spaces
  stringr::str_squish()
# remove very short elements
reviews_pos_split_clean <- reviews_pos_split_clean[nchar(reviews_pos_split_clean) > 10]
# inspect data
nchar(reviews_pos_split_clean)
```

Inspect text

```{r, eval = F, echo=T, message=FALSE, warning=FALSE}
reviews_pos_split_clean[1]
```

## Concordancing and KWICs

Creating concordances or key-word-in-context displays is one of the most common practices when dealing with text data. Fortunately, there exist ready-made functions that make this a very easy task in R. We will use the `kwic` function from the `quanteda` package to create kwics here. 

```{r, message=FALSE, warning=FALSE}
kwic_multiple <- quanteda::kwic(reviews_pos_split_clean, 
       pattern = phrase("audience"),
       window = 3, 
       valuetype = "regex") %>%
  as.data.frame()
# inspect data
head(kwic_multiple)
```

We can now also select concordances based on specific features. For example, we only want those instances of "great again" if the preceding word was "America". 

```{r, message=FALSE, warning=FALSE}
kwic_multiple_select <- kwic_multiple %>%
  # last element before search term is "the"
  dplyr::filter(str_detect(pre, "the$"))
# inspect data
head(kwic_multiple_select)
```

Again, we can use the `write.table` function to save our kwics to disc.

```{r, eval = F, message=FALSE, warning=FALSE}
write.table(kwic_multiple_select, here::here("data", "kwic_multiple_select.txt"), sep = "\t")
```

As most of the data that we use is on out computers (rather than being somewhere on the web), we now load files with text from your computer. 

We now turn to data visualization basics.

# Basics of Data Visualization

There are numerous function in R that we can use to visualize data. We will use the `ggplot` function from the `ggplot2` package here to visualize the data. 

The `ggplot2` package was developed by Hadley Wickham in 2005 and it implements the graphics scheme described in the book *The Grammar of Graphics* by Leland Wilkinson.

The idea behind the  *Grammar of Graphics* can be boiled down to 5 bullet points (see Wickham 2016: 4):

- a statistical graphic is a mapping from data to **aes**thetic attributes (location, color, shape, size) of **geom**etric objects (points, lines, bars). 

- the geometric objects are drawn in a specific **coord**inate system.

- **scale**s control the mapping from data to aesthetics and provide tools to read the plot (i.e., axes and legends).

- the plot may also contain **stat**istical transformations of the data (means, medians, bins of data, trend lines).

- **facet**ing can be used to generate the same plot for different subsets of the data.


## Basics of ggplot2 syntax 

**Specify data, aesthetics and geometric shapes** 

`ggplot(data, aes(x=, y=, color=, shape=, size=)) +`   
`geom_point()`, or `geom_histogram()`, or `geom_boxplot()`, etc.   

- This combination is very effective for exploratory graphs. 

- The data must be a data frame.

- The `aes()` function maps columns of the data frame to aesthetic properties of geometric shapes to be plotted.

- `ggplot()` defines the plot; the `geoms` show the data; each component is added with `+` 

- Some examples should make this clear

Before we start plotting, we will load tabular data (information about the speakers in the dinner conversations) and the prepare the data that we want to visualize. From this table, we create two versions:

* a basic cleaned version 

* a summary table with the mean word counts by gender and age.

```{r, message=F, warning=F}
# create clean basic table 
readxl::read_excel(here::here("data", "ICEdata.xlsx")) %>%
  # only private dialogue
  dplyr::filter(stringr::str_detect(text.id, "S1A"),
                # without speaker younger than 19
                age != "0-18",
                age != "NA",
                sex != "NA",
                word.count > 10) %>%
  dplyr::mutate(age = factor(age),
                sex = factor(sex),
                date = factor(date)) -> pdat

# create summary table
readxl::read_excel(here::here("data", "ICEdata.xlsx")) %>%
  # only private dialogue
  dplyr::filter(stringr::str_detect(text.id, "S1A"),
                # without speaker younger than 19
                age != "0-18",
                age != "NA") %>%
  dplyr::group_by(sex, age) %>%
  dplyr::summarise(words = mean(word.count)) -> psum
# inspect
head(pdat); head(psum)
```


We will now create some basic visualizations or plots.

## Line plot

In the example below, we specify that we want to visualize the `plotdata` and that the x-axis should represent `Age` and the y-axis `Words`(the mean frequency of words). We also tell R that we want to group the data by `Sex` (i.e. that we want to distinguish between men and women). Then, we add `geom_line` which tells R that we want a line graph. The result of this is shown below. 

```{r, message=FALSE, warning=FALSE}
ggplot(psum, aes(x = age, y = words, group = sex, color = sex)) +
  geom_line()
```

## Prettifying plots 

Once you have a basic plot like the one above, you can prettify the plot. For example, you can 

+ change the width of the lines (`size = 1.25`)

+ change the y-axis limits (`coord_cartesian(ylim = c(0, 1000)) `)

+ use a different theme (`theme_bw()` means black and white theme)

+ move the legend to the top

+ change the default colors to colors you like (*scale_color_manual ...`)

+ change the linetype (`scale_linetype_manual ...`)

```{r, message=FALSE, warning=FALSE}
psum %>%
  ggplot(aes(x = age, y = words, color = sex,  group = sex, linetype = sex)) +
  geom_line(size = 1.25) +
  coord_cartesian(ylim = c(0, 1500)) +
  theme_bw() + 
  theme(legend.position = "top") + 
  scale_color_manual(breaks = c("female", "male"),
                     values = c("gray20", "gray50")) +
  scale_linetype_manual(breaks = c("female", "male"),
                        values = c("solid", "dotted")) +
  labs(x = "Age groups", y = "Average word count\n(per 1,000 words)")
```

## Combining plots

An additional and very handy feature of this way of producing graphs is that you 

+ can integrate them into pipes 

+ can easily combine plots.


Below, we first generate a scatter plot with `geom_jitter` and then add a line plot by summarising the data.

```{r, message=FALSE, warning=FALSE}
pdat %>%
  ggplot(aes(x = age, 
             y = word.count,
             color = sex,
             linetype = sex)) +
  geom_jitter(alpha = .5, width = .2) +
  stat_summary(fun=mean, geom="line", aes(group=sex)) +
  coord_cartesian(ylim = c(0, 2500)) +
  theme_bw() + 
  theme(legend.position = "top") + 
  scale_color_manual(breaks = c("female", "male"),
                     values = c("indianred", "darkblue")) +
  scale_linetype_manual(breaks = c("female", "male"),
                        values = c("solid", "dotted"))
```

## Boxplots

 Below, we generate a boxplot using `geom_box` and break it up into different facets. 

```{r, message=FALSE, warning=FALSE}
pdat %>%
  ggplot(aes(x = age, 
             y = word.count, 
             fill = sex)) +
  facet_grid(vars(date)) +
  geom_boxplot() +
  coord_cartesian(ylim = c(0, 2000)) +
  theme_bw() + 
  theme(legend.position = "top") + 
  scale_fill_manual(breaks = c("female", "male"),
                     values = c("#E69F00", "#56B4E9"))
```

## Barplots

To exemplify a bar plot, we generate a plot with `geom_bar` which shows the number of men and women by `Date`.

```{r, message=FALSE, warning=FALSE}
pdat %>%
  dplyr::select(date, sex, text.id) %>%
  unique() %>%
  dplyr::group_by(date, sex) %>%
  dplyr::summarize(speakers = n()) %>%
  ggplot(aes(x = date, y = speakers, fill = date, label = speakers)) +
  facet_wrap(vars(sex), ncol = 2) +
  geom_bar(stat = "identity") +
  geom_text(vjust=-1.6, color = "black") +
  coord_cartesian(ylim = c(0, 100)) +
  theme_bw() + 
  scale_fill_manual(breaks = c("1990-1994", "1995-2001", "2002-2005"),
                     values = c("#E69F00", "lightgray", "#56B4E9"))
```


## Wordclouds

Word clouds are visual representations where words appear larger based on their frequency, offering a quick visual summary of word importance in a dataset.

```{r wc1, message=FALSE, warning=FALSE}
# create a word cloud visualization
reviews_pos %>%
  # Convert text data to a quanteda corpus
  quanteda::corpus() %>%
  # tokenize the corpus, removing punctuation
  quanteda::tokens(remove_punct = TRUE) %>%
  # remove English stopwords
  quanteda::tokens_remove(stopwords("english")) %>%
  # create a document-feature matrix (DFM)
  quanteda::dfm() %>%
  # generate a word cloud using textplot_wordcloud
  quanteda.textplots::textplot_wordcloud(
    # maximum words to display in the word cloud
    max_words = 150,
    # determine the maximum size of words
    max_size = 10,
    # determine the minimum size of words
    min_size = 1.5,
    # Define a color palette for the word cloud
    color = scales::viridis_pal(option = "A")(10))  

```


<br>

<div class="warning" style='padding:0.5em; background-color:rgba(215,209,204,.3); color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
The `textplot_wordcloud` function creates a word cloud visualization of text data in R. Its main arguments are `x` (a Document-Feature Matrix or DFM), `max_words` (maximum words to display), and `color` (color palette for the word cloud).<br>
</p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br>


Another form of word clouds, known as *comparison clouds*, is helpful in discerning disparities between texts. For instance, we can load various texts and assess how they vary in terms of word frequencies. To illustrate this, we'll reload the positive reviews but also load negative reviews. We also combine the two types of reviews into single documents (one document for the positive reviews and another document for the negative reviews).


```{r load_texts_wc,  message=F, warning=F}
# load reviews
posreviews <- list.files(here::here("data/reviews_pos"), full.names = T, pattern = ".*txt") %>%
  purrr::map_chr(~ readr::read_file(.)) %>% str_c(collapse = " ") %>% str_remove_all("<.*?>")
negreviews <- list.files(here::here("data/reviews_neg"), full.names = T, pattern = ".*txt") %>%
  purrr::map_chr(~ readr::read_file(.))%>% str_c(collapse = " ") %>% str_remove_all("<.*?>")
# inspect
str(posreviews); str(negreviews)
```

Now, we generate a corpus object from these texts and create a variable with the author name.

```{r wc3, message=FALSE, warning=FALSE}
corp_dom <- quanteda::corpus(c(posreviews, negreviews)) 
attr(corp_dom, "docvars")$Author = c("Positive Reviews", "Negative Reviews")
```

Now, we can remove so-called *stopwords* (non-lexical function words) and punctuation and generate the comparison cloud.

```{r wc4, message=FALSE, warning=FALSE}
# create a comparison word cloud for a corpus
corp_dom %>%
  # tokenize the corpus, removing punctuation, symbols, and numbers
  quanteda::tokens(remove_punct = TRUE,
                   remove_symbols = TRUE,
                   remove_numbers = TRUE) %>%
  # remove English stopwords
  quanteda::tokens_remove(stopwords("english")) %>%
  # create a Document-Feature Matrix (DFM)
  quanteda::dfm() %>%
  # group the DFM by the 'Author' column from 'corp_dom'
  quanteda::dfm_group(groups = corp_dom$Author) %>%
  # trim the DFM, keeping terms that occur at least 10 times
  quanteda::dfm_trim(min_termfreq = 10, verbose = FALSE) %>%
  # generate a comparison word cloud
  quanteda.textplots::textplot_wordcloud(
    # create a comparison word cloud
    comparison = TRUE,  
    # set colors for different groups
    color = c("darkgray", "orange"),  
    # define the maximum number of words to display in the word cloud
    max_words = 150)  
```



## Going further

If you want to know more, there are various online resources available to learn R (you can check out a very recommendable introduction [here](https://uvastatlab.github.io/phdplus/intror.html)). 

Here are also some additional resources that you may find helpful:

* Grolemund. G., and Wickham, H., [*R 4 Data Science*](http://r4ds.had.co.nz/), 2017.
    + Highly recommended! (especially chapters 1, 2, 4, 6, and 8)
* Stat545 - Data wrangling, exploration, and analysis with R. University of British Columbia.  <http://stat545.com/>
* Swirlstats, a package that teaches you R and statistics within R: <https://swirlstats.com/>
* DataCamp's (free) *Intro to R* interactive tutorial: <https://www.datacamp.com/courses/free-introduction-to-r>
    + DataCamp's advanced R tutorials require a subscription.
*Twitter: 
    + Explore RStudio Tips https://twitter.com/rstudiotips 
    + Explore #rstats, #rstudioconf

## Ending R sessions

At the end of each session, you can extract information about the session itself (e.g. which R version you used and which versions of packages). This can help others (or even your future self) to reproduce the analysis that you have done.

You can extract the session information by running the `sessionInfo` function (without any arguments)

```{r}
sessionInfo()
```


